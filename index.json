[
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/",
	"title": "AWS Service Catalog Tools Intro Workshop",
	"tags": [],
	"description": "",
	"content": "Welcome Builders! This site is a collection of guidance, tutorials and workshops focusing on helping AWS customers build and manage a multi account environment using the Service Catalog tools.\nUse the headings on the left to find what you are looking for.\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/servicecatalog.html",
	"title": "AWS Service Catalog",
	"tags": [],
	"description": "",
	"content": "What is AWS Service Catalog? AWS Service Catalog Service allows organizations to create and manage catalogs of IT services that are approved for use on AWS.\nThese IT services can include everything from virtual machine images, servers, software, and databases to complete multi-tier application architectures. AWS Service Catalog allows you to centrally manage commonly deployed IT services, and helps you achieve consistent governance and meet your compliance requirements, while enabling users to quickly deploy only the approved IT services they need.\n NOTE: Whilst this Workshop makes use of AWS Service Catalog, the primary goal is to learn how to use a suite of open source tools created to compliment the native AWS service.\n "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/installation/50-bootstrapping/10-using-cfn-and-codebuild.html",
	"title": "CloudFormation/CodeBuild",
	"tags": [],
	"description": "",
	"content": "What are we going to do? You will need to bootstrap spoke accounts so you can configure them using the Service Catalog Tools.\nBootstrapping a spoke account will create an AWS CloudFormation stack in it. This stack will contain the Puppet IAM Role (PuppetRole) which is needed by framework to perform actions in the spoke account.\nThe following steps should be executed using the provided AWS CloudFormation templates and AWS CodeBuild projects which are linked to here or available in the home region of your puppet hub account.\nBootstrapping a spoke You should log into the account you want to bootstrap as a spoke and navigate to the AWS CloudFormation console in your home region using your web browser.\nYou should create an AWS CloudFormation stack with the name servicecatalog-puppet-spoke\nusing the template\nhttps://service-catalog-tools.s3.eu-west-2.amazonaws.com/puppet/latest/servicecatalog-puppet-spoke.template.yaml\nYou should set parameter with the name of PuppetAccountId to the 12 digit AWS Account Id of your puppet hub account.\nBootstrapping spokes in OU If you have enabled AWS Organizations support you may want to bootstrap all spokes within an organizational unit.\nFollowing these steps will allow you to bootstrap all AWS Accounts that exist within the same organizational unit.\nIn your AWS Account you can find an AWS CodeBuild project named: servicecatalog-puppet-bootstrap-an-ou\n  Click Start Build\n  Before you select Start Build again, expand the Environment variables override section.\n  Set OU_OR_PATH to the OU path or AWS Organizational Unit ID that contains all of the spokes you want to bootstrap\n  Set IAM_ROLE_NAME to the name of the IAM Role that is assumable in the spoke accounts - this must be the same name in all accounts\n  Set IAM_ROLE_ARNS to the ARNs you want to assume before assuming before the IAM_ROLE_NAME. This is should you need to assume a role with cross account permissions.\n  Click Start Build again\n  "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/2021-workshop/10-prerequisites.html",
	"title": "Pre-requisites",
	"tags": [],
	"description": "",
	"content": "This workshop assumes you have no experience using the Service Catalog tools. The expected completion time is between 2 - 3 hours. You can complete this in multiple sittings and if you have previous experience using the Service Catalog tools you can expect to complete it sooner.\nIn order to complete this workshop you will need:\n a web browser to access the AWS console AWS account to use the Service Catalog tools installed into the account  If you are using your own AWS account you will need to be able to use the AWS Console to provision resources.\n  "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/design-considerations/multi-account-strategy/starter-framework.html",
	"title": "Starter framework",
	"tags": [],
	"description": "",
	"content": " What are we going to do? This article will explain the starter multi-account framework\nStarter framework Within your AWS Organization there are two types of Organizational Units (OUs) - foundational and additional.\nThe Foundational OUs group the shared accounts needed to manage the your overall AWS environment. The areas considered foundational are security, networking and logs. Each of the AWS Accounts within the foundational OUs are grouped into production and non-production environments in order to clearly distinguish between production and non production policies.\nThe additional OUs group the accounts directly related to the software development lifecycle. This includes the accounts for development (development sandboxes, source code and continuous delivery) and the accounts for the staging process from earliest testing to production.\n  Foundational Security This OUs where the accounts for security are grouped. This OU and the accounts within it is the main responsibility of the security organization.\nLog Archive Account The log archive account would be home to AWS CloudTrail logs and other security logs. We would expect to see versioned, and encrypted Amazon S3 buckets with restrictive bucket policies and MFA on delete. There should be limited access to the account and there should be alarms on user login. The log archive account should be the single source of truth.\nRead only This account would typically be owned by the security team and used to enable security operations. It would have cross account roles for viewing / scanning resources in other accounts. It would be used for exploratory security testing.\nBreak glass This account would typically be owned by the security team and used to enable security operations. It would have cross account roles for making changes to resources in other accounts. It would be used in case of an event. It should have extremely limited access, almost never be used and have an alert on login.\nTooling This account / these accounts would typically be owned by the security team and used to enable security operations. It would would be used for tools such as AWS Guard Duty, AWS Security Hub, AWS Config aggregation or Cloud Custodian.\nThere could be more than one tooling account.\nInfrastructure Infrastructure is intended for shared infrastructure services such as networking and optionally any shared hosted infrastructure services.\nAccounts in other foundational and additional OUs should only depend on the infrastructure/prod OU accounts.\nShared Services This account / these accounts would typically host the following shared services consumed by others:\n LDAP/Active Directory Deployment tools  Golden AMI Pipeline    Network This account / these accounts would typically host the following networking services consumed by others:\n DNS Shared Services VPC  Additional The Additional OUs group the accounts directly related to the SDLC. This includes the accounts for development (development sandboxes, source code and continuous delivery) and the accounts for the staging process from earliest testing to production.\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/installation/30-service-catalog-puppet/10-using-aws-organizations.html",
	"title": "Using AWS Organizations",
	"tags": [],
	"description": "",
	"content": "You can use Service Catalog Puppet with AWS Organizations. Using it will allow you describe all accounts within an organizational unit as a single entity. This provides a quicker way to get started and an easier way of managing multiple account environments. You can also use AWS Service Catalog's support for AWS Organizations delegated administrator to reduce the number of invites and accepts for portfolio sharing.\nWhat are we going to do? When enabling AWS Organizations you will need to provision an IAM Role in the Organization's management account and you will then need to provide the ARN of that role to your puppet account as an AWS SSM parameter.\nThe role provisioned in the Organizations management account is only used to list accounts. It has no write access.\nYou can use the AWS CloudFormation to enable AWS Organizations.\nUsing AWS CloudFormation to create the IAM Role Within your AWS Organizations management account you should create an AWS CloudFormation stack with the following name:\npuppet-organizations-initialization-stack\nUsing the template of the URL:\nhttps://service-catalog-tools.s3.eu-west-2.amazonaws.com/puppet/latest/servicecatalog-puppet-org-master.template.yaml\nThis stack will have an output named PuppetOrgRoleForExpandsArn. Take a note of this Arn as you will need it in the next step.\nSetting the Org IAM role ARN Once you have created the IAM Role, you need to tell the framework which role you want to use. You do this by creating an AWS Systems Manager Parameter Store parameter named /servicecatalog-puppet/org-iam-role-arn in the region of the account where you will install puppet and use as your hub account. You can do this via the console or via the cli.\n aws ssm put-parameter --name /servicecatalog-puppet/org-iam-role-arn --type String --value \u0026lt;VALUE-FROM-STEP-ABOVE\u0026gt;  "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/tools.html",
	"title": "Service Catalog Tools",
	"tags": [],
	"description": "",
	"content": " What are the Service Catalog Tools The Service Catalog Tools are a collection of open source tools authored to accelerate your use as AWS Service Catalog.\nTo find out more about the tools please read through the following:\nWhat is Service Catalog Factory Service Catalog Factory is part of a suite of open source tools which have been built to complement the AWS Service Catalog Service.\nService Catalog Factory enables you to quickly build AWS CodePipelines that will create AWS Service Catalog portfolios and populate them with products across multiple regions of your AWS Account. You specify where in git the source code is for your products and you specify which regions you would like your products to exist and the framework will perform all of the undifferentiated heavy lifting for you.\nIn addition, the pipelines that the framework creates can perform functional tests and static code analysis on your templates to help you with your Software Development Life-Cycle (SDLC).\nService Catalog Factory allows you to define AWS Service Catalog portfolios and products using YAML. You can version your products and specify where the source code for them can be found.\nYou can configure the framework to publish the portfolios, products and versions in every AWS Region that you specify.\nHigh level architecture diagram You build products in a central hub account using AWS CodePipeline and AWS CodeBuild, you then deploy them into AWS Service Catalog in every enabled region of your hub account using AWS CodePipeline and AWS CloudFormation.\n  User interaction with the framework is via a YAML file. The YAML file contains the definition of the portfolios and products you want to manage. Updates to the YAML file in AWS CodeCommit triggers the AWS CodePipeline to manage execute the tasks required.\nWhat is Service Catalog Puppet Service Catalog Puppet is part of a suite of open source tools which have been built to complement the AWS Service Catalog Service.\nService Catalog Puppet enables you to provision AWS Service Catalog Products into multiple AWS Accounts and Regions across your AWS estate.\nThe tool reduces the operational burden of engineering a solution to support portfolio sharing and product launches across an enterprise and allows you to focus on writing the products you require to support your organization's needs.\nService Catalog Puppet makes use of a number of AWS services including AWS CodePipeline, AWS CodeBuild and AWS CloudFormation to manage this for you.\nHigh-Level Architecture Diagram You use an AWS CodeBuild project in a central hub account that provisions AWS Service Catalog Products into spoke accounts on your behalf. The framework takes care of cross-account sharing and cross region product replication for you.\n  User interaction with the framework is via a YAML file. The YAML file contains the definition of the AWS Accounts you want to manage (using tags), the portfolios you want to share and the products you want to launch.\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/2021-workshop/300-provisioning-subnets/15-using-service-catalog.html",
	"title": "Using AWS Service Catalog",
	"tags": [],
	"description": "",
	"content": " What are we going to do? We are going to perform the following steps:\n Define a product with a version and a portfolio Add the source code for our product Provision the product subnet into a spoke account  Step by step guide Here are the steps you need to follow to \u0026ldquo;Using AWS Service Catalog\u0026rdquo;\nDefine a product with a version and a portfolio   Navigate to the ServiceCatalogFactory CodeCommit repository\n  Open the Add file menu and click the Create file button\n  Copy the following snippet into the main input field:\n Schema: factory-2019-04-01 Products: - Name: \u0026#34;subnet\u0026#34; Owner: \u0026#34;networking@example.com\u0026#34; Description: \u0026#34;subnet for networking\u0026#34; Distributor: \u0026#34;networking team\u0026#34; SupportDescription: \u0026#34;Speak to networking@example.com about exceptions and speak to cloud-engineering@example.com about implementation issues\u0026#34; SupportEmail: \u0026#34;cloud-engineering@example.com\u0026#34; SupportUrl: \u0026#34;https://wiki.example.com/cloud-engineering/networking/subnet\u0026#34; Tags: - Key: \u0026#34;type\u0026#34; Value: \u0026#34;governance\u0026#34; - Key: \u0026#34;creator\u0026#34; Value: \u0026#34;cloud-engineering\u0026#34; - Key: \u0026#34;cost-center\u0026#34; Value: \u0026#34;governance\u0026#34; Versions: - Name: \u0026#34;v1\u0026#34; Description: \u0026#34;v1 of subnet\u0026#34; Active: True Source: Provider: \u0026#34;CodeCommit\u0026#34; Configuration: RepositoryName: \u0026#34;subnet\u0026#34; BranchName: \u0026#34;main\u0026#34; Portfolios: - \u0026#34;mandatory\u0026#34; Portfolios: - DisplayName: \u0026#34;mandatory\u0026#34; Description: \u0026#34;Portfolio containing the mandatory networking components\u0026#34; ProviderName: \u0026#34;cloud-engineering\u0026#34; Associations: - \u0026#34;arn:aws:iam::${AWS::AccountId}:role/\u0026lt;INSERT YOUR ROLE NAME HERE\u0026gt;\u0026#34; Tags: - Key: \u0026#34;type\u0026#34; Value: \u0026#34;governance\u0026#34; - Key: \u0026#34;creator\u0026#34; Value: \u0026#34;cloud-engineering\u0026#34; - Key: \u0026#34;cost-center\u0026#34; Value: \u0026#34;governance\u0026#34;     Set the File name to portfolios/networking.yaml\n  Set your Author name\n  Set your Email address\n  Set your Commit message\n  Using a good / unique commit message will help you understand what is going on later.\n  Click the Commit changes button:    What did we just do? The YAML file we created in the CodeCommit repository told the framework to perform several actions:\n create a product named subnet add a v1 of our product create a portfolio named networking-mandatory add the product: subnet to the portfolio: networking-mandatory  Verify the change worked Once you have made your changes the ServiceCatalogFactory Pipeline should have run. If you were very quick in making the change, the pipeline may still be running. If it has not yet started feel free to the hit the Release change button.\nOnce it has completed it should show the Source and Build stages in green to indicate they have completed successfully:\n  Add the source code for our product When you configured your product version, you specified the following version:\n Versions: - Name: \u0026#34;v1\u0026#34; Description: \u0026#34;v1 of subnet\u0026#34; Active: True Source: Provider: \u0026#34;CodeCommit\u0026#34; Configuration: RepositoryName: \u0026#34;subnet\u0026#34; BranchName: \u0026#34;main\u0026#34;   This tells the framework the source code for the product comes from the main branch of a CodeCommit repository of the name subnet.\nWe now need to create the CodeCommit repository and add the AWS CloudFormation template we are going to use for our product.\n  Navigate to AWS CodeCommit\n  Click Create repository\n     Input the name subnet     Click Create     Scroll down to the bottom of the page and hit the Create file button     Copy the following snippet into the main input field:   AWSTemplateFormatVersion: \u0026#39;2010-09-09\u0026#39; Description: | Builds out a VPC for use Parameters: SubnetCIDR: Type: String Description: | Subnet to use for the Subnet VPCID: Type: String Description: | VPC to create Subnet in Resources: Subnet: Type: AWS::EC2::Subnet Properties: VpcId: Ref: VPCID CidrBlock: Ref: SubnetCIDR AvailabilityZone: !Select - 0 - !GetAZs Ref: \u0026#39;AWS::Region\u0026#39;     Set the File name to product.template.yaml\n  Set your Author name\n  Set your Email address\n  Set your Commit message\n  Using a good / unique commit message will help you understand what is going on later.\n Creating that file should trigger your subnet-v1-pipeline.\nOnce the pipeline has completed it should show the stages in green to indicate they have completed successfully:\n  You should see your commit message on this screen, it will help you know which version of ServiceCatalogFactory repository the pipeline is processing.\n Once you have verified the pipeline has run you can go to Service Catalog products to view your newly created version.\nYou should see the product you created listed:\n  Click on the product and verify v1 is there\n  If you cannot see your version please raise your hand for some assistance\n You have now successfully created a version for your product!\nVerify the product was added to the portfolio Now that you have verified the pipeline has run you can go to Service Catalog portfolios to view your portfolio.\n Click on networking-mandatory      Click on the product subnet\n  Click on the version v1\n    Provision the product subnet into a spoke account   Navigate to the ServiceCatalogPuppet CodeCommit repository again\n  Click on manifest.yaml\n  Click Edit\n  Append the following snippet to the end of the main input field:\n launches: subnet: portfolio: \u0026#34;networking-mandatory\u0026#34; product: \u0026#34;subnet\u0026#34; version: \u0026#34;v1\u0026#34; depends_on: name: vpc type: stack affinity: stack parameters: VPCID: ssm: name: \u0026#34;/networking/vpc/account-parameters/${AWS::AccountId}/${AWS::Region}/VPCId\u0026#34; SubnetCIDR: default: \u0026#39;10.0.0.0/24\u0026#39; deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;     The main input field should look like this (remember to set your account_id):\n   accounts: - account_id: \u0026#34;\u0026lt;YOUR_ACCOUNT_ID_WITHOUT_HYPHENS\u0026gt;\u0026#34; name: \u0026#34;puppet-account\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34; stacks: delete-default-networking-function: name: \u0026#34;delete-default-networking-function\u0026#34; version: \u0026#34;v1\u0026#34; capabilities: - CAPABILITY_NAMED_IAM deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34; vpc: name: \u0026#34;vpc\u0026#34; version: \u0026#34;v1\u0026#34; depends_on: - name: \u0026#34;delete-default-networking\u0026#34; type: \u0026#34;lambda-invocation\u0026#34; affinity: \u0026#34;lambda-invocation\u0026#34; deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34; outputs: ssm: - param_name: \u0026#34;/networking/vpc/account-parameters/${AWS::AccountId}/${AWS::Region}/VPCId\u0026#34; stack_output: VPCId lambda-invocations: delete-default-networking: function_name: DeleteDefaultNetworking qualifier: $LATEST invocation_type: Event depends_on: - name: \u0026#34;delete-default-networking-function\u0026#34; type: \u0026#34;stack\u0026#34; affinity: \u0026#34;stack\u0026#34; invoke_for: tags: - regions: \u0026#34;default_region\u0026#34; tag: \u0026#34;type:prod\u0026#34; assertions: assert-no-default-vpcs: expected: source: manifest config: value: - \u0026#34;\u0026#34; actual: source: boto3 config: client: \u0026#39;ec2\u0026#39; call: describe_vpcs arguments: {} use_paginator: true filter: Vpcs[?IsDefault==`true`].State depends_on: - name: \u0026#34;delete-default-networking\u0026#34; type: \u0026#34;lambda-invocation\u0026#34; affinity: \u0026#34;lambda-invocation\u0026#34; assert_for: tags: - regions: regions_enabled tag: type:prod launches: subnet: portfolio: \u0026#34;networking-mandatory\u0026#34; product: \u0026#34;subnet\u0026#34; version: \u0026#34;v1\u0026#34; depends_on: name: vpc type: stack affinity: stack parameters: VPCID: ssm: name: \u0026#34;/networking/vpc/account-parameters/${AWS::AccountId}/${AWS::Region}/VPCId\u0026#34; SubnetCIDR: default: \u0026#39;10.0.0.0/24\u0026#39; deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;   Committing the manifest file  Set your Author name Set your Email address Set your Commit message  Using a good / unique commit message will help you understand what is going on later.\n  Click the Commit changes button:    What did we just do? The YAML file we created in the previous step told the framework to perform the following actions:\n provision a product named subnet into the default region of the account  When you added the following:\n launches: subnet: portfolio: \u0026#34;networking-mandatory\u0026#34; product: \u0026#34;subnet\u0026#34; version: \u0026#34;v1\u0026#34; depends_on: name: vpc type: stack affinity: stack parameters: VPCID: ssm: name: \u0026#34;/networking/vpc/account-parameters/${AWS::AccountId}/${AWS::Region}/VPCId\u0026#34; SubnetCIDR: default: \u0026#39;10.0.0.0/24\u0026#39; deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;   You told the framework to provision v1 of subnet from the portfolio networking-mandatory into every account that has the tag type:prod\n accounts: - account_id: \u0026#34;\u0026lt;YOUR_ACCOUNT_ID_WITHOUT_HYPHENS\u0026gt;\u0026#34; name: \u0026#34;puppet-account\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34;    Within each account there will be a copy of the product provisioned into the default region:\n accounts: - account_id: \u0026#34;\u0026lt;YOUR_ACCOUNT_ID_WITHOUT_HYPHENS\u0026gt;\u0026#34; name: \u0026#34;puppet-account\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34;  regions_enabled: - \u0026#34;eu-west-1\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34;   Verifying the provisioned product Once you have made your changes the ServiceCatalogPuppet Pipeline should have run. If you were quick in making the change, the pipeline may still be running. If it has not yet started feel free to the hit the Release change button.\nOnce it has completed it should show the stages in green to indicate they have completed successfully:\n  Once you have verified the pipeline has run you can go to Service Catalog provisioned products to view your provisioned product. Please note when you arrive at the provisioned product page you will need to select account from the filter by drop down in the top right:\n  You have now successfully provisioned a product\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/2021-workshop/400-self-service/15-using-service-catalog.html",
	"title": "Using AWS Service Catalog",
	"tags": [],
	"description": "",
	"content": " What are we going to do? We are going to perform the following steps:\n Create a portfolio for sharing products Share the portfolio networking-optional into a spoke account  Step by step guide Here are the steps you need to follow to \u0026ldquo;Using AWS Service Catalog\u0026rdquo;\nCreate a portfolio for sharing products   Navigate to the ServiceCatalogFactory CodeCommit repository\n  Click on portfolios, then networking.yaml and click \u0026ldquo;Edit\u0026rdquo;.\n  Append the following snippet into the portfolios section whilst updating the role name to the one you are using in the associations section:\n - DisplayName: \u0026#34;Optional\u0026#34; Description: \u0026#34;Portfolio containing the optional networking components\u0026#34; ProviderName: \u0026#34;cloud-engineering\u0026#34; Associations: - \u0026#34;arn:aws:iam::${AWS::AccountId}:role/\u0026lt;INSERT YOUR ROLE NAME HERE\u0026gt;\u0026#34; Tags: - Key: \u0026#34;type\u0026#34; Value: \u0026#34;governance\u0026#34; - Key: \u0026#34;creator\u0026#34; Value: \u0026#34;cloud-engineering\u0026#34; - Key: \u0026#34;cost-center\u0026#34; Value: \u0026#34;governance\u0026#34;     Add the portfolio of - \u0026quot;optional\u0026quot; to the portfolios list for subnet product.\n  The file should look like the following:\n Schema: factory-2019-04-01 Products: - Name: \u0026#34;subnet\u0026#34; Owner: \u0026#34;networking@example.com\u0026#34; Description: \u0026#34;subnet for networking\u0026#34; Distributor: \u0026#34;networking team\u0026#34; SupportDescription: \u0026#34;Speak to networking@example.com about exceptions and speak to cloud-engineering@example.com about implementation issues\u0026#34; SupportEmail: \u0026#34;cloud-engineering@example.com\u0026#34; SupportUrl: \u0026#34;https://wiki.example.com/cloud-engineering/networking/subnet\u0026#34; Tags: - Key: \u0026#34;type\u0026#34; Value: \u0026#34;governance\u0026#34; - Key: \u0026#34;creator\u0026#34; Value: \u0026#34;cloud-engineering\u0026#34; - Key: \u0026#34;cost-center\u0026#34; Value: \u0026#34;governance\u0026#34; Versions: - Name: \u0026#34;v1\u0026#34; Description: \u0026#34;v1 of subnet\u0026#34; Active: True Source: Provider: \u0026#34;CodeCommit\u0026#34; Configuration: RepositoryName: \u0026#34;subnet\u0026#34; BranchName: \u0026#34;main\u0026#34; Portfolios: - \u0026#34;mandatory\u0026#34; - \u0026#34;optional\u0026#34; Portfolios: - DisplayName: \u0026#34;mandatory\u0026#34; Description: \u0026#34;Portfolio containing the mandatory networking components\u0026#34; ProviderName: \u0026#34;cloud-engineering\u0026#34; Associations: - \u0026#34;arn:aws:iam::${AWS::AccountId}:role/\u0026lt;INSERT YOUR ROLE NAME HERE\u0026gt;\u0026#34; Tags: - Key: \u0026#34;type\u0026#34; Value: \u0026#34;governance\u0026#34; - Key: \u0026#34;creator\u0026#34; Value: \u0026#34;cloud-engineering\u0026#34; - Key: \u0026#34;cost-center\u0026#34; Value: \u0026#34;governance\u0026#34; - DisplayName: \u0026#34;Optional\u0026#34; Description: \u0026#34;Portfolio containing the optional networking components\u0026#34; ProviderName: \u0026#34;cloud-engineering\u0026#34; Associations: - \u0026#34;arn:aws:iam::${AWS::AccountId}:role/\u0026lt;INSERT YOUR ROLE NAME HERE\u0026gt;\u0026#34; Tags: - Key: \u0026#34;type\u0026#34; Value: \u0026#34;governance\u0026#34; - Key: \u0026#34;creator\u0026#34; Value: \u0026#34;cloud-engineering\u0026#34; - Key: \u0026#34;cost-center\u0026#34; Value: \u0026#34;governance\u0026#34;     Set your Author name\n  Set your Email address\n  Set your Commit message\n  Using a good / unique commit message will help you understand what is going on later.\n  Click the Commit changes button:    What did we just do? The YAML file we created in the CodeCommit repository told the framework to create a new portfolio and to add the subnet product to that new portfolio.\nVerify the change worked Once you have made your changes the ServiceCatalogFactory Pipeline should have run. If you were very quick in making the change, the pipeline may still be running. If it has not yet started feel free to the hit the Release change button.\nOnce it has completed it should show the Source and Build stages in green to indicate they have completed successfully:\n  Verify the product was added to the portfolio Now that you have verified the pipeline has run you can go to Service Catalog portfolios to view your portfolio.\n Click on networking-optional      Click on the product subnet\n  Click on the version v1\n    Share the portfolio networking-optional into a spoke account   Navigate to the ServiceCatalogPuppet CodeCommit repository again\n  Click on manifest.yaml\n  Click Edit\n  Append the following snippet to the end of the main input field:\n spoke-local-portfolios: networking-optional: portfolio: \u0026#34;networking-optional\u0026#34; product_generation_method: copy depends_on: name: vpc type: stack affinity: stack deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;     The main input field should look like this (remember to set your account_id):\n   accounts: - account_id: \u0026#34;\u0026lt;YOUR_ACCOUNT_ID_WITHOUT_HYPHENS\u0026gt;\u0026#34; name: \u0026#34;puppet-account\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34; stacks: delete-default-networking-function: name: \u0026#34;delete-default-networking-function\u0026#34; version: \u0026#34;v1\u0026#34; capabilities: - CAPABILITY_NAMED_IAM deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34; vpc: name: \u0026#34;vpc\u0026#34; version: \u0026#34;v1\u0026#34; depends_on: - name: \u0026#34;delete-default-networking\u0026#34; type: \u0026#34;lambda-invocation\u0026#34; affinity: \u0026#34;lambda-invocation\u0026#34; deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34; outputs: ssm: - param_name: \u0026#34;/networking/vpc/account-parameters/${AWS::AccountId}/${AWS::Region}/VPCId\u0026#34; stack_output: VPCId lambda-invocations: delete-default-networking: function_name: DeleteDefaultNetworking qualifier: $LATEST invocation_type: Event depends_on: - name: \u0026#34;delete-default-networking-function\u0026#34; type: \u0026#34;stack\u0026#34; affinity: \u0026#34;stack\u0026#34; invoke_for: tags: - regions: \u0026#34;default_region\u0026#34; tag: \u0026#34;type:prod\u0026#34; assertions: assert-no-default-vpcs: expected: source: manifest config: value: - \u0026#34;\u0026#34; actual: source: boto3 config: client: \u0026#39;ec2\u0026#39; call: describe_vpcs arguments: {} use_paginator: true filter: Vpcs[?IsDefault==`true`].State depends_on: - name: \u0026#34;delete-default-networking\u0026#34; type: \u0026#34;lambda-invocation\u0026#34; affinity: \u0026#34;lambda-invocation\u0026#34; assert_for: tags: - regions: regions_enabled tag: type:prod launches: subnet: portfolio: \u0026#34;networking-mandatory\u0026#34; product: \u0026#34;subnet\u0026#34; version: \u0026#34;v1\u0026#34; depends_on: name: vpc type: stack affinity: stack parameters: VPCID: ssm: name: \u0026#34;/networking/vpc/account-parameters/${AWS::AccountId}/${AWS::Region}/VPCId\u0026#34; SubnetCIDR: default: \u0026#39;10.0.0.0/24\u0026#39; deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34; workspaces: subnet: name: \u0026#34;subnet\u0026#34; version: \u0026#34;v1\u0026#34; depends_on: name: vpc type: stack affinity: stack parameters: VPCID: ssm: name: \u0026#34;/networking/vpc/account-parameters/${AWS::AccountId}/${AWS::Region}/VPCId\u0026#34; SubnetCIDR: default: \u0026#39;10.0.1.0/24\u0026#39; deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34; spoke-local-portfolios: networking-optional: portfolio: \u0026#34;networking-optional\u0026#34; product_generation_method: copy depends_on: name: vpc type: stack affinity: stack deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;   Committing the manifest file  Set your Author name Set your Email address Set your Commit message  Using a good / unique commit message will help you understand what is going on later.\n  Click the Commit changes button:    What did we just do? The changes we made told the framework to make a portfolio in the default region of each spoke with the tag type:prod. This portfolio will contain copies of the products that exist in the hub account.\nVerifying the portfolio share Once you have made your changes the ServiceCatalogPuppet Pipeline should have run. If you were quick in making the change, the pipeline may still be running. If it has not yet started feel free to the hit the Release change button.\nAs this workshop has been designed to run in a single region of a single account you cannot verify this step. If the pipeline ran and each stage has succeeded the share should have taken place. When sharing a portfolio with the same account it was created in the framework does not perform any actions.\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/2021-workshop/100-removing-the-default-networking/20-creating-the-lambda.html",
	"title": "Creating the Lambda",
	"tags": [],
	"description": "",
	"content": " What are we going to do? We are going to perform the following steps:\n Define a stack Add the source code for our product  The hub AWS Account is the source of truth for our stacks. Spoke AWS accounts are consumers of these stacks, you can think of them as accounts that need governance controls applied. For this workshop, we are using the same account as both the hub and spoke for simplicity; in a multi-account setup, these could be separate AWS Accounts and Regions.\nStep by step guide Here are the steps you need to follow to \u0026ldquo;Creating the Lambda\u0026rdquo;\nDefine a stack   Navigate to the ServiceCatalogFactory CodeCommit repository\n  Scroll down to the bottom of the page and hit the Create file button\n      Copy the following snippet into the main input field:\n Schema: factory-2019-04-01 Stacks: - Name: \u0026#34;delete-default-networking-function\u0026#34; Versions: - Name: \u0026#34;v1\u0026#34; Active: True Source: Provider: \u0026#34;CodeCommit\u0026#34; Configuration: RepositoryName: \u0026#34;delete-default-networking-function\u0026#34; BranchName: \u0026#34;main\u0026#34;     Set the File name to stacks/network-workshop.yaml\n  Set your Author name\n  Set your Email address\n  Set your Commit message\n  Using a good / unique commit message will help you understand what is going on later.\n  Click the Commit changes button:    What did we just do? The YAML file we created in the CodeCommit repository told the framework to:\n create a pipeline that will take source code from a branch named main of CodeCommit repo named delete-default-networking-function  Verify the change worked Once you have made your changes the ServiceCatalogFactory Pipeline should have run. If you were very quick in making the change, the pipeline may still be running. If it has not yet started feel free to the hit the Release change button.\nOnce it has completed it should show the Source and Build stages in green to indicate they have completed successfully:\n  The screenshots may differ slightly as the design of AWS CodePipeline changes. You should see a pipeline where each stage is green.\n Add the source code for our product When you configured your product version, you specified the following version:\n Versions: - Name: \u0026#34;v1\u0026#34; Active: True Source: Provider: \u0026#34;CodeCommit\u0026#34; Configuration: RepositoryName: \u0026#34;delete-default-networking-function\u0026#34; BranchName: \u0026#34;main\u0026#34;   This tells the framework the source code for the product comes from the main branch of a CodeCommit repository of the name delete-default-networking-function.\nWe now need to create the CodeCommit repository and add the AWS CloudFormation template we are going to use for our product.\n  Navigate to AWS CodeCommit\n  Click Create repository\n     Input the name delete-default-networking-function     Click Create     Scroll down to the bottom of the page and hit the Create file button     Copy the following snippet into the main input field:   AWSTemplateFormatVersion: \u0026#34;2010-09-09\u0026#34; Description: | Deletes the following default networking components from AWS Accounts: 1) Deletes the internet gateway 2) Deletes the subnets 4) Deletes the network access lists 5) Deletes the security groups 6) Deletes the default VPC {\u0026#34;framework\u0026#34;: \u0026#34;servicecatalog-products\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;product\u0026#34;, \u0026#34;product-set\u0026#34;: \u0026#34;delete-default-vpc\u0026#34;, \u0026#34;product\u0026#34;: \u0026#34;delete-default-vpc\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;v1\u0026#34;} Parameters: DeleteDefaultNetworkingRoleNameToAssume: Description: \u0026#34;Name of the IAM Role that will be assumed in the spoke account to remove networking\u0026#34; Type: String Default: \u0026#34;servicecatalog-puppet/PuppetRole\u0026#34; DeleteDefaultVPCLambdaExecutionIAMRoleName: Description: \u0026#34;Name of the IAM Role that will be created to execute the lambda function\u0026#34; Type: String Default: \u0026#34;DeleteDefaultVPCLambdaExecution\u0026#34; DeleteDefaultVPCLambdaExecutionIAMRolePath: Description: \u0026#34;The path for the IAM Role that will be created to execute the lambda function\u0026#34; Type: String Default: \u0026#34;/DeleteDefaultVPCLambdaExecution/\u0026#34; DeleteDefaultNetworkingLambdaFunctionName: Description: \u0026#34;The name to give the function that deletes the default networking resources\u0026#34; Type: String Default: \u0026#34;DeleteDefaultNetworking\u0026#34; Resources: DefaultVpcDeletionRole: Type: AWS::IAM::Role Properties: RoleName: !Ref DeleteDefaultVPCLambdaExecutionIAMRoleName Path: !Ref DeleteDefaultVPCLambdaExecutionIAMRolePath AssumeRolePolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: sts:AssumeRole Principal: Service: lambda.amazonaws.com Policies: - PolicyName: Organizations PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: - \u0026#34;sts:AssumeRole\u0026#34; Resource: !Sub \u0026#39;arn:${AWS::Partition}:iam::*:role/${DeleteDefaultNetworkingRoleNameToAssume}\u0026#39; - Effect: Allow Action: - \u0026#34;ec2:DescribeInternetGateways\u0026#34; - \u0026#34;ec2:DetachInternetGateway\u0026#34; - \u0026#34;ec2:DeleteInternetGateway\u0026#34; - \u0026#34;ec2:DescribeSubnets\u0026#34; - \u0026#34;ec2:DeleteSubnet\u0026#34; - \u0026#34;ec2:DescribeRouteTables\u0026#34; - \u0026#34;ec2:DeleteRouteTable\u0026#34; - \u0026#34;ec2:DescribeNetworkAcls\u0026#34; - \u0026#34;ec2:DeleteNetworkAcl\u0026#34; - \u0026#34;ec2:DeleteSecurityGroup\u0026#34; - \u0026#34;ec2:DeleteVpc\u0026#34; - \u0026#34;ec2:DescribeRegions\u0026#34; - \u0026#34;ec2:DescribeAccountAttributes\u0026#34; - \u0026#34;ec2:DescribeNetworkInterfaces\u0026#34; - \u0026#34;ec2:DescribeSecurityGroups\u0026#34; - \u0026#34;ec2:DeleteSecurityGroup\u0026#34; - \u0026#34;logs:CreateLogGroup\u0026#34; - \u0026#34;logs:CreateLogStream\u0026#34; - \u0026#34;logs:PutLogEvents\u0026#34; Resource: \u0026#39;*\u0026#39; Function: Type: AWS::Lambda::Function Properties: FunctionName: !Ref DeleteDefaultNetworkingLambdaFunctionName Handler: index.lambda_handler Runtime: python3.7 Timeout: 600 Role: !GetAtt DefaultVpcDeletionRole.Arn Environment: Variables: DeleteDefaultNetworkingRoleNameToAssume: !Ref DeleteDefaultNetworkingRoleNameToAssume Partition: !Sub \u0026#39;${AWS::Partition}\u0026#39; Code: ZipFile: | import boto3, logging, traceback, os from boto3 import Session logger = logging.getLogger() logger.setLevel(logging.INFO) logging.basicConfig( format=\u0026#39;%(levelname)s %(threadName)s [%(filename)s:%(lineno)d] %(message)s\u0026#39;, datefmt=\u0026#39;%Y-%m-%d:%H:%M:%S\u0026#39;, level=logging.INFO ) def delete_igw(client, vpc_id): fltr = [{\u0026#39;Name\u0026#39;: \u0026#39;attachment.vpc-id\u0026#39;, \u0026#39;Values\u0026#39;: [vpc_id]}] try: igw = client.describe_internet_gateways(Filters=fltr)[\u0026#39;InternetGateways\u0026#39;] if igw: igw_id = igw[0][\u0026#39;InternetGatewayId\u0026#39;] client.detach_internet_gateway(InternetGatewayId=igw_id, VpcId=vpc_id) client.delete_internet_gateway(InternetGatewayId=igw_id) return except Exception as ex: logger.error(ex) traceback.print_tb(ex.__traceback__) raise def delete_subnets(client): try: subs = client.describe_subnets()[\u0026#39;Subnets\u0026#39;] if subs: for sub in subs: sub_id = sub[\u0026#39;SubnetId\u0026#39;] client.delete_subnet(SubnetId=sub_id) return except Exception as ex: logger.error(ex) traceback.print_tb(ex.__traceback__) raise def delete_rtbs(client): try: rtbs = client.describe_route_tables()[\u0026#39;RouteTables\u0026#39;] if rtbs: for rtb in rtbs: main = False for assoc in rtb[\u0026#39;Associations\u0026#39;]: main = assoc[\u0026#39;Main\u0026#39;] if main: continue rtb_id = rtb[\u0026#39;RouteTableId\u0026#39;] client.delete_route_table(RouteTableId=rtb_id) return except Exception as ex: logger.error(ex) traceback.print_tb(ex.__traceback__) raise def delete_acls(client): try: acls = client.describe_network_acls()[\u0026#39;NetworkAcls\u0026#39;] if acls: for acl in acls: default = acl[\u0026#39;IsDefault\u0026#39;] if default: continue acl_id = acl[\u0026#39;NetworkAclId\u0026#39;] client.delete_network_acl(NetworkAclId=acl_id) return except Exception as ex: logger.error(ex) traceback.print_tb(ex.__traceback__) raise def delete_sgps(client): try: sgps = client.describe_security_groups()[\u0026#39;SecurityGroups\u0026#39;] if sgps: for sgp in sgps: default = sgp[\u0026#39;GroupName\u0026#39;] if default == \u0026#39;default\u0026#39;: continue sg_id = sgp[\u0026#39;GroupId\u0026#39;] client.delete_security_group(GroupId=sg_id) return except Exception as ex: logger.error(ex) traceback.print_tb(ex.__traceback__) raise def delete_vpc(client, vpc_id, region): try: client.delete_vpc(VpcId=vpc_id) logger.info(\u0026#39;VPC {} has been deleted from the {} region.\u0026#39;.format(vpc_id, region)) return except Exception as ex: logger.error(ex) traceback.print_tb(ex.__traceback__) raise def lambda_handler(e, c): account_id=e.get(\u0026#34;account_id\u0026#34;) region=e.get(\u0026#34;region\u0026#34;) role_arn = f\u0026#34;arn:{os.getenv(\u0026#39;Partition\u0026#39;)}:iam::{account_id}:role/{os.getenv(\u0026#39;DeleteDefaultNetworkingRoleNameToAssume\u0026#39;)}\u0026#34; sts = Session().client(\u0026#39;sts\u0026#39;) assumed_role_object = sts.assume_role( RoleArn=role_arn, RoleSessionName=\u0026#34;spoke\u0026#34;, ) credentials = assumed_role_object[\u0026#39;Credentials\u0026#39;] kwargs = { \u0026#34;service_name\u0026#34;: \u0026#34;ec2\u0026#34;, \u0026#34;aws_access_key_id\u0026#34;: credentials[\u0026#39;AccessKeyId\u0026#39;], \u0026#34;aws_secret_access_key\u0026#34;: credentials[\u0026#39;SecretAccessKey\u0026#39;], \u0026#34;aws_session_token\u0026#34;: credentials[\u0026#39;SessionToken\u0026#39;], } ec2 = Session().client(**kwargs, region_name=region) try: attribs = ec2.describe_account_attributes(AttributeNames=[\u0026#39;default-vpc\u0026#39;])[\u0026#39;AccountAttributes\u0026#39;] vpc_id = attribs[0][\u0026#39;AttributeValues\u0026#39;][0][\u0026#39;AttributeValue\u0026#39;] if vpc_id == \u0026#39;none\u0026#39;: logger.info(\u0026#39;Default VPC not found in {}\u0026#39;.format(region)) return # Since most resources are attached an ENI, this checks for additional resources f = [{\u0026#39;Name\u0026#39;: \u0026#39;vpc-id\u0026#39;, \u0026#39;Values\u0026#39;: [vpc_id]}] eni = ec2.describe_network_interfaces(Filters=f)[\u0026#39;NetworkInterfaces\u0026#39;] if eni: logger.error(\u0026#39;VPC {} has existing resources in the {} region.\u0026#39;.format(vpc_id, region)) return delete_igw(ec2, vpc_id) delete_subnets(ec2) delete_rtbs(ec2) delete_acls(ec2) delete_sgps(ec2) delete_vpc(ec2, vpc_id, region) except Exception as ex: logger.error(ex) traceback.print_tb(ex.__traceback__)     Set the File name to stack.template.yaml\n  Set your Author name\n  Set your Email address\n  Set your Commit message\n  Using a good / unique commit message will help you understand what is going on later.\n Creating that file should trigger your stack\u0026ndash;delete-default-networking-function-v1-pipeline.\nOnce the pipeline has completed it should show the stages in green to indicate they have completed successfully:\n  You should see your commit message on this screen, it will help you know which version of ServiceCatalogFactory repository the pipeline is processing.\n The screenshots may differ slightly as the design of AWS CodePipeline changes. You should see a pipeline where each stage is green.\n You have now successfully created a stack!\nVerify the stack is present in Amazon S3 Now that you have verified the pipeline has run correctly you can go to Amazon S3 to view the stack.\n  Navigate to https://s3.console.aws.amazon.com/s3/home\n  Select the bucket named sc-puppet-stacks-repository-\u0026lt;account_id\u0026gt;\n  Navigate to stack/delete-default-networking-function/v1 where you should see an object named stack.template.yaml\n  "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/design-considerations.html",
	"title": "Design considerations",
	"tags": [],
	"description": "",
	"content": "Following the best practices for the Service Catalog Tools will ensure you get the most out of these tools with the most minimal amount of effort.\nThe following articles will help you design your solution. It is recommended you read through each - ideally before you install the tools or start provisioning / sharing products and portfolios.\n Multi Account Strategy   Selecting a factory account   Using parameters in the real world   Fine grained depends_on   Portfolio Management   Account Tagging   Product SDLC   Using IAM and SCP Effectively   "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/installation.html",
	"title": "Installation",
	"tags": [],
	"description": "",
	"content": "Prerequisites In order to install these tools you will need:\n A single AWS Account which you can log into A web browser where to access the AWS console.  You will also need to decide which account to install these tools into.\nThis account will contain the AWS CodePipelines and will need to be accessible to any accounts you would like to share products with. If you want to use the optional AWS Organizations support you will need to install the tools into an AWS Account where there is (or can be) a trust relationship with the AWS Organizations management account. You can install these tools into your management account but this is not recommended.\nBoth tools should be installed into the same region of the same account.\nTask list Here:\n Service Catalog Factory   Service Catalog Puppet   Onboarding spokes   Securing the installation   "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/installation/20-service-catalog-factory.html",
	"title": "Service Catalog Factory",
	"tags": [],
	"description": "",
	"content": "Navigate to CloudFormation  Select the AWS CloudFormation Service.    If you are installing Service Catalog Puppet it will need to be installed into the same account as Service Catalog Factory.\n Create a new CloudFormation Stack  Select \u0026lsquo;Create Stack\u0026rsquo;    Select the pre-configured CloudFormation Template Service Catalog Factory can be installed via a pre-created AWS CloudFormation template stored in Amazon S3 under the following URL:\n https://service-catalog-tools.s3.eu-west-2.amazonaws.com/factory/latest/servicecatalog-factory-initialiser.template.yaml\n  Paste this URL under \u0026lsquo;Amazon S3 URL\u0026rsquo;: Hit Next  Specify Stack Details  Specify the Stack details as follows:  Stack Name: factory-initialization-stack    You should fill in the details depending on which source code management system you want to use:\nCodeCommit You need to set SCMSourceProvider to CodeCommit.\nYou should also set the following:\n SCMRepositoryName - this is the name of the git repo to use SCMBranchName - this is the branch you want to use SCMShouldCreateRepo - set this to true if you want the tools to create the repo for you  Github.com / Github Enterprise / Bitbucket Cloud You should have a read through the following guide: https://docs.aws.amazon.com/dtconsole/latest/userguide/connections.html\nYou need to set SCMSourceProvider to CodeStarSourceConnection.\nYou should also set the following:\n SCMConnectionArn - this is the Arn of the connection you created in the console SCMFullRepositoryId - the value for this is dependant on which SCM you use SCMBranchName - this is the branch you want to use  S3 You need to set SCMSourceProvider to S3.\nYou should also set the following:\n SCMBucketName - this is the name of the S3 bucket you want to use SCMObjectKey - this is the name of the object key you will be uploading your zip file as to trigger pipeline runs SCMShouldCreateRepo - set this to true if you want the tools to create the repo for you  Create the CloudFormation Stack  Leave Defaults for \u0026lsquo;Configure Stack Options\u0026rsquo; Hit Next Acknowledge that the Stack will create an IAM Role Hit \u0026lsquo;Create Stack\u0026rsquo;     You will now see the stack status as CREATE_IN_PROGRESS     Wait for the stack status to go to CREATE_COMPLETE    What have we deployed? The following AWS resources have just been deployed into your AWS Account:\nAWS CloudFormation stacks The AWS CodeBuild job created two AWS CloudFormation stacks which in turn deployed the resources listed below:\n URL: https://eu-west-1.console.aws.amazon.com/cloudformation/home?region=eu-west-1\n   Factory AWS CodeCommit repository This repository holds the Service Catalog Factory YAML files which are used to configure AWS Service Catalog portfolios and products.\n URL: https://eu-west-1.console.aws.amazon.com/codesuite/codecommit/repositories?region=eu-west-1\n   Factory AWS CodePipeline This AWS CodePipeline is triggered by updates to the AWS CodeCommit repository. When run, it will create the AWS Service Catalog portfolios and products defined in the portfolio files.\n URL: https://eu-west-1.console.aws.amazon.com/codesuite/codepipeline/pipelines?region=eu-west-1\n   Amazon S3 Buckets An Amazon S3 Bucket was created to store artifacts for Service Catalog factory.\n URL: https://s3.console.aws.amazon.com/s3/home?region=eu-west-1\n   "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/upgrading/service-catalog-factory.html",
	"title": "Service Catalog Factory",
	"tags": [],
	"description": "",
	"content": "Before you update This guide applies to all users who installed using the AWS CloudFormation template. If you did not install using the template then you need to follow the install guide. When following the install guide, ensure you provision the initialiser stack into the same region you installed the tools using the CLI method. Installing this way will perform an update and will not break your install.\nIf you have added any regions since your initial install or modified any settings provided by the parameters in the install template ensure you specify the values you want for them as the install process will overwrite any previous settings you configured in your existing install.\nNavigate to CloudFormation  Select the AWS CloudFormation Service.      Select the initialization stack you created when installing Service Catalog Factory. The recommended stack name was factory-initialization-stack\n  Select \u0026lsquo;Update\u0026rsquo;\n  Select \u0026lsquo;Replace current template\u0026rsquo;\n  For Template source select \u0026lsquo;Amazon S3 URL\u0026rsquo; and paste in the following: https://service-catalog-tools.s3.eu-west-2.amazonaws.com/factory/latest/servicecatalog-factory-initialiser.template.yaml\n  Then hit \u0026lsquo;Next\u0026rsquo;\n  Make any changes you want to the parameters you provided and then hit \u0026lsquo;Next\u0026rsquo; again\n  Follow the rest of the steps to update the stack\n  As the stack updates the AWS CodeBuild project named servicecatalog-product-factory-initialiser is started. Once it is completed the stack update will complete. The project will update your installation to the latest version\n  "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/2021-workshop/300-provisioning-subnets/20-using-terraform.html",
	"title": "Using Terraform",
	"tags": [],
	"description": "",
	"content": " What are we going to do? We are going to perform the following steps:\n Define a workspace Add the source code for our product Update the manifest file  Step by step guide Here are the steps you need to follow to \u0026ldquo;Using Terraform\u0026rdquo;\nDefine a workspace   Navigate to the ServiceCatalogFactory CodeCommit repository\n  Open the Add file menu and click the Create file button\n  Paste the following snippet to the main input field:\n Schema: factory-2019-04-01 Workspaces: - Name: \u0026#34;subnet\u0026#34; Versions: - Name: \u0026#34;v1\u0026#34; Active: True Source: Provider: \u0026#34;CodeCommit\u0026#34; Configuration: RepositoryName: \u0026#34;subnet\u0026#34; BranchName: \u0026#34;main\u0026#34;     Set your Author name\n  Set your Email address\n  Set your Commit message\n  Using a good / unique commit message will help you understand what is going on later.\n  Click the Commit changes button:    What did we just do? The YAML file we created in the CodeCommit repository told the framework to:\n create a pipeline that will take source code from a branch named main of CodeCommit repo named subnet-terraform  Verify the change worked Once you have made your changes the ServiceCatalogFactory Pipeline should have run. If you were very quick in making the change, the pipeline may still be running. If it has not yet started feel free to the hit the Release change button.\nOnce it has completed it should show the Source and Build stages in green to indicate they have completed successfully:\n  The screenshots may differ slightly as the design of AWS CodePipeline changes. You should see a pipeline where each stage is green.\n Add the source code for our product When you configured your product version, you specified the following version:\n Versions: - Name: \u0026#34;v1\u0026#34; Active: True Source: Provider: \u0026#34;CodeCommit\u0026#34; Configuration: RepositoryName: \u0026#34;subnet\u0026#34; BranchName: \u0026#34;main\u0026#34;   This tells the framework the source code for the product comes from the main branch of a CodeCommit repository of the name subnet-terraform.\nWe now need to create the CodeCommit repository and add the AWS CloudFormation template we are going to use for our workspace.\n  Navigate to AWS CodeCommit\n  Click Create repository\n     Input the name subnet-terraform     Click Create     Scroll down to the bottom of the page and hit the Create file button     Copy the following snippet into the main input field:   variable \u0026#34;VPCId\u0026#34; { type = string } variable \u0026#34;SubnetCIDR\u0026#34; { type = string } resource \u0026#34;aws_subnet\u0026#34; \u0026#34;main\u0026#34; { vpc_id = var.VPCId cidr_block = var.SubnetCIDR }     Set the File name to subnet.tf\n  Set your Author name\n  Set your Email address\n  Set your Commit message\n  Using a good / unique commit message will help you understand what is going on later.\n The name or number of files does not matter when you are creating your own workspaces using Terraform.\n Creating that file should trigger your workspace\u0026ndash;subnet-v1-pipeline.\nOnce the pipeline has completed it should show the stages in green to indicate they have completed successfully:\n  You should see your commit message on this screen, it will help you know which version of ServiceCatalogFactory repository the pipeline is processing.\n The screenshots may differ slightly as the design of AWS CodePipeline changes. You should see a pipeline where each stage is green.\n You have now successfully created a stack!\nVerify the stack is present in Amazon S3 Now that you have verified the pipeline has run correctly you can go to Amazon S3 to view the stack.\n  Navigate to https://s3.console.aws.amazon.com/s3/home\n  Select the bucket named sc-puppet-stacks-repository-\u0026lt;account_id\u0026gt;\n  Navigate to workspace/subnet/v1 where you should see an object named workspace.zip\n  Update the manifest file   Navigate to the ServiceCatalogPuppet CodeCommit repository again\n  Click on manifest.yaml\n  Click Edit\n  Append the following snippet to the end of the file in the input field:\n workspaces: subnet: name: \u0026#34;subnet\u0026#34; version: \u0026#34;v1\u0026#34; depends_on: name: vpc type: stack affinity: stack parameters: VPCID: ssm: name: \u0026#34;/networking/vpc/account-parameters/${AWS::AccountId}/${AWS::Region}/VPCId\u0026#34; SubnetCIDR: default: \u0026#39;10.0.1.0/24\u0026#39; deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;     The main input field should look like this (remember to set your account_id):\n   accounts: - account_id: \u0026#34;\u0026lt;YOUR_ACCOUNT_ID_WITHOUT_HYPHENS\u0026gt;\u0026#34; name: \u0026#34;puppet-account\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34; stacks: delete-default-networking-function: name: \u0026#34;delete-default-networking-function\u0026#34; version: \u0026#34;v1\u0026#34; capabilities: - CAPABILITY_NAMED_IAM deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34; vpc: name: \u0026#34;vpc\u0026#34; version: \u0026#34;v1\u0026#34; depends_on: - name: \u0026#34;delete-default-networking\u0026#34; type: \u0026#34;lambda-invocation\u0026#34; affinity: \u0026#34;lambda-invocation\u0026#34; deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34; outputs: ssm: - param_name: \u0026#34;/networking/vpc/account-parameters/${AWS::AccountId}/${AWS::Region}/VPCId\u0026#34; stack_output: VPCId lambda-invocations: delete-default-networking: function_name: DeleteDefaultNetworking qualifier: $LATEST invocation_type: Event depends_on: - name: \u0026#34;delete-default-networking-function\u0026#34; type: \u0026#34;stack\u0026#34; affinity: \u0026#34;stack\u0026#34; invoke_for: tags: - regions: \u0026#34;default_region\u0026#34; tag: \u0026#34;type:prod\u0026#34; assertions: assert-no-default-vpcs: expected: source: manifest config: value: - \u0026#34;\u0026#34; actual: source: boto3 config: client: \u0026#39;ec2\u0026#39; call: describe_vpcs arguments: {} use_paginator: true filter: Vpcs[?IsDefault==`true`].State depends_on: - name: \u0026#34;delete-default-networking\u0026#34; type: \u0026#34;lambda-invocation\u0026#34; affinity: \u0026#34;lambda-invocation\u0026#34; assert_for: tags: - regions: regions_enabled tag: type:prod launches: subnet: portfolio: \u0026#34;networking-mandatory\u0026#34; product: \u0026#34;subnet\u0026#34; version: \u0026#34;v1\u0026#34; depends_on: name: vpc type: stack affinity: stack parameters: VPCID: ssm: name: \u0026#34;/networking/vpc/account-parameters/${AWS::AccountId}/${AWS::Region}/VPCId\u0026#34; SubnetCIDR: default: \u0026#39;10.0.0.0/24\u0026#39; deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34; workspaces: subnet: name: \u0026#34;subnet\u0026#34; version: \u0026#34;v1\u0026#34; depends_on: name: vpc type: stack affinity: stack parameters: VPCID: ssm: name: \u0026#34;/networking/vpc/account-parameters/${AWS::AccountId}/${AWS::Region}/VPCId\u0026#34; SubnetCIDR: default: \u0026#39;10.0.1.0/24\u0026#39; deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;   Committing the manifest file Now that we have updated the manifest file we are ready to commit it.\n Set your Author name Set your Email address Set your Commit message  Using a good / unique commit message will help you understand what is going on later.\n  Click the Commit changes button:    What did we just do? When you added the following:\n workspaces: subnet: name: \u0026#34;subnet\u0026#34; version: \u0026#34;v1\u0026#34; depends_on: name: vpc type: stack affinity: stack parameters: VPCID: ssm: name: \u0026#34;/networking/vpc/account-parameters/${AWS::AccountId}/${AWS::Region}/VPCId\u0026#34; SubnetCIDR: default: \u0026#39;10.0.1.0/24\u0026#39; deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;   You told the framework to provision v1 of subnet into the default region of each account that has the tag type:prod\nVerifying the provisioned stack Once you have made your changes the ServiceCatalogPuppet Pipeline should have run. If you were quick in making the change, the pipeline may still be running. If it has not yet started feel free to the hit the Release change button.\nOnce it has completed it should show the stages in green to indicate they have completed successfully:\n  The screenshots may differ slightly as the design of AWS CodePipeline changes. You should see a pipeline where each stage is green.\n You have now successfully provisioned a workspace.\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/upgrading.html",
	"title": "Upgrading",
	"tags": [],
	"description": "",
	"content": "Prerequisites In order to upgrade these tools you will need:\n to have installed the tools  Task list  Service Catalog Factory   Service Catalog Puppet   "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/2021-workshop/200-provisioning-a-vpc/20-creating-the-stack.html",
	"title": "Creating the Stack",
	"tags": [],
	"description": "",
	"content": " What are we going to do? We are going to perform the following steps:\n Define a stack Add the source code for our product  Step by step guide Here are the steps you need to follow to \u0026ldquo;Creating the Stack\u0026rdquo;\nDefine a stack   Navigate to the ServiceCatalogFactory CodeCommit repository\n  Click on stacks\n  Click on network-workshop.yaml\n  Click on edit\n  Append the following snippet to the main input field:\n - Name: \u0026#34;vpc\u0026#34; Versions: - Name: \u0026#34;v1\u0026#34; Active: True Source: Provider: \u0026#34;CodeCommit\u0026#34; Configuration: RepositoryName: \u0026#34;vpc\u0026#34; BranchName: \u0026#34;main\u0026#34;     The full file should look like this:\n Schema: factory-2019-04-01 Stacks: - Name: \u0026#34;delete-default-networking-function\u0026#34; Versions: - Name: \u0026#34;v1\u0026#34; Active: True Source: Provider: \u0026#34;CodeCommit\u0026#34; Configuration: RepositoryName: \u0026#34;delete-default-networking-function\u0026#34; BranchName: \u0026#34;main\u0026#34; - Name: \u0026#34;vpc\u0026#34; Versions: - Name: \u0026#34;v1\u0026#34; Active: True Source: Provider: \u0026#34;CodeCommit\u0026#34; Configuration: RepositoryName: \u0026#34;vpc\u0026#34; BranchName: \u0026#34;main\u0026#34;     Set your Author name\n  Set your Email address\n  Set your Commit message\n  Using a good / unique commit message will help you understand what is going on later.\n  Click the Commit changes button:    What did we just do? The YAML file we created in the CodeCommit repository told the framework to:\n create a pipeline that will take source code from a branch named main of CodeCommit repo named vpc  Verify the change worked Once you have made your changes the ServiceCatalogFactory Pipeline should have run. If you were very quick in making the change, the pipeline may still be running. If it has not yet started feel free to the hit the Release change button.\nOnce it has completed it should show the Source and Build stages in green to indicate they have completed successfully:\n  The screenshots may differ slightly as the design of AWS CodePipeline changes. You should see a pipeline where each stage is green.\n Add the source code for our product When you configured your product version, you specified the following version:\n Versions: - Name: \u0026#34;v1\u0026#34; Active: True Source: Provider: \u0026#34;CodeCommit\u0026#34; Configuration: RepositoryName: \u0026#34;vpc\u0026#34; BranchName: \u0026#34;main\u0026#34;   This tells the framework the source code for the product comes from the main branch of a CodeCommit repository of the name vpc.\nWe now need to create the CodeCommit repository and add the AWS CloudFormation template we are going to use for our product.\n  Navigate to AWS CodeCommit\n  Click Create repository\n     Input the name vpc     Click Create     Scroll down to the bottom of the page and hit the Create file button     Copy the following snippet into the main input field:   AWSTemplateFormatVersion: \u0026#39;2010-09-09\u0026#39; Description: | Builds out a VPC for use Parameters: VPCCIDR: Type: String Default: \u0026#39;10.0.0.0/16\u0026#39; Description: | Subnet to use for the VPC Resources: VPC: Type: AWS::EC2::VPC Description: The vpc being created Properties: EnableDnsSupport: true EnableDnsHostnames: true CidrBlock: !Ref VPCCIDR Outputs: VPCId: Description: The ID of the VPC that was created Value: !Ref VPC     Set the File name to stack.template.yaml\n  Set your Author name\n  Set your Email address\n  Set your Commit message\n  Using a good / unique commit message will help you understand what is going on later.\n Creating that file should trigger your stack\u0026ndash;vpc-v1-pipeline.\nOnce the pipeline has completed it should show the stages in green to indicate they have completed successfully:\n  You should see your commit message on this screen, it will help you know which version of ServiceCatalogFactory repository the pipeline is processing.\n The screenshots may differ slightly as the design of AWS CodePipeline changes. You should see a pipeline where each stage is green.\n You have now successfully created a stack!\nVerify the stack is present in Amazon S3 Now that you have verified the pipeline has run correctly you can go to Amazon S3 to view the stack.\n  Navigate to https://s3.console.aws.amazon.com/s3/home\n  Select the bucket named sc-puppet-stacks-repository-\u0026lt;account_id\u0026gt;\n  Navigate to stack/vpc/v1 where you should see an object named stack.template.yaml\n  "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/2021-workshop/100-removing-the-default-networking/30-deploying-the-lambda.html",
	"title": "Deploying the Lambda",
	"tags": [],
	"description": "",
	"content": " What are we going to do? We are going to perform the following steps:\n Create a manifest file with our account in it Provision the stack delete-default-networking-function into a spoke account  Step by step guide Here are the steps you need to follow to provision the stack.\nCreate a manifest file with our account in it   Navigate to the ServiceCatalogPuppet CodeCommit repository\n  Scroll down to the bottom of the page and hit the Create file button\n     For the next step you will need to know your account id. To find your account id you can check the console, in the top right drop down. It is a 12 digit number. When using your account id please do not include the hyphens ('-') and do not use the angle brackets (\u0026lsquo;\u0026lt;\u0026rsquo;,\u0026lsquo;\u0026gt;\u0026rsquo;)    The screenshots may differ slightly as the design of AWS CodePipeline changes. You should see a pipeline where each stage is green.\n  Copy the following snippet into the main input field and replace account_id to show your account id on the highlighted line:   accounts: - account_id: \u0026#34;\u0026lt;YOUR_ACCOUNT_ID_WITHOUT_HYPHENS\u0026gt;\u0026#34;  name: \u0026#34;puppet-account\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34;   it should look like the following - but with your account id on the highlighted line:\n accounts: - account_id: \u0026#34;012345678910\u0026#34;  name: \u0026#34;puppet-account\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34;   Provision the stack delete-default-networking-function into a spoke account   Append the following snippet to the end of the main input field:\n stacks: delete-default-networking-function: name: \u0026#34;delete-default-networking-function\u0026#34; version: \u0026#34;v1\u0026#34; capabilities: - CAPABILITY_NAMED_IAM deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;     The main input field should look like this (remember to set your account_id):\n   accounts: - account_id: \u0026#34;\u0026lt;YOUR_ACCOUNT_ID_WITHOUT_HYPHENS\u0026gt;\u0026#34; name: \u0026#34;puppet-account\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34; stacks: delete-default-networking-function: name: \u0026#34;delete-default-networking-function\u0026#34; version: \u0026#34;v1\u0026#34; capabilities: - CAPABILITY_NAMED_IAM deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;   Committing the manifest file Now that we have written the manifest file we are ready to commit it.\n  Set the File name to manifest.yaml\n  Set your Author name\n  Set your Email address\n  Set your Commit message\n  Using a good / unique commit message will help you understand what is going on later.\n  Click the Commit changes button:    What did we just do? When you added the following:\n stacks: delete-default-networking-function: name: \u0026#34;delete-default-networking-function\u0026#34; version: \u0026#34;v1\u0026#34; capabilities: - CAPABILITY_NAMED_IAM deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;   You told the framework to provision v1 of delete-default-networking-function into the default region of each account that has the tag type:prod\n accounts: - account_id: \u0026#34;\u0026lt;YOUR_ACCOUNT_ID_WITHOUT_HYPHENS\u0026gt;\u0026#34; name: \u0026#34;puppet-account\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34;   Verifying the provisioned stack Once you have made your changes the ServiceCatalogPuppet Pipeline should have run. If you were quick in making the change, the pipeline may still be running. If it has not yet started feel free to the hit the Release change button.\nOnce it has completed it should show the stages in green to indicate they have completed successfully:\n  The screenshots may differ slightly as the design of AWS CodePipeline changes. You should see a pipeline where each stage is green.\n Once you have verified the pipeline has run you can go to the AWS CloudFormation console in the default region of the account you specified to view your provisioned stack.\nYou have now successfully provisioned a stack.\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/installation/30-service-catalog-puppet/30-installing-puppet.html",
	"title": "Installing Puppet",
	"tags": [],
	"description": "",
	"content": "Navigate to CloudFormation  Select the AWS CloudFormation service.    Create a new AWS CloudFormation stack  Select \u0026lsquo;Create Stack\u0026rsquo;    Note you must have installed factory into this account already. If you have not done this already navigate to \u0026lsquo;Install Factory Process\u0026rsquo;\n Select the pre-configured AWS CloudFormation template Service Catalog Puppet can be installed via a pre-created AWS CloudFormation template stored in Amazon S3 under the following URL:\n https://service-catalog-tools.s3.eu-west-2.amazonaws.com/puppet/latest/servicecatalog-puppet-initialiser.template.yaml\n  Paste this URL under \u0026lsquo;Amazon S3 URL\u0026rsquo;: Hit Next    Specify AWS CloudFormation stack details You will need to fill in the parameters for the template. We recommend the Stack Name: of puppet-initialization-stack\nYou should fill in the details depending on which source code management system you want to use:\nCodeCommit You need to set SCMSourceProvider to CodeCommit.\nYou should also set the following:\n SCMRepositoryName - this is the name of the git repo to use SCMBranchName - this is the branch you want to use SCMShouldCreateRepo - set this to true if you want the tools to create the repo for you  Github.com / Github Enterprise / Bitbucket Cloud You should have a read through the following guide: https://docs.aws.amazon.com/dtconsole/latest/userguide/connections.html\nYou need to set SCMSourceProvider to CodeStarSourceConnection.\nYou should also set the following:\n SCMConnectionArn - this is the Arn of the connection you created in the console SCMFullRepositoryId - the value for this is dependant on which SCM you use SCMBranchName - this is the branch you want to use  S3 You need to set SCMSourceProvider to S3.\nYou should also set the following:\n SCMBucketName - this is the name of the S3 bucket you want to use SCMObjectKey - this is the name of the object key you will be uploading your zip file as to trigger pipeline runs SCMShouldCreateRepo - set this to true if you want the tools to create the repo for you  Continuing Once you have done this, hit Next\n  Create the AWS CloudFormation stack  Leave Defaults for \u0026lsquo;Configure Stack Options\u0026rsquo; Hit Next Acknowledge that the Stack will create an IAM Role Hit \u0026lsquo;Create Stack\u0026rsquo;     You will now see the stack status as CREATE_IN_PROGRESS     Wait for the stack status to go to CREATE_COMPLETE    What have we deployed? The following AWS resources have just been deployed into your AWS Account:\nAWS CloudFormation stacks The AWS CodeBuild job created two AWS CloudFormation stacks which in turn deployed the resources listed below\n URL: https://eu-west-1.console.aws.amazon.com/cloudformation/home?region=eu-west-1\n   Puppet AWS CodeCommit repository This respository holds the Service Catalog Puppet manifest YAML file which is used to configure provisioning and sharing.\n URL: https://eu-west-1.console.aws.amazon.com/codesuite/codecommit/repositories?region=eu-west-1\n   Puppet AWS CodePipeline This AWS CodePipeline is triggered by updates to the AWS CodeCommit repository. When run, it will create the Service Catalog portfolios and products defined in the portfolio files.\n URL: https://eu-west-1.console.aws.amazon.com/codesuite/codepipeline/pipelines?region=eu-west-1\n   Amazon S3 buckets Three Amazon S3 buckets were created to store artifacts for Service Catalog Puppet.\n URL: https://s3.console.aws.amazon.com/s3/home?region=eu-west-1\n   "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/40-reinvent2019/30-prerequisites.html",
	"title": "Pre-requisites",
	"tags": [],
	"description": "",
	"content": "In order to complete this workshop you will need:\n A single AWS Account which you can log into A web browser to access the AWS console  If you are taking this workshop at re:Invent 2019 you should have been given a note when you entered the workshop. This note contains all you need to log into a AWS account we have created for you.\nWe have installed the tools needed for you to get going.\n If you want to run through this workshop in your own account please check the next section titled running-yourself.\n  Running yourself   "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/2021-workshop/200-provisioning-a-vpc/30-provisioning-the-stack.html",
	"title": "Provisioning the Stack",
	"tags": [],
	"description": "",
	"content": " What are we going to do? We are going to perform the following steps:\n Update the manifest file  Step by step guide Here are the steps you need to follow to provision the stack.\nUpdate the manifest file   Navigate to the ServiceCatalogPuppet CodeCommit repository again\n  Click on manifest.yaml\n  Click Edit\n  Append the following snippet to the end of the stacks section in the input field:\n vpc: name: \u0026#34;vpc\u0026#34; version: \u0026#34;v1\u0026#34; deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;     The main input field should look like this (remember to set your account_id):\n   accounts: - account_id: \u0026#34;\u0026lt;YOUR_ACCOUNT_ID_WITHOUT_HYPHENS\u0026gt;\u0026#34; name: \u0026#34;puppet-account\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34; stacks: delete-default-networking-function: name: \u0026#34;delete-default-networking-function\u0026#34; version: \u0026#34;v1\u0026#34; capabilities: - CAPABILITY_NAMED_IAM deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34; vpc: name: \u0026#34;vpc\u0026#34; version: \u0026#34;v1\u0026#34; depends_on: - name: \u0026#34;delete-default-networking\u0026#34; type: \u0026#34;lambda-invocation\u0026#34; affinity: \u0026#34;lambda-invocation\u0026#34; deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34; outputs: ssm: - param_name: \u0026#34;/networking/vpc/account-parameters/${AWS::AccountId}/${AWS::Region}/VPCId\u0026#34; stack_output: VPCId lambda-invocations: delete-default-networking: function_name: DeleteDefaultNetworking qualifier: $LATEST invocation_type: Event depends_on: - name: \u0026#34;delete-default-networking-function\u0026#34; type: \u0026#34;stack\u0026#34; affinity: \u0026#34;stack\u0026#34; invoke_for: tags: - regions: \u0026#34;default_region\u0026#34; tag: \u0026#34;type:prod\u0026#34; assertions: assert-no-default-vpcs: expected: source: manifest config: value: - \u0026#34;\u0026#34; actual: source: boto3 config: client: \u0026#39;ec2\u0026#39; call: describe_vpcs arguments: {} use_paginator: true filter: Vpcs[?IsDefault==`true`].State depends_on: - name: \u0026#34;delete-default-networking\u0026#34; type: \u0026#34;lambda-invocation\u0026#34; affinity: \u0026#34;lambda-invocation\u0026#34; assert_for: tags: - regions: regions_enabled tag: type:prod   Committing the manifest file Now that we have updated the manifest file we are ready to commit it.\n Set your Author name Set your Email address Set your Commit message  Using a good / unique commit message will help you understand what is going on later.\n  Click the Commit changes button:    What did we just do? When you added the following:\n vpc: name: \u0026#34;vpc\u0026#34; version: \u0026#34;v1\u0026#34; depends_on: - name: \u0026#34;delete-default-networking\u0026#34; type: \u0026#34;lambda-invocation\u0026#34; affinity: \u0026#34;lambda-invocation\u0026#34; deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;   You told the framework to provision v1 of vpc into the default region of each account that has the tag type:prod\nVerifying the provisioned stack Once you have made your changes the ServiceCatalogPuppet Pipeline should have run. If you were quick in making the change, the pipeline may still be running. If it has not yet started feel free to the hit the Release change button.\nOnce it has completed it should show the stages in green to indicate they have completed successfully:\n  The screenshots may differ slightly as the design of AWS CodePipeline changes. You should see a pipeline where each stage is green.\n Once you have verified the pipeline has run you can go to the AWS CloudFormation console in the default region of the account you specified to view your provisioned stack.\nYou have now successfully provisioned a stack.\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/installation/30-service-catalog-puppet.html",
	"title": "Service Catalog Puppet",
	"tags": [],
	"description": "",
	"content": "Service Catalog Puppet does not depend on AWS Organizations.\nUsing AWS Organizations provides performance and productivity improvements but it is optional. If you would like to use it please follow the \u0026ldquo;Using AWS Organizations\u0026rdquo; step below before you continue with the \u0026ldquo;Installing Puppet\u0026rdquo; step.\n Using AWS Organizations   Installing Puppet   "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/upgrading/service-catalog-puppet.html",
	"title": "Service Catalog Puppet",
	"tags": [],
	"description": "",
	"content": "Before you update This guide applies to all users who installed using the AWS CloudFormation template. If you did not install using the template then you need to follow the install guide. When following the install guide, ensure you provision the initialiser stack into the same region you installed the tools using the CLI method. Installing this way will perform an update and will not break your install.\nIf you have added any regions since your initial install or modified any settings provided by the parameters in the install template ensure you specify the values you want for them as the install process will overwrite any previous settings you configured in your existing install.\nNavigate to CloudFormation  Select the AWS CloudFormation Service.      Select the initialization stack you created when installing Service Catalog Factory. The recommended stack name was puppet-initialization-stack\n  Select \u0026lsquo;Update\u0026rsquo;\n  Select \u0026lsquo;Replace current template\u0026rsquo;\n  For Template source select \u0026lsquo;Amazon S3 URL\u0026rsquo; and paste in the following: https://service-catalog-tools.s3.eu-west-2.amazonaws.com/puppet/latest/servicecatalog-puppet-initialiser.template.yaml\n  Then hit \u0026lsquo;Next\u0026rsquo;\n  Make any changes you want to the parameters you provided and then hit \u0026lsquo;Next\u0026rsquo; again\n  Follow the rest of the steps to update the stack\n  As the stack updates the AWS CodeBuild project named servicecatalog-product-puppet-initialiser is started. Once it is completed the stack update will complete. The project will update your installation to the latest version\n  "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/every-day-use.html",
	"title": "Every day use",
	"tags": [],
	"description": "",
	"content": "Welcome builders From here you can see a list of our how to articles\nYou can use the left and right arrows to navigate\n  Creating a product   Deleting a product   Creating a portfolio   Adding a product to a portfolio   Creating a manifest   Adding an account   Provisioning a product   Sharing a portfolio   Adding a region   AWS Organizations Integration   Provisioning CloudFormation   Provisioning a Stack   Invoking a Lambda Function   Starting a CodeBuild project   Using assertions   "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/2021-workshop/100-removing-the-default-networking/40-invoking-the-lambda.html",
	"title": "Invoking the Lambda",
	"tags": [],
	"description": "",
	"content": " What are we going to do? We are going to perform the following steps:\n Invoke the lambda  Step by step guide Here are the steps you need to follow to \u0026ldquo;Invoking the Lambda\u0026rdquo;\nInvoke the lambda   Navigate to the ServiceCatalogPuppet CodeCommit repository again\n  Click on manifest.yaml\n  Click Edit\n  Append the following snippet to the end of the main input field:\n   lambda-invocations: delete-default-networking: function_name: DeleteDefaultNetworking qualifier: $LATEST invocation_type: Event invoke_for: tags: - regions: \u0026#34;default_region\u0026#34; tag: \u0026#34;type:prod\u0026#34;    The main input field should look like this (remember to set your account_id):   accounts: - account_id: \u0026#34;\u0026lt;YOUR_ACCOUNT_ID_WITHOUT_HYPHENS\u0026gt;\u0026#34; name: \u0026#34;puppet-account\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34; stacks: delete-default-networking-function: name: \u0026#34;delete-default-networking-function\u0026#34; version: \u0026#34;v1\u0026#34; capabilities: - CAPABILITY_NAMED_IAM deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34; lambda-invocations: delete-default-networking: function_name: DeleteDefaultNetworking qualifier: $LATEST invocation_type: Event depends_on: - name: \u0026#34;delete-default-networking-function\u0026#34; type: \u0026#34;stack\u0026#34; affinity: \u0026#34;stack\u0026#34; invoke_for: tags: - regions: \u0026#34;default_region\u0026#34; tag: \u0026#34;type:prod\u0026#34;   AWS Committing the manifest file Now that we have written the manifest file we are ready to commit it.\n Set your Author name Set your Email address Set your Commit message  Using a good / unique commit message will help you understand what is going on later.\n  Click the Commit changes button:    What did we just do? The YAML we pasted in the previous step told the framework to perform the following actions:\n Invoke a lambda in the hub account named DeleteDefaultNetworking. It will be invoked each time with parameters account_id and region. It will be invoked one time for each account tagged type:prod using the account_id and default region specified for it.  Verifying the provisioning Once the pipeline has completed you can verify the lambda was invoked by verifying there is no default VPC in the default region of your account or you can check the AWS CloudWatch logs for the AWS Lambda function or you could check the execution history for the Lambda function.\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/2021-workshop/100-removing-the-default-networking/50-asserting-the-resources-are-removed.html",
	"title": "Asserting the resources are removed",
	"tags": [],
	"description": "",
	"content": " What are we going to do? We are going to perform the following steps:\n Define an assertion  Step by step guide Here are the steps you need to follow to \u0026ldquo;Asserting the resources are removed\u0026rdquo;\nDefine an assertion   Navigate to the ServiceCatalogPuppet CodeCommit repository again\n  Click on manifest.yaml\n  Click Edit\n  Append the following snippet to the end of the main input field:\n   assertions: assert-no-default-vpcs: expected: source: manifest config: value: - \u0026#34;\u0026#34; actual: source: boto3 config: client: \u0026#39;ec2\u0026#39; call: describe_vpcs arguments: {} use_paginator: true filter: Vpcs[?IsDefault==`true`].State assert_for: tags: - regions: regions_enabled tag: type:prod    The main input field should look like this (remember to set your account_id):   accounts: - account_id: \u0026#34;\u0026lt;YOUR_ACCOUNT_ID_WITHOUT_HYPHENS\u0026gt;\u0026#34; name: \u0026#34;puppet-account\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34; stacks: delete-default-networking-function: name: \u0026#34;delete-default-networking-function\u0026#34; version: \u0026#34;v1\u0026#34; capabilities: - CAPABILITY_NAMED_IAM deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34; lambda-invocations: delete-default-networking: function_name: DeleteDefaultNetworking qualifier: $LATEST invocation_type: Event depends_on: - name: \u0026#34;delete-default-networking-function\u0026#34; type: \u0026#34;stack\u0026#34; affinity: \u0026#34;stack\u0026#34; invoke_for: tags: - regions: \u0026#34;default_region\u0026#34; tag: \u0026#34;type:prod\u0026#34; assertions: assert-no-default-vpcs: expected: source: manifest config: value: - \u0026#34;\u0026#34; actual: source: boto3 config: client: \u0026#39;ec2\u0026#39; call: describe_vpcs arguments: {} use_paginator: true filter: Vpcs[?IsDefault==`true`].State depends_on: - name: \u0026#34;delete-default-networking\u0026#34; type: \u0026#34;lambda-invocation\u0026#34; affinity: \u0026#34;lambda-invocation\u0026#34; assert_for: tags: - regions: regions_enabled tag: type:prod   AWS Committing the manifest file Now that we have written the manifest file we are ready to commit it.\n Set your Author name Set your Email address Set your Commit message  Using a good / unique commit message will help you understand what is going on later.\n  Click the Commit changes button:    What did we just do? The YAML we pasted in the previous step told the framework to use the ec2 client from boto3 to describe vpcs. We told the framework not to use any arguments when invoking describe vpcs and we told the framework to use a paginator ensuring we find every page of results. We then used a filter to include only VPCs where the IsDefault attribute is set to true\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/installation/50-bootstrapping/50-using-the-cli.html",
	"title": "Command Line Interface",
	"tags": [],
	"description": "",
	"content": "What are we going to do? You will need to bootstrap spoke accounts so you can configure them using the Service Catalog Tools.\nBootstrapping a spoke account will create an AWS CloudFormation stack in it. This stack will contain the Puppet IAM Role (PuppetRole) which is needed by framework to perform actions in the spoke account.\nThe following steps should be executed using the Service Catalog Puppet CLI which is an application built using Python 3.7.\nIf you have not already installed the framework you can do so by following these steps:\nInstalling It is good practice to install Python libraries in isolated environments.\nYou can create the a virtual environment using the following command:\nvirtualenv --python=python3.7 venv source venv/bin/activate Once you have decided where to install the library you can install the package:\npip install aws-service-catalog-puppet This will install the library and all of the dependencies.\nBootstrapping a spoke You should export the credentials for the spoke account or set your profile so that AWS CLI commands will execute as a role in the spoke account.\nThen you can run the following command:\nWithout a permission boundary servicecatalog-puppet bootstrap-spoke \u0026lt;ACCOUNT_ID_OF_YOUR_PUPPET\u0026gt;\nWith a permission boundary servicecatalog-puppet bootstrap-spoke \u0026lt;ACCOUNT_ID_OF_YOUR_PUPPET\u0026gt; --permission-boundary arn:aws:iam::aws:policy/AdministratorAccess\nEnsure you replace \u0026lt;ACCOUNT_ID_OF_YOUR_PUPPET\u0026gt; with the AWS Account id of the AWS Account you will be using as your puppet account.\nBootstrapping a spoke as If you want to assume a role into the spoke from your currently active role you can use the following command.\nWithout a boundary servicecatalog-puppet bootstrap-spoke-as \u0026lt;ACCOUNT_ID_OF_YOUR_PUPPET\u0026gt; \u0026lt;ARN_OF_ASSUMABLE_ROLE_IN_SPOKE\u0026gt;\nWith a boundary servicecatalog-puppet bootstrap-spoke-as \u0026lt;ACCOUNT_ID_OF_YOUR_PUPPET\u0026gt; \u0026lt;ARN_OF_ASSUMABLE_ROLE_IN_SPOKE\u0026gt; --permission-boundary arn:aws:iam::aws:policy/AdministratorAccess\nThis will assume the role \u0026lt;ARN_OF_ASSUMABLE_ROLE_IN_SPOKE\u0026gt; before running boostrap-spoke. This is useful if you do not want to perform the AWS STS assume-role yourself.\nEnsure you replace \u0026lt;ACCOUNT_ID_OF_YOUR_PUPPET\u0026gt; with the account id of the account you will be using as your puppet account.\nYou can use the following AWS CloudFormation template to provision the needed role:\n# Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. # SPDX-License-Identifier: Apache-2.0 AWSTemplateFormatVersion: \u0026#39;2010-09-09\u0026#39; Description: IAM Role needed to use AWS Organizations to assume role into member AWS Accounts. Parameters: ServiceCatalogFactoryAccountId: Description: The account you will be installing AWS Service Catalog Factory into Type: String OrganizationAccountAccessRole: Description: Name of the IAM role used to access cross accounts for AWS Orgs usage Default: OrganizationAccountAccessRole Type: String Resources: RoleForBootstrappingSpokes: Type: AWS::IAM::Role Description: | IAM Role needed by the account vending machine so it can create and move accounts Properties: Path: /servicecatalog-puppet/ Policies: - PolicyName: Organizations PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: - sts:AssumeRole Resource: !Sub \u0026#34;arn:aws:iam::*:role/${OrganizationAccountAccessRole}\u0026#34; AssumeRolePolicyDocument: Version: \u0026#34;2012-10-17\u0026#34; Statement: - Effect: \u0026#34;Allow\u0026#34; Principal: AWS: !Sub \u0026#34;arn:aws:iam::${ServiceCatalogFactoryAccountId}:root\u0026#34; Action: - \u0026#34;sts:AssumeRole\u0026#34; Outputs: RoleForBootstrappingSpokesArn: Description: The ARN for your Assumable role in root account Value: !GetAtt RoleForBootstrappingSpokes.Arn Bootstrapping spokes in OU You should export the credentials for the account that allows you to list accounts in the org and assume an IAM Role in each of the spokes.\nThen you can run the following command:\nWithout a Permission Boundary servicecatalog-puppet bootstrap-spokes-in-ou /dev DevOpsAdminRole\nIn this example /dev is the ou path and DevOpsAdminRole is the name of the assumable role in each spoke account.\nWith a Permission Boundary servicecatalog-puppet bootstrap-spokes-in-ou /dev DevOpsAdminRole --permission-boundary arn:aws:iam::aws:policy/AdministratorAccess\nIf your current role does not allow you to list accounts in the AWS Organization or allow you to assume-role across AWS accounts you can specify an ARN of an IAM role that does. When you do so the framework will assume that IAM Role first and then perform the bootstrapping.\nservicecatalog-puppet bootstrap-spokes-in-ou /dev DevOpsAdminRole arn:aws:iam::0123456789010:role/OrgRoleThatAllowsListAndAssumeRole You can use the following AWS CloudFormation template to provision the needed role:\n# Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. # SPDX-License-Identifier: Apache-2.0 AWSTemplateFormatVersion: \u0026#39;2010-09-09\u0026#39; Description: IAM Role needed to use AWS Organizations to assume role into member AWS Accounts. Parameters: ServiceCatalogFactoryAccountId: Description: The account you will be installing AWS Service Catalog Factory into Type: String OrganizationAccountAccessRole: Description: Name of the IAM role used to access cross accounts for AWS Orgs usage Default: OrganizationAccountAccessRole Type: String Resources: RoleForBootstrappingSpokes: Type: AWS::IAM::Role Description: | IAM Role needed by the account vending machine so it can create and move accounts Properties: Path: /servicecatalog-puppet/ Policies: - PolicyName: Organizations PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: - sts:AssumeRole Resource: !Sub \u0026#34;arn:aws:iam::*:role/${OrganizationAccountAccessRole}\u0026#34; AssumeRolePolicyDocument: Version: \u0026#34;2012-10-17\u0026#34; Statement: - Effect: \u0026#34;Allow\u0026#34; Principal: AWS: !Sub \u0026#34;arn:aws:iam::${ServiceCatalogFactoryAccountId}:root\u0026#34; Action: - \u0026#34;sts:AssumeRole\u0026#34; Outputs: RoleForBootstrappingSpokesArn: Description: The ARN for your Assumable role in root account Value: !GetAtt RoleForBootstrappingSpokes.Arn "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/design-considerations/multi-account-strategy.html",
	"title": "Multi Account Strategy",
	"tags": [],
	"description": "",
	"content": " What are we going to do? This article will help you align your multi-account strategy with best practices.\nAWS Organizations AWS Organizations helps you centrally govern your environment as you grow and scale your workloads on AWS. Whether you are a growing startup or a large enterprise, AWS Organizations helps you to centrally manage billing; control access, compliance, and security; and share resources across your AWS accounts.\nUsing AWS Organizations, you can automate account creation, create groups of accounts to reflect your business needs, and apply policies for these groups for governance. You can also simplify billing by setting up a single payment method for all of your AWS accounts. Through integrations with other AWS services, you can use AWS Organizations to define central configurations and resource sharing across accounts in your organization. AWS Organizations is available to all AWS customers at no additional charge.\nGetting started You can read through the following pages to understand best practices:\n Starter framework   "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/installation/50-bootstrapping.html",
	"title": "Onboarding spokes",
	"tags": [],
	"description": "",
	"content": "As part of the installation process of installing puppet into your hub account that account is bootstrapped as a spoke so you can use it via automation.\nIf you want to configure other accounts using the Service Catalog Tools you will need to bootstrap those too.\nYou can use the following options to bootstrap accounts:\n CloudFormation/CodeBuild   Command Line Interface   Restricting Spokes   "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/installation/50-bootstrapping/80-restricting-spokes.html",
	"title": "Restricting Spokes",
	"tags": [],
	"description": "",
	"content": "Restricting spokes The PuppetRole created by the framework has the AdministratorAccess IAM managed policy attached to it. It is reccommended that you can define an IAM Permission Boundary for the PuppetRole for any production applications of this framework.\nThe IAM Permission Boundary you provide should permit the PuppetRole to interact with AWS Service Catalog to accept shares, manage portfolios and to add, provision and terminate products. In addition the IAM Role should allow the use of AWS SNS, AWS EventBridge, AWS OpsCenter if you are making use of those features.\nIn order to use an IAM Permission Boundary you will need to append the following to your commands:\n--permission-boundary arn:aws:iam::aws:policy/AdministratorAccess There will be an example of this for each command in these how tos.\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/40-reinvent2019/100-task-1.html",
	"title": "Budget &amp; Cost governance",
	"tags": [],
	"description": "",
	"content": " The ask Cloud usage within Example Corp has picked up significantly and a number of teams are now using EC2 instances in innovative ways. The customer has noticed that teams are often not sure which EC2 instance types to use for their applications and this is leading to underutilized EC2 instances that are run on-demand. To bring down costs, the customer has purchased a set of EC2 reserved instances, based on common workload profiles, and we need to ensure the teams are using them for long running applications.\nTo help the customer, we will design and then deploy a control that gives them visibility into which EC2 instance types are being used within an AWS account.\nThe plan We are going to create and deploy a governance control using an AWS Config managed rule to ensure the right instance types are being used.\nYou can follow these steps to do this:\n Create the control   Provision the control   Future work will involve mandating that teams use only approved instance types. To start, we will gather data via AWS Config.\nIf you need help at any time please raise your hand.\n "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/40-reinvent2019/100-task-1/100-create-the-control.html",
	"title": "Create the control",
	"tags": [],
	"description": "",
	"content": " What are we going to do? We are going to perform the following steps:\n define a product with a version and a portfolio in a hub account add the source code for the product provision that product into a spoke account  The hub AWS Account is the source of truth for our AWS Service Catalog products. Spoke AWS accounts are consumers of these products, you can think of them as accounts that need governance controls applied. For this workshop, we are using the same account as both the hub and spoke for simplicity; in a multi-account setup, these could be separate AWS Accounts and Regions.\nStep by step guide Here are the steps you need to follow to \u0026ldquo;Create the control\u0026rdquo;\nDefine a product with a version and a portfolio   Navigate to the ServiceCatalogFactory CodeCommit repository\n  Scroll down to the bottom of the page and hit the Create file button\n      Copy the following snippet into the main input field:\n Schema: factory-2019-04-01 Products: - Name: \u0026#34;aws-config-desired-instance-types\u0026#34; Owner: \u0026#34;budget-and-cost-governance@example.com\u0026#34; Description: \u0026#34;Enables AWS Config rule - desired-instance-type with our RIs\u0026#34; Distributor: \u0026#34;cloud-engineering\u0026#34; SupportDescription: \u0026#34;Speak to budget-and-cost-governance@example.com about exceptions and speak to cloud-engineering@example.com about implementation issues\u0026#34; SupportEmail: \u0026#34;cloud-engineering@example.com\u0026#34; SupportUrl: \u0026#34;https://wiki.example.com/cloud-engineering/budget-and-cost-governance/aws-config-desired-instance-types\u0026#34; Tags: - Key: \u0026#34;type\u0026#34; Value: \u0026#34;governance\u0026#34; - Key: \u0026#34;creator\u0026#34; Value: \u0026#34;cloud-engineering\u0026#34; - Key: \u0026#34;cost-center\u0026#34; Value: \u0026#34;governance\u0026#34; Versions: - Name: \u0026#34;v1\u0026#34; Description: \u0026#34;v1 of aws-config-desired-instance-types\u0026#34; Active: True Source: Provider: \u0026#34;CodeCommit\u0026#34; Configuration: RepositoryName: \u0026#34;aws-config-desired-instance-types\u0026#34; BranchName: \u0026#34;main\u0026#34; Portfolios: - \u0026#34;cloud-engineering-governance\u0026#34; Portfolios: - DisplayName: \u0026#34;cloud-engineering-governance\u0026#34; Description: \u0026#34;Portfolio containing the products needed to govern AWS accounts\u0026#34; ProviderName: \u0026#34;cloud-engineering\u0026#34; Associations: - \u0026#34;arn:aws:iam::${AWS::AccountId}:role/TeamRole\u0026#34; Tags: - Key: \u0026#34;type\u0026#34; Value: \u0026#34;governance\u0026#34; - Key: \u0026#34;creator\u0026#34; Value: \u0026#34;cloud-engineering\u0026#34; - Key: \u0026#34;cost-center\u0026#34; Value: \u0026#34;governance\u0026#34;     Set the File name to portfolios/reinvent.yaml\n  Set your Author name\n  Set your Email address\n  Set your Commit message\n  Using a good / unique commit message will help you understand what is going on later.\n  Click the Commit changes button:    What did we just do? The YAML file we created in the CodeCommit repository told the framework to perform several actions:\n create a product named aws-config-desired-instance-types add a v1 of our product create a portfolio named cloud-engineering-governance add the product: aws-config-desired-instance-types to the portfolio: cloud-engineering-governance  Verify the change worked Once you have made your changes the ServiceCatalogFactory Pipeline should have run. If you were very quick in making the change, the pipeline may still be running. If it has not yet started feel free to the hit the Release change button.\nOnce it has completed it should show the Source and Build stages in green to indicate they have completed successfully:\n  If this is failing please raise your hand for some assistance\n Add the source code for our product When you configured your product version, you specified the following version:\n Versions: - Name: \u0026#34;v1\u0026#34; Description: \u0026#34;v1 of aws-config-desired-instance-types\u0026#34; Active: True Source: Provider: \u0026#34;CodeCommit\u0026#34; Configuration: RepositoryName: \u0026#34;aws-config-desired-instance-types\u0026#34; BranchName: \u0026#34;main\u0026#34;   This tells the framework the source code for the product comes from the main branch of a CodeCommit repository of the name aws-config-desired-instance-types.\nWe now need to create the CodeCommit repository and add the AWS CloudFormation template we are going to use for our product.\n  Navigate to AWS CodeCommit\n  Click Create repository\n     Input the name aws-config-desired-instance-types     Click Create     Scroll down to the bottom of the page and hit the Create file button     Copy the following snippet into the main input field:   AWSTemplateFormatVersion: \u0026#34;2010-09-09\u0026#34; Description: \u0026#34;Create an AWS Config rule ensuring the given instance types are the only instance types used\u0026#34; Parameters: InstanceType: Type: String Description: \u0026#34;Comma separated list of EC2 instance types (for example, \u0026#39;t2.small, m4.large\u0026#39;).\u0026#34; Default: \u0026#34;t2.micro, t2.small\u0026#34; Resources: AWSConfigRule: Type: AWS::Config::ConfigRule Properties: ConfigRuleName: \u0026#34;desired-instance-type\u0026#34; Description: \u0026#34;Checks whether your EC2 instances are of the specified instance types.\u0026#34; InputParameters: instanceType: !Ref InstanceType Scope: ComplianceResourceTypes: - \u0026#34;AWS::EC2::Instance\u0026#34; Source: Owner: AWS SourceIdentifier: DESIRED_INSTANCE_TYPE     Set the File name to product.template.yaml\n  Set your Author name\n  Set your Email address\n  Set your Commit message\n  Using a good / unique commit message will help you understand what is going on later.\n Creating that file should trigger your aws-config-desired-instance-types-v1-pipeline.\nOnce the pipeline has completed it should show the Source, Tests, Package and Deploy stages in green to indicate they have completed successfully:\n  You should see your commit message on this screen, it will help you know which version of ServiceCatalogFactory repository the pipeline is processing.\n If this is failing please raise your hand for some assistance\n Once you have verified the pipeline has run you can go to Service Catalog products to view your newly created version.\nYou should see the product you created listed:\n  Click on the product and verify v1 is there\n  If you cannot see your version please raise your hand for some assistance\n You have now successfully created a version for your product!\nVerify the product was added to the portfolio Now that you have verified the pipeline has run you can go to Service Catalog portfolios to view your portfolio.\n Click on reinvent-cloud-engineering-governance      Click on the product aws-config-desired-instance-types\n  Click on the version v1\n    "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/40-reinvent2019/150-task-2/100-create-the-control.html",
	"title": "Create the control",
	"tags": [],
	"description": "",
	"content": " What are we going to do? We are going to perform the following steps:\n define another product with a version and add it to the existing cloud-engineering-governance portfolio add the source code for our product provision that product into a spoke account  Step by step guide Here are the steps you need to follow to \u0026ldquo;Create the control\u0026rdquo;\nDefine a product with a version and a portfolio   Navigate to the ServiceCatalogFactory CodeCommit repository again\n  Click on portfolios\n     Click on reinvent.yaml     Click Edit     We will need to insert the following to the products section:   - Name: \u0026#34;aws-config-rds-storage-encrypted\u0026#34; Owner: \u0026#34;data-governance@example.com\u0026#34; Description: \u0026#34;Enables AWS Config rule - aws-config-rds-storage-encrypted\u0026#34; Distributor: \u0026#34;cloud-engineering\u0026#34; SupportDescription: \u0026#34;Speak to data-governance@example.com about exceptions and speak to cloud-engineering@example.com about implementation issues\u0026#34; SupportEmail: \u0026#34;cloud-engineering@example.com\u0026#34; SupportUrl: \u0026#34;https://wiki.example.com/cloud-engineering/data-governance/aws-config-rds-storage-encrypted\u0026#34; Tags: - Key: \u0026#34;type\u0026#34; Value: \u0026#34;governance\u0026#34; - Key: \u0026#34;creator\u0026#34; Value: \u0026#34;cloud-engineering\u0026#34; - Key: \u0026#34;cost-center\u0026#34; Value: \u0026#34;governance\u0026#34; Versions: - Name: \u0026#34;v1\u0026#34; Description: \u0026#34;v1 of aws-config-rds-storage-encrypted\u0026#34; Active: True Source: Provider: \u0026#34;CodeCommit\u0026#34; Configuration: RepositoryName: \u0026#34;aws-config-rds-storage-encrypted\u0026#34; BranchName: \u0026#34;main\u0026#34; Portfolios: - \u0026#34;cloud-engineering-governance\u0026#34;    Once completed it should like look this:   Schema: factory-2019-04-01 Products: - Name: \u0026#34;aws-config-desired-instance-types\u0026#34; Owner: \u0026#34;budget-and-cost-governance@example.com\u0026#34; Description: \u0026#34;Enables AWS Config rule - desired-instance-type with our RIs\u0026#34; Distributor: \u0026#34;cloud-engineering\u0026#34; SupportDescription: \u0026#34;Speak to budget-and-cost-governance@example.com about exceptions and speak to cloud-engineering@example.com about implementation issues\u0026#34; SupportEmail: \u0026#34;cloud-engineering@example.com\u0026#34; SupportUrl: \u0026#34;https://wiki.example.com/cloud-engineering/budget-and-cost-governance/aws-config-desired-instance-types\u0026#34; Tags: - Key: \u0026#34;type\u0026#34; Value: \u0026#34;governance\u0026#34; - Key: \u0026#34;creator\u0026#34; Value: \u0026#34;cloud-engineering\u0026#34; - Key: \u0026#34;cost-center\u0026#34; Value: \u0026#34;governance\u0026#34; Versions: - Name: \u0026#34;v1\u0026#34; Description: \u0026#34;v1 of aws-config-desired-instance-types\u0026#34; Active: True Source: Provider: \u0026#34;CodeCommit\u0026#34; Configuration: RepositoryName: \u0026#34;aws-config-desired-instance-types\u0026#34; BranchName: \u0026#34;main\u0026#34; Portfolios: - \u0026#34;cloud-engineering-governance\u0026#34; - Name: \u0026#34;aws-config-rds-storage-encrypted\u0026#34; Owner: \u0026#34;data-governance@example.com\u0026#34; Description: \u0026#34;Enables AWS Config rule - aws-config-rds-storage-encrypted\u0026#34; Distributor: \u0026#34;cloud-engineering\u0026#34; SupportDescription: \u0026#34;Speak to data-governance@example.com about exceptions and speak to cloud-engineering@example.com about implementation issues\u0026#34; SupportEmail: \u0026#34;cloud-engineering@example.com\u0026#34; SupportUrl: \u0026#34;https://wiki.example.com/cloud-engineering/data-governance/aws-config-rds-storage-encrypted\u0026#34; Tags: - Key: \u0026#34;type\u0026#34; Value: \u0026#34;governance\u0026#34; - Key: \u0026#34;creator\u0026#34; Value: \u0026#34;cloud-engineering\u0026#34; - Key: \u0026#34;cost-center\u0026#34; Value: \u0026#34;governance\u0026#34; Versions: - Name: \u0026#34;v1\u0026#34; Description: \u0026#34;v1 of aws-config-rds-storage-encrypted\u0026#34; Active: True Source: Provider: \u0026#34;CodeCommit\u0026#34; Configuration: RepositoryName: \u0026#34;aws-config-rds-storage-encrypted\u0026#34; BranchName: \u0026#34;main\u0026#34; Portfolios: - \u0026#34;cloud-engineering-governance\u0026#34; Portfolios: - DisplayName: \u0026#34;cloud-engineering-governance\u0026#34; Description: \u0026#34;Portfolio containing the products needed to govern AWS accounts\u0026#34; ProviderName: \u0026#34;cloud-engineering\u0026#34; Associations: - \u0026#34;arn:aws:iam::${AWS::AccountId}:role/TeamRole\u0026#34; Tags: - Key: \u0026#34;type\u0026#34; Value: \u0026#34;governance\u0026#34; - Key: \u0026#34;creator\u0026#34; Value: \u0026#34;cloud-engineering\u0026#34; - Key: \u0026#34;cost-center\u0026#34; Value: \u0026#34;governance\u0026#34;    Set your Author name Set your Email address Set your Commit message  Using a good / unique commit message will help you understand what is going on later.\n  Click the Commit changes button:    What did we just do? The YAML we pasted in the previous step told the framework to perform several actions:\n create a product named aws-config-rds-storage-encrypted add a v1 of our product add the product: aws-config-rds-storage-encrypted to the portfolio: cloud-engineering-governance  Verify that the change worked Once you have made your changes the ServiceCatalogFactory Pipeline should have run. If you were very quick, the pipeline may still be running. If it has not yet started feel free to the hit the Release change button.\nOnce it has completed it should show the Source and Build stages in green to indicate they have completed successfully:\n  If this is failing please raise your hand for some assistance\n Add the source code for our product When you configured your product version, you specified the following version:\n Versions: - Name: \u0026#34;v1\u0026#34; Description: \u0026#34;v1 of aws-config-rds-storage-encrypted\u0026#34; Active: True Source: Provider: \u0026#34;CodeCommit\u0026#34; Configuration: RepositoryName: \u0026#34;aws-config-rds-storage-encrypted\u0026#34; BranchName: \u0026#34;main\u0026#34;   This tells the framework the source code for the product comes from the main branch of a CodeCommit repository of the name aws-config-rds-storage-encrypted.\nWe now need to create the CodeCommit repository and add the CloudFormation template we are going to use for our product.\n  Navigate to AWS CodeCommit\n  Click Create repository\n     Input the name aws-config-rds-storage-encrypted     Click Create     Scroll down to the bottom of the page and hit the Create file button     Copy the following snippet into the main input field:   AWSTemplateFormatVersion: \u0026#39;2010-09-09\u0026#39; Description: \u0026#34;Create an AWS Config rule ensuring RDS instances use encrypted storage\u0026#34; Resources: AWSConfigRule: Type: AWS::Config::ConfigRule Properties: ConfigRuleName: \u0026#34;rds-storage-encrypted\u0026#34; Description: \u0026#34;Checks whether storage encryption is enabled for your RDS DB instances.\u0026#34; Scope: ComplianceResourceTypes: - \u0026#34;AWS::RDS::DBInstance\u0026#34; Source: Owner: AWS SourceIdentifier: RDS_STORAGE_ENCRYPTED     Set the File name to product.template.yaml\n  Set your Author name\n  Set your Email address\n  Set your Commit message\n  Using a good / unique commit message will help you understand what is going on later.\n Creating that file should trigger your aws-config-rds-storage-encrypted-v1-pipeline.\nOnce the pipeline has completed it should show the Source, Package, Package and Deploy stages in green to indicate they have completed successfully:\n  You should see your commit message on this screen, it will help you know which version of ServiceCatalogFactory repository the pipeline is processing.\n If this is failing please raise your hand for some assistance\n Once you have verified the pipeline has run you can go to Service Catalog products to view your newly created version.\nYou should see the product you created listed:\n  Click on the product and verify v1 is there\n  If you cannot see your version please raise your hand for some assistance\n You have now successfully created a version for your product!\nVerify the product was added to the portfolio Now that you have verified the pipeline has run you can go to Service Catalog portfolios to view your portfolio.\n Click on reinvent-cloud-engineering-governance      Click on the product aws-config-rds-storage-encrypted\n  Click on the version v1\n    "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/every-day-use/100-creating-a-product.html",
	"title": "Creating a product",
	"tags": [],
	"description": "",
	"content": " This tutorial will walk you through \u0026ldquo;Creating a product\u0026rdquo;\nWe will assume you have:\n installed Service Catalog Factory correctly  In the tutorial you will:\n Define the product   Add the source code   Creating CodeStar pipelines   Creating S3 pipelines   During this process you will check your progress by verifying what the framework is doing.\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/every-day-use/100-creating-a-product/100-define-the-product.html",
	"title": "Define the product",
	"tags": [],
	"description": "",
	"content": " What are we going to do? We are going to perform the following steps:\n create a portfolio file define a product define a version for our product commit our portfolio file verify the framework has create an AWS CodePipeline for our product version  Step by step guide Here are the steps you need to follow to \u0026ldquo;Define the product\u0026rdquo;\nCreate the portfolio file We need to tell the framework that a product exists. We do that by creating a portfolio file and by describing the products details there.\nHere is how we do this:\n Navigate to the ServiceCatalogFactory CodeCommit repository Scroll down to the bottom of the page and hit the Create file button      Copy the following snippet into the main input field:\n Schema: factory-2019-04-01 Products: - Name: \u0026#34;aws-config-enable-config\u0026#34; Owner: \u0026#34;data-governance@example.com\u0026#34; Description: \u0026#34;Enables AWS Config\u0026#34; Distributor: \u0026#34;cloud-engineering\u0026#34; SupportDescription: \u0026#34;Speak to data-governance@example.com about exceptions and speak to cloud-engineering@example.com about implementation issues\u0026#34; SupportEmail: \u0026#34;cloud-engineering@example.com\u0026#34; SupportUrl: \u0026#34;https://wiki.example.com/cloud-engineering/governance/aws-config-enable-config\u0026#34; Tags: - Key: \u0026#34;type\u0026#34; Value: \u0026#34;governance\u0026#34; - Key: \u0026#34;creator\u0026#34; Value: \u0026#34;cloud-engineering\u0026#34; - Key: \u0026#34;cost-center\u0026#34; Value: \u0026#34;cloud-engineering\u0026#34;     Set the File name to portfolios/reinvent.yaml\n  Set your Author name\n  Set your Email address\n  Set your Commit message\n  Using a good / unique commit message will help you understand what is going on later.\n  Click the Commit changes button:    We have just told the framework there is a product named aws-config-enable-config. This product has no versions and so it will not appear in AWS Service Catalog yet.\nCreate the version We now need to tell the framework we want to create a new version of our product.\n  Navigate to the ServiceCatalogFactory CodeCommit repository again\n  Click on portfolios\n     Click on reinvent.yaml     Click Edit      Add the following to the end of the file (be careful with your indentation):\n Versions: - Name: \u0026#34;v1\u0026#34; Description: \u0026#34;v1 of aws-config-enable-config\u0026#34; Active: True Source: Provider: \u0026#34;CodeCommit\u0026#34; Configuration: RepositoryName: \u0026#34;aws-config-enable-config\u0026#34; BranchName: \u0026#34;main\u0026#34;     Verify the contents of your file matches this:\n Schema: factory-2019-04-01 Products: - Name: \u0026#34;aws-config-enable-config\u0026#34; Owner: \u0026#34;data-governance@example.com\u0026#34; Description: \u0026#34;Enables AWS Config\u0026#34; Distributor: \u0026#34;cloud-engineering\u0026#34; SupportDescription: \u0026#34;Speak to data-governance@example.com about exceptions and speak to cloud-engineering@example.com about implementation issues\u0026#34; SupportEmail: \u0026#34;cloud-engineering@example.com\u0026#34; SupportUrl: \u0026#34;https://wiki.example.com/cloud-engineering/governance/aws-config-enable-config\u0026#34; Tags: - Key: \u0026#34;type\u0026#34; Value: \u0026#34;governance\u0026#34; - Key: \u0026#34;creator\u0026#34; Value: \u0026#34;cloud-engineering\u0026#34; - Key: \u0026#34;cost-center\u0026#34; Value: \u0026#34;cloud-engineering\u0026#34; Versions: - Name: \u0026#34;v1\u0026#34; Description: \u0026#34;v1 of aws-config-enable-config\u0026#34; Active: True Source: Provider: \u0026#34;CodeCommit\u0026#34; Configuration: RepositoryName: \u0026#34;aws-config-enable-config\u0026#34; BranchName: \u0026#34;main\u0026#34;     Once you have updated the file fill in the fields for Author name, Email address, Commit message and hit Commit changes\n  Using a good / unique commit message will help you understand what is going on later.\n Verify the version was created Once you have made your changes the ServiceCatalogFactory Pipeline should have run or if you were quick may still be running. If it has not yet started feel free to the hit the Release change button.\nOnce it has completed it should show the Source and Build stages in green to indicate they have completed successfully:\n  You should see your commit message on this screen, it will help you know which version of ServiceCatalogFactory repo the pipeline is processing.\n Now that your ServiceCatalogFactory pipeline has completed you can view the newly created pipeline: aws-config-enable-config-v1-pipeline\nYou can safely ignore the aws-config-enable-config-v1-pipeline has failed warning. For the pipeline to succeed, we need to add the source code for it to work which we will do in the next step.\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/every-day-use/110-deleting-a-product/100-disabling-the-product-versions.html",
	"title": "Disabling the product versions",
	"tags": [],
	"description": "",
	"content": " What are we going to do? We are going to perform the following steps:\n disable a product version  Step by step guide Here are the steps you need to follow to \u0026ldquo;Disabling the product versions\u0026rdquo;\nDisable the product version When working with other teams it is recommended that you disable a product version before you delete it. This gives teams time to react before deletion of the product. If they are dependent on the product version still they can reach out to you to inform you.\nTo disable a version you need to set its Active attribute to False. You do this by editing its definition in the portfolio yaml.\n  Navigate to the ServiceCatalogFactory CodeCommit repository\n  Click on portfolios\n      Click on the portfolio yaml containing your product\n  Click Edit\n  Add or set the attribute Active for the version you want to disable to False:\n Schema: factory-2019-04-01 Products: - Name: account-vending-machine Owner: central-it@customer.com Description: The iam roles needed for you to do your jobs Distributor: central-it-team SupportDescription: Contact us on Chime for help #central-it-team SupportEmail: central-it-team@customer.com SupportUrl: https://wiki.customer.com/central-it-team/self-service/account-iam  Tags: - Key: product-type Value: iam Versions: - Name: v1 Description: The iam roles needed for you to do your jobs Active: False  Source: Provider: CodeCommit Configuration: RepositoryName: account-vending-machine BranchName: v1     Set your Author name\n  Set your Email address\n  Set your Commit message\n  Click the Commit changes button:\n    When the framework runs the product will be disabled. This change will only affect the version of the product in your factory account. If you are using the imported product in your spoke accounts it will have affect there otherwise you will need to run service-catalog-puppet to cascade the change.\nYou can verify this by navigating to Service Catalog and checking your disabled product. It should look like:\n  "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/2021-workshop/100-removing-the-default-networking.html",
	"title": "Removing the default networking",
	"tags": [],
	"description": "",
	"content": " What are we going to do? When you create a new AWS account there are some networking resources already provisioned in the account. If you are planning on using an AWS Transit Gateway or connecting the AWS account to your local network then these resources may not be required.\nWe are going to create an AWS Lambda function that assumes role into an AWS account to remove the networking resources. We will use a Service Catalog tools stack for this. To do this we will create a pipeline that will create the stack and then we will provision the stack. Once the stack is provisioned we will invoke the lambda function so that the resources are removed. Finally, we will verify the resources are no longer present using an assertion.\nHow are we going to do it? This task is broken down into the following steps:\n Creating the Lambda   Deploying the Lambda   Invoking the Lambda   Asserting the resources are removed   "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/40-reinvent2019/30-prerequisites/100-running-yourself.html",
	"title": "Running yourself",
	"tags": [],
	"description": "",
	"content": " What are we going to do? We are going to perform the following steps:\n Enable AWS Config Install the tools Create an IAM Role  In order to run the workshop in your own account you will need to enable AWS Config and create an IAM Role named TeamRole which you must then assume in order to complete the activities.\nStep by step guide Here are the steps you need to follow to \u0026ldquo;Running yourself\u0026rdquo;\nEnable AWS Config   You should save the following into a file named enable-aws-config.template.yaml:\n AWSTemplateFormatVersion: 2010-09-09 Description: | This template creates a Config Recorder and an Amazon S3 bucket where logs are published. Resources: ConfigRole: Type: \u0026#39;AWS::IAM::Role\u0026#39; Description: The IAM role used to configure AWS Config Properties: AssumeRolePolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Principal: Service: - config.amazonaws.com Action: - \u0026#39;sts:AssumeRole\u0026#39; ManagedPolicyArns: - arn:aws:iam::aws:policy/service-role/AWSConfigRole Policies: - PolicyName: root PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: \u0026#39;s3:GetBucketAcl\u0026#39; Resource: !Sub arn:aws:s3:::${S3ConfigBucket} - Effect: Allow Action: \u0026#39;s3:PutObject\u0026#39; Resource: !Sub arn:aws:s3:::${S3ConfigBucket}/AWSLogs/${AWS::AccountId}/${AWS::Region} Condition: StringEquals: \u0026#39;s3:x-amz-acl\u0026#39;: bucket-owner-full-control - Effect: Allow Action: \u0026#39;config:Put*\u0026#39; Resource: \u0026#39;*\u0026#39; ConfigRecorder: Type: \u0026#39;AWS::Config::ConfigurationRecorder\u0026#39; DependsOn: ConfigRole Properties: Name: default RoleARN: !GetAtt ConfigRole.Arn DeliveryChannel: Type: \u0026#39;AWS::Config::DeliveryChannel\u0026#39; Properties: ConfigSnapshotDeliveryProperties: DeliveryFrequency: Six_Hours S3BucketName: !Ref S3ConfigBucket S3ConfigBucket: DeletionPolicy: Retain Description: S3 bucket with AES256 Encryption set Type: AWS::S3::Bucket Properties: BucketName: !Sub config-bucket-${AWS::AccountId} PublicAccessBlockConfiguration: BlockPublicAcls: True BlockPublicPolicy: True IgnorePublicAcls: True RestrictPublicBuckets: True BucketEncryption: ServerSideEncryptionConfiguration: - ServerSideEncryptionByDefault: SSEAlgorithm: AES256 S3ConfigBucketPolicy: Type: AWS::S3::BucketPolicy Description: S3 bucket policy Properties: Bucket: !Ref S3ConfigBucket PolicyDocument: Version: 2012-10-17 Statement: - Sid: AWSBucketPermissionsCheck Effect: Allow Principal: Service: - config.amazonaws.com Action: s3:GetBucketAcl Resource: - !Sub \u0026#34;arn:aws:s3:::${S3ConfigBucket}\u0026#34; - Sid: AWSBucketDelivery Effect: Allow Principal: Service: - config.amazonaws.com Action: s3:PutObject Resource: !Sub \u0026#34;arn:aws:s3:::${S3ConfigBucket}/AWSLogs/*/*\u0026#34; Outputs: ConfigRoleArn: Value: !GetAtt ConfigRole.Arn S3ConfigBucketArn: Value: !GetAtt S3ConfigBucket.Arn     You should then use AWS CloudFormation to create a stack named enable-aws-config using the template you just created\n  What did we just do?  You created an AWS CloudFormation template You used the template to create a stack The stack you created enabled AWS Config  Install the tools  You should save the following into a file named install-the-tools.template.yaml:   AWSTemplateFormatVersion: 2010-09-09 Description: \u0026#34;This template uses Stack Resource to install Service Catalog Factory and Puppet\u0026#34; Resources: FactoryInstall: Type: AWS::CloudFormation::Stack Properties: Parameters: EnabledRegions: eu-west-1 TemplateURL: \u0026#34;https://service-catalog-tools.s3.eu-west-2.amazonaws.com/factory/latest/servicecatalog-factory-initialiser.template.yaml\u0026#34; TimeoutInMinutes: 30 PuppetInstall: Type: AWS::CloudFormation::Stack Properties: Parameters: EnabledRegions: eu-west-1 ShouldCollectCloudformationEvents: False ShouldForwardEventsToEventbridge: False ShouldForwardFailuresToOpscenter: False TemplateURL: \u0026#34;https://service-catalog-tools.s3.eu-west-2.amazonaws.com/puppet/latest/servicecatalog-puppet-initialiser.template.yaml\u0026#34; TimeoutInMinutes: 30    You should then use AWS CloudFormation to create a stack named install-the-tools.template using the template you just created  What did we just do?  You created an AWS CloudFormation template You used the template to create a stack named install-the-tools The stack you created installed the service catalog tools with the correct configuration for the workshop to run  Create an IAM Role  You should save the following into a file named create-iam-role.template.yaml:   AWSTemplateFormatVersion: \u0026#34;2010-09-09\u0026#34; Description: | Template used to create an IAM role to be used for the service catalog tools workshop Resources: TeamRole: Type: AWS::IAM::Role Properties: RoleName: TeamRole AssumeRolePolicyDocument: Version: \u0026#34;2012-10-17\u0026#34; Statement: - Effect: \u0026#34;Allow\u0026#34; Principal: AWS: !Sub \u0026#34;arn:aws:iam::${AWS::AccountId}:root\u0026#34; Action: - \u0026#34;sts:AssumeRole\u0026#34; ManagedPolicyArns: - arn:aws:iam::aws:policy/AdministratorAccess     You should then use AWS CloudFormation to create a stack named create-iam-role.template using the template you just created\n  You should then assume that role in order to start the workshop\n  What did we just do?  You created an AWS CloudFormation template You used the template to create a stack named create-iam-role You then assumed the role so you have the correct permissions needed and have the correct role name  "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/installation/100-securing-the-installation.html",
	"title": "Securing the installation",
	"tags": [],
	"description": "",
	"content": "Service Catalog Puppet introduces two high-privileged IAM roles (PuppetDeployInSpokeRole and PuppetRole). To ensure that your Service Catalog Puppet installation is secure, it is needed to take additional precautions securing these IAM roles.\nThe related IAM Roles are deployed in both, hub and spoke accounts and have the following trust relationships:\nPuppetDeployInSpokeRole { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;Service\u0026quot;: \u0026quot;codebuild.amazonaws.com\u0026quot; }, \u0026quot;Action\u0026quot;: \u0026quot;sts:AssumeRole\u0026quot; } ] } PuppetRole { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;AWS\u0026quot;: \u0026quot;arn:aws:iam::\u0026lt;hub-account-id\u0026gt;:root\u0026quot; }, \u0026quot;Action\u0026quot;: \u0026quot;sts:AssumeRole\u0026quot; }, { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;AWS\u0026quot;: \u0026quot;arn:aws:iam::\u0026lt;spoke-account-id\u0026gt;:root\u0026quot; }, \u0026quot;Action\u0026quot;: \u0026quot;sts:AssumeRole\u0026quot; } ] } Risk The risk with these IAM role is a potential privilege escalation by either directly assuming the PuppetRole or by launching a CodeBuild instance and assigning the PuppetDeployInSpokeRole to this instance. This would allow any IAM user/role with appropriate permissions to execute commands with the privileges of the Puppet roles.\nMitigating risk via SCP The following Service Control Policy denies the sts:AssumeRole and iam:* actions on all IAM roles which are created with the path servicecatalog-puppet. This path is the default setting. If you defined another path for these IAM roles, you will need to adapt the SCP accordingly.\nApply this SCP to all spoke accounts and the hub account of Service Catalog Puppet after deploying the Puppet resources in these accounts. See the AWS documentation  on how to create and apply SCPs.\n{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Sid\u0026quot;: \u0026quot;PreventPrivilegeEscalationInServiceCatalogPuppet\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Deny\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;sts:AssumeRole\u0026quot;, \u0026quot;iam:*\u0026quot; ], \u0026quot;Resource\u0026quot;: [ \u0026quot;arn:aws:iam::*:role/servicecatalog-puppet/*\u0026quot; ], \u0026quot;Condition\u0026quot;: { \u0026quot;ArnNotLike\u0026quot;: { \u0026quot;aws:PrincipalARN\u0026quot;: [ \u0026quot;arn:aws:iam::*:role/servicecatalog-puppet/*\u0026quot; ] } } } ] } "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/design-considerations/selecting-a-factory-account.html",
	"title": "Selecting a factory account",
	"tags": [],
	"description": "",
	"content": " What are we going to do? This article will help you choose which AWS Account is most suitable to use for the Service Catalog Tools\nRecommended background reading? It is recommended that you have read the following:\n Multi Account Strategy  Selecting how many factory accounts to have Currently, you can only have one factory per account but you can create multiple accounts each with their own factory.\nIf multiple teams want to make use of Service Catalog Factory the recommendation is that each team have their own instance. The teams are then independent and can operate without impacting each other. There is also a separation of concerns if there is a security factory account and a networking factory account. This reduces the blast radius should there be an incident and it enables easier billing calculations.\nIf your organization has a central cloud engineering team who work to deliver the requirements of these other teams it may be easier to manage all of the provisioning from a single factory account.\nSelecting a factory account In order to select a factory account we need to consider how you are going to be using the framework.\nA common use case is where teams use the framework to build and provision security controls into accounts that are operating their customer facing applications.\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/every-day-use/110-deleting-a-product.html",
	"title": "Deleting a product",
	"tags": [],
	"description": "",
	"content": " This tutorial will walk you through \u0026ldquo;Deleting a product\u0026rdquo;\nWe will assume you have:\n installed Service Catalog Factory correctly added a product correctly  In the tutorial you will:\n Disabling the product versions   Deleting the product   During this process you will check your progress by verifying what the framework is doing.\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/design-considerations/using-parameters-in-the-real-world.html",
	"title": "Using parameters in the real world",
	"tags": [],
	"description": "",
	"content": " What are we going to do? This article will show you how to manage parameters across your Service Catalog Tools environment. It is a collection of real world examples of how to use parameters for organization unit, account and region level wide configurations.\nUsing parameters (basic usage) When you specify a launch you can specify parameters. Here is an example where a vpc is provisioned into the default region of each account tagged as type:spoke:\n launches: networking: portfolio: \u0026#34;networking\u0026#34; product: \u0026#34;vpc\u0026#34; version: \u0026#34;v1\u0026#34; parameters: cidr: default: \u0026#34;10.0.0.1/24\u0026#34; deploy_to: tags: - tag: \u0026#34;type:spoke\u0026#34; regions: \u0026#34;default_region\u0026#34;   You could have also retrieved the value from SSM:\n launches: networking: portfolio: \u0026#34;networking\u0026#34; product: \u0026#34;vpc\u0026#34; version: \u0026#34;v1\u0026#34; parameters: cidr: ssm: name: \u0026#34;/multi-account-config/networking/vpc/default/cidr\u0026#34; deploy_to: tags: - tag: \u0026#34;type:spoke\u0026#34; regions: \u0026#34;default_region\u0026#34;   This makes it dynamic but what happens if you want to have a different value for each region?\nUsing mappings for parameters You can use a mapping to make this more configurable:\n mappings: VPCs: us-east-1: \u0026#34;cidr\u0026#34;: \u0026#34;10.0.0.1/24\u0026#34; us-west-1: \u0026#34;cidr\u0026#34;: \u0026#34;192.168.0.1/26\u0026#34; launches: networking: portfolio: \u0026#34;networking\u0026#34; product: \u0026#34;vpc\u0026#34; version: \u0026#34;v1\u0026#34; parameters: cidr: mapping: [VPCs, AWS::Region, cidr] deploy_to: tags: - tag: \u0026#34;type:spoke\u0026#34; regions: \u0026#34;default_region\u0026#34;   With the above configuration you are saying when provisioning vpc into us-east-1 use 10.0.0.1/24 and when provisioning into us-west-1 use 192.168.0.1/26. This allows you to have a different parameter value for each region but that value will be the same for every launch. To make it different per account you can use the following:\n mappings: VPCs: 0123456789010: \u0026#34;cidr\u0026#34;: \u0026#34;10.0.0.1/24\u0026#34; 0098765432110: \u0026#34;cidr\u0026#34;: \u0026#34;192.168.0.1/26\u0026#34; launches: networking: portfolio: \u0026#34;networking\u0026#34; product: \u0026#34;vpc\u0026#34; version: \u0026#34;v1\u0026#34; parameters: cidr: mapping: [VPCs, AWS::AccountId, cidr] deploy_to: tags: - tag: \u0026#34;type:spoke\u0026#34; regions: \u0026#34;default_region\u0026#34;   With the above configuration you are saying when provisioning vpc into account 0123456789010 use 10.0.0.1/24 and when provisioning into account 0098765432110 use 192.168.0.1/26. This allows you to have a different parameter value for each account but you will have to update your manifest file each time you want to add an account.\nUsing intrinsic functions in ssm parameter names You can use the account id and region name within the SSM parameter name value to use account and region specific ssm parameters:\n launches: networking: portfolio: \u0026#34;networking\u0026#34; product: \u0026#34;vpc\u0026#34; version: \u0026#34;v1\u0026#34; parameters: cidr: ssm: name: \u0026#34;/vpcs/${AWS::AccountId}/${AWS::Region}/cidr\u0026#34; deploy_to: tags: - tag: \u0026#34;type:spoke\u0026#34; regions: \u0026#34;default_region\u0026#34;   Each time vpc is provisioned into a region of an account the region name and account id will be used to substitute values in the ssm name attribute. For example, when you provision into us-east-1 of account 012345678910 the ssm parameter used to get the value for the cidr parameter will be the one with the name \u0026quot;/vpcs/012345678910/us-east-1/cidr\u0026rdquo;.\nStoring values in ssm using intrinsic functions You can store the stack outputs for your products in SSM and use intrinsic functions to derive the name:\n launches: networking: portfolio: \u0026#34;networking\u0026#34; product: \u0026#34;vpc\u0026#34; version: \u0026#34;v1\u0026#34; parameters: cidr: ssm: name: \u0026#34;/vpcs/${AWS::AccountId}/${AWS::Region}/cidr\u0026#34; deploy_to: tags: - tag: \u0026#34;type:spoke\u0026#34; regions: \u0026#34;default_region\u0026#34; outputs: ssm: - param_name: \u0026#34;/vpcs/${AWS::AccountId}/${AWS::Region}/id\u0026#34; stack_output: VPCId  When you provision into us-east-1 of account 012345678910 the ssm parameter used to store the stack output will have the name of \u0026quot;/vpcs/012345678910/us-east-1/id\u0026rdquo;\nCustomer provided parameters If you have built a self-service / account vending mechanism you may want to allow the customers of your solution to set some parameters to be used later on - for example whether they require a connected account or not, if they want private subnets or public or even if they want to have networking at all or not.\nIf you are vending accounts by provisioning a product into your Service Catalog Tools account you have a very easy option. Include an SSM parameter into your account creation product. The name of the parameter should be derived from the account id of the newly created account:\n AWSTemplateFormatVersion: \u0026#39;2010-09-09\u0026#39; Parameters: NetworkType: Type: String AllowedValues\u0026#34; : [\u0026#34;connected\u0026#34;, \u0026#34;private\u0026#34;, \u0026#34;public\u0026#34;, \u0026#34;none\u0026#34;] Description: Type of networking setup required Resources: Account: Type: Custom::Resource Description: A custom resource representing an AWS Account Properties: ServiceToken: !Ref AccountCreatorLambdaArn Email: !Ref Email AccountName: !Ref AccountName IamUserAccessToBilling: !Ref IamUserAccessToBilling NetworkingRequired: Type: AWS::SSM::Parameter Properties: Name: !Sub \u0026#34;/networking/${Account.AccountId}/NetworkType\u0026#34; Value: !Ref NetworkType  Please note some of the parameters and resources have been omitted from the example above.\nThis will create an SSM parameter in your Service Catalog Tools account that can be used in your launches:\n launches: networking: portfolio: \u0026#34;networking\u0026#34; product: \u0026#34;networks\u0026#34; version: \u0026#34;v1\u0026#34; parameters: NetworkType: ssm: name: \u0026#34;/networking/${AWS::AccountId}/NetworkType\u0026#34; deploy_to: tags: - tag: \u0026#34;type:spoke\u0026#34; regions: \u0026#34;default_region\u0026#34;   Within your product you can use conditions to provision the correct set of resources or you can use three launches (one for each network type) along with a condition on whether they should do anything or not:\n launches: networking-connected: portfolio: \u0026#34;networking\u0026#34; product: \u0026#34;networks\u0026#34; version: \u0026#34;v1\u0026#34; parameters: NetworkType: ssm: name: \u0026#34;/networking/${AWS::AccountId}/NetworkType\u0026#34; deploy_to: tags: - tag: \u0026#34;type:spoke\u0026#34; regions: \u0026#34;default_region\u0026#34; networking-private: portfolio: \u0026#34;networking\u0026#34; product: \u0026#34;networks\u0026#34; version: \u0026#34;v1\u0026#34; parameters: NetworkType: ssm: name: \u0026#34;/networking/${AWS::AccountId}/NetworkType\u0026#34; deploy_to: tags: - tag: \u0026#34;type:spoke\u0026#34; regions: \u0026#34;default_region\u0026#34; networking-private: portfolio: \u0026#34;networking\u0026#34; product: \u0026#34;networks\u0026#34; version: \u0026#34;v1\u0026#34; parameters: NetworkType: ssm: name: \u0026#34;/networking/${AWS::AccountId}/NetworkType\u0026#34; deploy_to: tags: - tag: \u0026#34;type:spoke\u0026#34; regions: \u0026#34;default_region\u0026#34;   Example product template for private product:\n AWSTemplateFormatVersion: 2010-09-09 Parameters: NetworkType: Description: Type of network to create Type: String AllowedValues\u0026#34; : [\u0026#34;connected\u0026#34;, \u0026#34;private\u0026#34;, \u0026#34;public\u0026#34;, \u0026#34;none\u0026#34;] Conditions: CreateNetwork: !Equals - !Ref NetworkType - private Resources: Network: Type: \u0026#39;AWS::EC2::VPC\u0026#39; Properties: CidrBlock: 10.0.0.0/16 EnableDnsSupport: \u0026#39;false\u0026#39; EnableDnsHostnames: \u0026#39;false\u0026#39;  "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/every-day-use/150-creating-a-portfolio.html",
	"title": "Creating a portfolio",
	"tags": [],
	"description": "",
	"content": " What are we going to do? This tutorial will walk you through \u0026ldquo;Creating a portfolio\u0026rdquo; with a spoke account.\nWe will assume you have installed Service Catalog Puppet correctly.\nWe are going to perform the following steps:\n create a portfolio file define a product define a version for our product commit our portfolio file verify the framework has create an AWS CodePipeline for our product version  During this process you will check your progress by verifying what the framework is doing at each step.\nStep by step guide Here are the steps you need to follow to \u0026ldquo;Creating a portfolio\u0026rdquo;\nAdding the portfolio to the framework   Navigate to the ServiceCatalogFactory CodeCommit repository\n  Click on portfolios\n     Click on reinvent.yaml     Click Edit     Add the following to the end of the file (be careful with your indentation):   Portfolios: - DisplayName: \u0026#34;cloud-engineering-governance\u0026#34; Description: \u0026#34;Portfolio containing the products needed to govern AWS accounts\u0026#34; ProviderName: \u0026#34;cloud-engineering\u0026#34; Associations: - \u0026#34;arn:aws:iam::${AWS::AccountId}:role/TeamRole\u0026#34; Tags: - Key: \u0026#34;type\u0026#34; Value: \u0026#34;governance\u0026#34; - Key: \u0026#34;creator\u0026#34; Value: \u0026#34;cloud-engineering\u0026#34; - Key: \u0026#34;cost-center\u0026#34; Value: \u0026#34;cloud-engineering\u0026#34;    Verify the contents of your file matches this:   Schema: factory-2019-04-01 Products: - Name: \u0026#34;aws-config-enable-config\u0026#34; Owner: \u0026#34;data-governance@example.com\u0026#34; Description: \u0026#34;Enables AWS Config\u0026#34; Distributor: \u0026#34;cloud-engineering\u0026#34; SupportDescription: \u0026#34;Speak to data-governance@example.com about exceptions and speak to cloud-engineering@example.com about implementation issues\u0026#34; SupportEmail: \u0026#34;cloud-engineering@example.com\u0026#34; SupportUrl: \u0026#39;https://wiki.example.com/cloud-engineering/governance/aws-config-enable-config\u0026#39; Tags: - Key: \u0026#34;type\u0026#34; Value: \u0026#34;governance\u0026#34; - Key: \u0026#34;creator\u0026#34; Value: \u0026#34;cloud-engineering\u0026#34; - Key: \u0026#34;cost-center\u0026#34; Value: \u0026#34;cloud-engineering\u0026#34; Versions: - Name: \u0026#34;v1\u0026#34; Description: \u0026#34;v1 of aws-config-enable-config\u0026#34; Active: True Source: Provider: \u0026#34;CodeCommit\u0026#34; Configuration: RepositoryName: \u0026#34;aws-config-enable-config\u0026#34; BranchName: \u0026#34;main\u0026#34; Portfolios: - DisplayName: \u0026#34;cloud-engineering-governance\u0026#34; Description: \u0026#34;Portfolio containing the products needed to govern AWS accounts\u0026#34; ProviderName: \u0026#34;cloud-engineering\u0026#34; Associations: - \u0026#34;arn:aws:iam::${AWS::AccountId}:role/TeamRole\u0026#34; Tags: - Key: \u0026#34;type\u0026#34; Value: \u0026#34;governance\u0026#34; - Key: \u0026#34;creator\u0026#34; Value: \u0026#34;cloud-engineering\u0026#34; - Key: \u0026#34;cost-center\u0026#34; Value: \u0026#34;cloud-engineering\u0026#34;   Once you have updated the file fill in the fields for Author name, Email address, Commit message and hit Commit changes\n  Using a good / unique commit message will help you understand what is going on later.\n Verify the portfolio was created Once you have made your changes the ServiceCatalogFactory Pipeline should have run or if you were quick may still be running. If it has not yet started feel free to the hit the Release change button.\nOnce it has completed it should show the Source and Build stages in green to indicate they have completed successfully:\n  Once you have verified the pipeline has run you can go to Service Catalog portfolios to view your portfolio.\nYou should see the portfolio you just created listed:\n  You have now successfully created a portfolio!\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/40-reinvent2019/150-task-2.html",
	"title": "Data governance",
	"tags": [],
	"description": "",
	"content": " The ask Example Corp now has a number of development and test workloads in AWS. Many of these workloads make use of data storage services such as Amazon RDS and Amazon S3. The customer has recently established a dedicated data governance team, who have been tasked with identifying controls for workloads that process data classified as confidential or internal-use only.\nThe data governance team has recently issued guidelines around the use of encryption for data at rest and in transit in the cloud. We have below an excerpt from the data governance standard:\n 3.1.1 All cloud data storage systems must be configured to support encryption at rest and in transit using industry supported encryption algorithms for data classified as Confidential, or Internal-Use only. For a list of approved encryption algorithms and key-lengths please see Appendix A.\n From these guidelines there are two new requirements:\n The data governance team at Example Corp wants to get visibility into resources where encryption at rest is not being used The team wants to use AWS Service Catalog to make it easy for development teams to comply with the encryption-at-rest requirement, without having to set it up themselves.  These requirements will shape the work that goes into building additional data governance controls as development teams look to use additional AWS services to store production or material data.\nThe plan We are going to create and deploy a data governance control using an AWS Config managed rule to ensure the teams are using encryption when creating an RDS instance. We will then create a self service Service Catalog product so the teams can create compliant resources.\n Create the control   Provision the control   Create the product   If you need help at any time please raise your hand.\n "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/design-considerations/fine-grained-depends-on.html",
	"title": "Fine grained depends_on",
	"tags": [],
	"description": "",
	"content": " What are we going to do? This article will explain how the depends_on clause works within service catalog puppet and how fine tune its use to achieve better saturation of workers as well reducing the impact of failures when managing your multi account.\nWhat is depends_on? Within service catalog puppet you can provision AWS Service Catalog products, share Service Catalog portfolios, execute AWS Lambda functions, run AWS CodeBuild projects and run assertions to verify the effects of your configurations.\nWhen configuring service catalog puppet you may want to specify the order in which these things happen. For example, when building out your networking capability, you may want to provision a Service Catalog product containing a lambda function that can remove any default networking resources in your AWS account (default VPC, subnet, security group etc).\nOnce you have provisioned that, you may want to execute it and then assert that the networking resources are removed before provisioning a new VPC and then Subnets. Once all that provisioning is completed, you may then want to share a portfolio that allows users to provision an EKS cluster into the newly created networking stacks.\nThe basic building blocks for ordering these things is a depends_on clause which you can use in the following sections of the manifest file:\n launches spoke-local-portfolios lambda-executions code-build-runs assertions  Here is an example:\n launches: networking: portfolio: \u0026#34;networking\u0026#34; product: \u0026#34;vpc\u0026#34; version: \u0026#34;v1\u0026#34; depends_on: - name: terminate-default-networking type: lambda-execution parameters: cidr: default: \u0026#34;10.0.0.1/24\u0026#34; deploy_to: tags: - tag: \u0026#34;type:spoke\u0026#34; regions: \u0026#34;default_region\u0026#34;   In this example before any provisioning occurs for the networking launch every lambda-executions for terminate-default-networking must have completed successfully across all regions of all accounts. If any fail the launch will not be scheduled.\nFine tuning depends_on When you configure service catalog puppet you are building a workflow that is executed by a set of workers. You can configure the number of workers in the workflow and you can configure the shape and size of the workflow by modifying the manifest file.\nWhen you use the default configuration of depends_on a natural choke point is created. By saying B depends on A, you are saying everything within A must complete before B can begin. You can reduce the impact of this by configuring the depends_on affinity value.\nRegion You can set the affinity to region:\n launches: networking: portfolio: \u0026#34;networking\u0026#34; product: \u0026#34;vpc\u0026#34; version: \u0026#34;v1\u0026#34; depends_on: - name: terminate-default-networking type: lambda-execution affinity: region parameters: cidr: default: \u0026#34;10.0.0.1/24\u0026#34; deploy_to: tags: - tag: \u0026#34;type:spoke\u0026#34; regions: \u0026#34;default_region\u0026#34;   In this example before any provisioning occurs for the networking launch in us-east-1 every lambda-executions for terminate-default-networking in us-east-1 across all accounts must have completed successfully. If any lambda-execution for terminate-default-networking in any account in us-east-1 fails then no launches for networking in us-east-1 will be scheduled.\nThis is useful when you are building product sets that should be provisioned in hub and spoke accounts and rely on regional resources like networking.\nAccount You can set the affinity to account:\n launches: networking: portfolio: \u0026#34;networking\u0026#34; product: \u0026#34;vpc\u0026#34; version: \u0026#34;v1\u0026#34; depends_on: - name: terminate-default-networking type: lambda-execution affinity: account parameters: cidr: default: \u0026#34;10.0.0.1/24\u0026#34; deploy_to: tags: - tag: \u0026#34;type:spoke\u0026#34; regions: \u0026#34;default_region\u0026#34;   In this example before any provisioning occurs for the networking launch in account 2 every lambda-executions for terminate-default-networking in account 1 across all regions must have completed successfully. If any lambda-execution for terminate-default-networking in any region in account 1 fails then no launches for networking in account 2 will be scheduled.\nThis is useful when you are building product sets that should be provisioned in hub and spoke accounts and rely on global resources like IAM.\nAccount and region You can set the affinity to account:\n launches: networking: portfolio: \u0026#34;networking\u0026#34; product: \u0026#34;vpc\u0026#34; version: \u0026#34;v1\u0026#34; depends_on: - name: terminate-default-networking type: lambda-execution affinity: account-and-region parameters: cidr: default: \u0026#34;10.0.0.1/24\u0026#34; deploy_to: tags: - tag: \u0026#34;type:spoke\u0026#34; regions: \u0026#34;default_region\u0026#34;   In this example before any provisioning occurs for the networking launch in us-east-1 of account 2 every lambda-executions for terminate-default-networking in account 1 in us-east-1 must have completed successfully. If any lambda-execution for terminate-default-networking in us-east-1 in account 1 fails then no launches for networking in account 2 will be scheduled.\nThis is useful when you are building products sets that should be provisioned into accounts in order into multiple spoke accounts, for example VPCs, Subnets and NACLs.\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/every-day-use/180-adding-a-product-to-a-portfolio.html",
	"title": "Adding a product to a portfolio",
	"tags": [],
	"description": "",
	"content": " What are we going to do? This tutorial will walk you through \u0026ldquo;Adding a product to a portfolio\u0026rdquo; into a spoke account.\nWe will assume you have:\n installed Service Catalog Puppet correctly you have created a product you have created a portfolio  We are going to perform the following steps:\n add a product to a portfolio  During this process you will check your progress by verifying what the framework is doing at each step.\nStep by step guide Here are the steps you need to follow to \u0026ldquo;Adding a product to a portfolio\u0026rdquo;\nAdd the product to the portfolio   Navigate to the ServiceCatalogFactory CodeCommit repository again\n  Click on portfolios\n     Click on reinvent.yaml     Click Edit     Replace the contents of your file with this:   Schema: factory-2019-04-01 Products: - Name: \u0026#34;aws-config-enable-config\u0026#34; Owner: \u0026#34;data-governance@example.com\u0026#34; Description: \u0026#34;Enables AWS Config\u0026#34; Distributor: \u0026#34;cloud-engineering\u0026#34; SupportDescription: \u0026#34;Speak to data-governance@example.com about exceptions and speak to cloud-engineering@example.com about implementation issues\u0026#34; SupportEmail: \u0026#34;cloud-engineering@example.com\u0026#34; SupportUrl: \u0026#34;https://wiki.example.com/cloud-engineering/governance/aws-config-enable-config\u0026#34; Tags: - Key: \u0026#34;type\u0026#34; Value: \u0026#34;governance\u0026#34; - Key: \u0026#34;creator\u0026#34; Value: \u0026#34;cloud-engineering\u0026#34; - Key: \u0026#34;cost-center\u0026#34; Value: \u0026#34;cloud-engineering\u0026#34; Versions: - Name: \u0026#34;v1\u0026#34; Description: \u0026#34;v1 of aws-config-enable-config\u0026#34; Active: True Source: Provider: \u0026#34;CodeCommit\u0026#34; Configuration: RepositoryName: \u0026#34;aws-config-enable-config\u0026#34; BranchName: \u0026#34;main\u0026#34; Portfolios: - \u0026#34;cloud-engineering-governance\u0026#34; Portfolios: - DisplayName: \u0026#34;cloud-engineering-governance\u0026#34; Description: \u0026#34;Portfolio containing the products needed to govern AWS accounts\u0026#34; ProviderName: \u0026#34;cloud-engineering\u0026#34; Associations: - \u0026#34;arn:aws:iam::${AWS::AccountId}:role/TeamRole\u0026#34; Tags: - Key: \u0026#34;type\u0026#34; Value: \u0026#34;governance\u0026#34; - Key: \u0026#34;creator\u0026#34; Value: \u0026#34;cloud-engineering\u0026#34; - Key: \u0026#34;cost-center\u0026#34; Value: \u0026#34;cloud-engineering\u0026#34;    Take note of the highlighted lines 26 and 27. We have added a portfolio to the product.  Once you have updated the file fill in the fields for Author name, Email address, Commit message and hit Commit changes\n  Using a good / unique commit message will help you understand what is going on later.\n Verify the product was added to the portfolio Once you have made your changes the ServiceCatalogFactory Pipeline should have run or if you were quick may still be running. If it has not yet started feel free to the hit the Release change button.\nOnce it has completed it should show the Source and Build stages in green to indicate they have completed successfully:\n  Once you have verified the pipeline has run you can go to Service Catalog portfolios to view your portfolio.\n Click on reinvent-cloud-engineering-governance     Click on the product aws-config-enable-config     Click on the version v1    "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/every-day-use/190-creating-a-manifest.html",
	"title": "Creating a manifest",
	"tags": [],
	"description": "",
	"content": " What are we going to do? This tutorial will walk you through \u0026ldquo;Creating a manifest\u0026rdquo;\nWe will assume you have installed Service Catalog Puppet correctly.\nWe are going to perform the following steps:\n create a manifest file  During this process you will check your progress by verifying what the framework is doing at each step.\nStep by step guide Here are the steps you need to follow to \u0026ldquo;Creating a manifest\u0026rdquo;\nCreating the manifest file   Navigate to the ServiceCatalogPuppet CodeCommit repository\n  Scroll down to the bottom of the page and hit the Create file button\n    Editing the manifest file  Write out the content of your manifest file. Here is an example snippet:   accounts: - account_id: \u0026#34;\u0026lt;YOUR_ACCOUNT_ID_WITHOUT_HYPHENS\u0026gt;\u0026#34;  name: \u0026#34;puppet-account\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34;    Please read through the docs to help you write out the full manifest file.  Committing the manifest file Now that we have written the manifest file we are ready to commit it.\n  Set the File name to manifest.yaml\n  Set your Author name\n  Set your Email address\n  Set your Commit message\n  Using a good / unique commit message will help you understand what is going on later.\n  Click the Commit changes button:    "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/every-day-use/192-adding-an-account.html",
	"title": "Adding an account",
	"tags": [],
	"description": "",
	"content": " What are we going to do? This tutorial will walk you through \u0026ldquo;Adding an account\u0026rdquo;\nWe will assume you have:\n installed Service Catalog Puppet correctly created a manifest bootstrapped a spoke  We are going to perform the following steps:\n adding an account to the manifest file  During this process you will check your progress by verifying what the framework is doing at each step.\nStep by step guide Here are the steps you need to follow to \u0026ldquo;Adding an account\u0026rdquo;\nAdding an account to the manifest file   Navigate to the ServiceCatalogPuppet CodeCommit repository\n  Click on manifest.yaml\n  Click Edit\n      Append the following snippet to the YAML document in the main input field (be careful with your indentation):\n  Copy the following snippet into the main input field:\n accounts: - account_id: \u0026#34;\u0026lt;YOUR_ACCOUNT_ID_WITHOUT_HYPHENS\u0026gt;\u0026#34; name: \u0026#34;puppet-account\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; - \u0026#34;eu-west-2\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34;     Update account_id on line to show the account id of the account you have bootstrapped\n  Committing the manifest file   Set the File name to manifest.yaml\n  Set your Author name\n  Set your Email address\n  Set your Commit message\n  Using a good / unique commit message will help you understand what is going on later.\n  Click the Commit changes button:    "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/conclusion/",
	"title": "Conclusion",
	"tags": [],
	"description": "",
	"content": "What have we accomplished? We started with a set of requirements for budget, cost, and data governance and turned them into codified controls in your AWS Accounts using native AWS Services and modern software development practices.\n An AWS Config rule that helps us discover Amazon EC2 instance types in use An AWS Config rule that identifies Amazon RDS instances that do not use encryption at rest An AWS Service Catalog product that creates a curated version of Amazon Aurora that enforces encryption by default  In the workshop we used a single AWS Account and a single AWS Region to show you what's possible using open source service catalog tools and AWS Service Catalog. You can use the service catalog tools and AWS Service Catalog to provision products across AWS estates that have hundreds of AWS Accounts, in different regions based on your requirements.\nWhat's next? We recommend that you follow this workshop with further reading about service catalog factory and service catalog puppet to get a deeper understanding of how the service catalog tools work behind the scenes to easily create and provision curated AWS Service Catalog products across your organisation.\nLevel up your skills by trying out the tasks you've completed across an estate of AWS Accounts. Think about how you could fit the tools into your day-to-day workflow with AWS Service Catalog.\nFeedback Remember to provide feedback for the workshop in the re:Invent mobile app before you leave. Your feedback helps us shape the content of workshops and is important in driving future work on service catalog tools.\nIf you'd like to provide feedback about the service catalog tools or report bugs in the workshop, please use our GitHub issue tracker.\nEnjoy re:Invent!\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/design-considerations/portfolio-management.html",
	"title": "Portfolio Management",
	"tags": [],
	"description": "",
	"content": " What are we going to do? This article will help you understand AWS Service Catalog portfolios, what they are, how they are used by the service and how you can best make use of them whilst using the Service Catalog Tools.\nWhat is a portfolio? Within AWS Service Catalog a portfolio is a logical grouping of products.\nIt makes sense to group products that provision similar or complimentary resources together, but this may not give you the flexibility you need:\n Within AWS Service Catalog you set associations at the portfolio level so by default when you grant access to a portfolio all products can be seen. Within AWS Service Catalog you can share portfolios with other accounts. You cannot share just a single product from a portfolio.  How you can make best use of them When using the Service Catalog Tools we recommend you think about how your products will be consumed. If you are going to offer a \u0026lsquo;service catalog\u0026rsquo; or build a vending machine we recommend grouping those products together into a portfolio.\nWhen using the Service Catalog Tools to provision resources into an account we recommend grouping those products into a portfolio.\nWhen you have multiple teams building products we recommend each team having their own portfolios.\nThis normally results in at least two portfolios per team:\n team a  self service offering other products   team b  self service offering other products    The teams we have worked with normally group the products into mandatory products and self service products. For example, if the team using Service Catalog Tools is a cloud engineering / CCOE team they would provision products like AWS Security Hub Enabler into an account - this would be mandatory. If the same team had some optional products like Encrypted S3 Bucket then this would go into an optional portfolio. This results in the following structure:\n team a  mandatory optional    The names of the portfolios are for you to chose as you know your audience better than we do but we have found the following names work well:\n mandatory optional vending-machine networking-self-service well-governed-vending-machine well-architected-vending-machine compulsory  If you would like to share your portfolio names raise a github issue to share\n "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/40-reinvent2019/100-task-1/200-provision-the-control.html",
	"title": "Provision the control",
	"tags": [],
	"description": "",
	"content": " What are we going to do? We are going to perform the following steps:\n create a manifest file with our account in it provision the product aws-config-desired-instance-types into our account  Step by step guide Here are the steps you need to follow to provision the control. In the previous task, we created an AWS Service Catalog product but it has not yet been provisioned.\nCreate a manifest file with our account in it   Navigate to the ServiceCatalogPuppet CodeCommit repository\n  Scroll down to the bottom of the page and hit the Create file button\n     For the next step you will need to know your account id. To find your account id you can check the console, in the top right drop down. It is a 12 digit number. When using your account id please do not include the hyphens ('-') and do not use the angle brackets (\u0026lsquo;\u0026lt;\u0026rsquo;,\u0026lsquo;\u0026gt;\u0026rsquo;)     Copy the following snippet into the main input field and replace account_id to show your account id on the highlighted line:   accounts: - account_id: \u0026#34;\u0026lt;YOUR_ACCOUNT_ID_WITHOUT_HYPHENS\u0026gt;\u0026#34;  name: \u0026#34;puppet-account\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34;   it should look like the following - but with your account id on the highlighted line:\n accounts: - account_id: \u0026#34;012345678910\u0026#34;  name: \u0026#34;puppet-account\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34;   Provision the product aws-config-desired-instance-types into a spoke account   Append the following snippet to the end of the main input field:\n launches: aws-config-desired-instance-types: portfolio: \u0026#34;reinvent-cloud-engineering-governance\u0026#34; product: \u0026#34;aws-config-desired-instance-types\u0026#34; version: \u0026#34;v1\u0026#34; parameters: InstanceType: default: \u0026#34;t2.medium, t2.large, t2.xlarge\u0026#34;  deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;     The CloudFormation template we used to create this product had a parameter named InstanceType. The highlighted lines show how we can use the framework to set a value for that parameter when provisioning it.\n  The main input field should look like this (remember to set your account_id):   accounts: - account_id: \u0026#34;\u0026lt;YOUR_ACCOUNT_ID_WITHOUT_HYPHENS\u0026gt;\u0026#34; name: \u0026#34;puppet-account\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34; launches: aws-config-desired-instance-types: portfolio: \u0026#34;reinvent-cloud-engineering-governance\u0026#34; product: \u0026#34;aws-config-desired-instance-types\u0026#34; version: \u0026#34;v1\u0026#34; parameters: InstanceType: default: \u0026#34;t2.medium, t2.large, t2.xlarge\u0026#34; deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;   Committing the manifest file Now that we have written the manifest file we are ready to commit it.\n  Set the File name to manifest.yaml\n  Set your Author name\n  Set your Email address\n  Set your Commit message\n  Using a good / unique commit message will help you understand what is going on later.\n  Click the Commit changes button:    What did we just do? The YAML file we created in the previous step told the framework to perform the following actions:\n provision a product named aws-config-desired-instance-types into each of the enabled regions of the account  When you added the following:\n launches: aws-config-desired-instance-types: portfolio: \u0026#34;reinvent-cloud-engineering-governance\u0026#34; product: \u0026#34;aws-config-desired-instance-types\u0026#34; version: \u0026#34;v1\u0026#34; parameters: InstanceType: default: \u0026#34;t2.medium, t2.large, t2.xlarge\u0026#34; deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;    You told the framework to provision v1 of aws-config-desired-instance-types from the portfolio cloud-engineering-governance into every account that has the tag type:prod\n accounts: - account_id: \u0026#34;\u0026lt;YOUR_ACCOUNT_ID_WITHOUT_HYPHENS\u0026gt;\u0026#34; name: \u0026#34;puppet-account\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34;   Within each account there will be a copy of the product provisioned into each of the regions listed in the regions_enabled section:\n accounts: - account_id: \u0026#34;\u0026lt;YOUR_ACCOUNT_ID_WITHOUT_HYPHENS\u0026gt;\u0026#34; name: \u0026#34;puppet-account\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; tags:  - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34;   For this workshop, we are creating and provisioning the product into the same AWS Account, but in a multi-account setup, you might choose to create a product in a \u0026ldquo;hub\u0026rdquo; account and provision it only to \u0026ldquo;spoke\u0026rdquo; accounts.\nIn the workshop, you will only have permission to view the products in eu-west-1.\n Verifying the provisioned product Once you have made your changes the ServiceCatalogPuppet Pipeline should have run. If you were quick in making the change, the pipeline may still be running. If it has not yet started feel free to the hit the Release change button.\nOnce it has completed it should show the Source, Generate and Deploy stages in green to indicate they have completed successfully:\n  If this is failing please raise your hand for some assistance\n Once you have verified the pipeline has run you can go to Service Catalog provisioned products to view your provisioned product. Please note when you arrive at the provisioned product page you will need to select account from the filter by drop down in the top right:\n  If you cannot see your product please raise your hand for some assistance\n You have now successfully provisioned a product\nVerify that the AWS Config rule is enabled To see the AWS Config rule enabled, navigate to AWS Config rules. Once there you should see the following:\n  "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/40-reinvent2019/150-task-2/200-provision-the-control.html",
	"title": "Provision the control",
	"tags": [],
	"description": "",
	"content": " What are we going to do? We are going to perform the following steps:\n provision the product aws-config-rds-storage-encrypted  For this workshop, we are using the same account as both the hub and spoke for simplicity; in a multi-account setup, products that are created in a hub account could be provisioned in multiple spoke accounts.\nStep by step guide Here are the steps you need to follow to \u0026ldquo;Provision the control\u0026rdquo;\nProvision the product aws-config-rds-storage-encrypted into a spoke account   Navigate to the ServiceCatalogPuppet CodeCommit repository again\n  Click on manifest.yaml\n  Click Edit\n  Append the following snippet to the end of the main input field:\n   aws-config-rds-storage-encrypted: portfolio: \u0026#34;reinvent-cloud-engineering-governance\u0026#34; product: \u0026#34;aws-config-rds-storage-encrypted\u0026#34; version: \u0026#34;v1\u0026#34; deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;    The main input field should look like this (remember to set your account_id):   accounts: - account_id: \u0026#34;\u0026lt;YOUR_ACCOUNT_ID_WITHOUT_HYPHENS\u0026gt;\u0026#34; name: \u0026#34;puppet-account\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34; launches: aws-config-desired-instance-types: portfolio: \u0026#34;reinvent-cloud-engineering-governance\u0026#34; product: \u0026#34;aws-config-desired-instance-types\u0026#34; version: \u0026#34;v1\u0026#34; parameters: InstanceType: default: \u0026#34;t2.medium, t2.large, t2.xlarge\u0026#34; deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34; aws-config-rds-storage-encrypted: portfolio: \u0026#34;reinvent-cloud-engineering-governance\u0026#34; product: \u0026#34;aws-config-rds-storage-encrypted\u0026#34; version: \u0026#34;v1\u0026#34; deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;   AWS Committing the manifest file Now that we have written the manifest file we are ready to commit it.\n Set your Author name Set your Email address Set your Commit message  Using a good / unique commit message will help you understand what is going on later.\n  Click the Commit changes button:    What did we just do? The YAML we pasted in the previous step told the framework to perform the following actions:\n provision a product named aws-config-rds-storage-encrypted into each of the enabled regions of the account  Verifying the provisioning Once you have made your changes the ServiceCatalogPuppet Pipeline should have run. If you were quick may still be running. If it has not yet started feel free to the hit the Release change button.\nOnce it has completed it should show the Source, Generate and Deploy stages in green to indicate they have completed successfully:\n  If this is failing please raise your hand for some assistance\n Once you have verified the pipeline has run you can go to Service Catalog provisioned products to view your provisioned product. Please note when you arrive at the provisioned product page you will need to select account from the filter by drop down in the top right:\n  If you cannot see your product please raise your hand for some assistance\n You have now successfully provisioned a product\nVerify the AWS Config rule is enabled To see the AWS Config rule enabled, navigate to AWS Config rules. Once there you should see the following:\n  "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/every-day-use/200-provisioning-a-product.html",
	"title": "Provisioning a product",
	"tags": [],
	"description": "",
	"content": " What are we going to do? This tutorial will walk you through \u0026ldquo;Provisioning a product\u0026rdquo; into a spoke account.\nWe will assume you have:\n installed Service Catalog Puppet correctly bootstrapped a spoke you have created a product you have created a portfolio  We are going to perform the following steps:\n create a manifest file add an account to the manifest file add a launch to the manifest file  During this process you will check your progress by verifying what the framework is doing at each step.\nStep by step guide Here are the steps you need to follow to \u0026ldquo;Provisioning a product\u0026rdquo;\nCreating the manifest file   Navigate to the ServiceCatalogPuppet CodeCommit repository\n  Scroll down to the bottom of the page and hit the Create file button\n    Adding an account to the manifest file   Copy the following snippet into the main input field:\n accounts: - account_id: \u0026#34;\u0026lt;YOUR_ACCOUNT_ID_WITHOUT_HYPHENS\u0026gt;\u0026#34; name: \u0026#34;puppet-account\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; - \u0026#34;eu-west-2\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34;     Update account_id on line to show your account id\n  Adding a launch to the manifest Now we are ready to add a product to the manifest file.\n Add the following snippet to the end of the main input field:   launches: aws-config-enable-config: portfolio: \u0026#34;reinvent-cloud-engineering-governance\u0026#34; product: \u0026#34;aws-config-enable-config\u0026#34; version: \u0026#34;v1\u0026#34; deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;    The main input field should look like this:   accounts: - account_id: \u0026#34;\u0026lt;YOUR_ACCOUNT_ID_WITHOUT_HYPHENS\u0026gt;\u0026#34; name: \u0026#34;puppet-account\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; - \u0026#34;eu-west-2\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34; launches: aws-config-enable-config: portfolio: \u0026#34;reinvent-cloud-engineering-governance\u0026#34; product: \u0026#34;aws-config-enable-config\u0026#34; version: \u0026#34;v1\u0026#34; deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;   Committing the manifest file Now that we have written the manifest file we are ready to commit it.\n  Set the File name to manifest.yaml\n  Set your Author name\n  Set your Email address\n  Set your Commit message\n  Using a good / unique commit message will help you understand what is going on later.\n  Click the Commit changes button:    Verifying the provisioning Once you have made your changes the ServiceCatalogPuppet Pipeline should have run or if you were quick may still be running. If it has not yet started feel free to the hit the Release change button.\nOnce it has completed it should show the Source and Deploy stages in green to indicate they have completed successfully:\n  Once you have verified the pipeline has run you can go to Service Catalog provisioned products to view your provisioned product. Please note when you arrive at the provisioned product page you will need to select account from the filter by drop down in the top right:\n  You have now successfully provisioned a product! When provisioned, this product will automatically enable AWS Config.\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/2021-workshop/200-provisioning-a-vpc.html",
	"title": "Provisioning a VPC",
	"tags": [],
	"description": "",
	"content": " What are we going to do? By now, you should have created a stack containing a lambda, provisioned it, invoked the lambda and then asserted the effect of the lambda invocation.\nNow the default networking resources are removed, you are ready to provision a new VPC. The following section will show you what it is like to create a VPC and storing a stack output in AWS Systems Manager Parameter Store for later use.\nHere are the steps you will need to follow:\n Creating the Stack   Provisioning the Stack   "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/design-considerations/account-tagging.html",
	"title": "Account Tagging",
	"tags": [],
	"description": "",
	"content": " What are we going to do? This article will explain how tags are used by the Service Catalog Tools and will make some recommendations on which tags you should be thinking about using.\nHow does Service Catalog Tools use account tags? When writing your manifest you can specify tags for AWS Accounts:\n accounts: - account_id: 012345678910\u0026#34; name: \u0026#34;prod-member-service-9\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; - \u0026#34;eu-west-2\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34;    The tags assigned to this account are type:prod and partition:eu.\nThese tags are later used in the launches and spoke-portfolio-shares sections of the manifest file to choose which AWS Accounts should have products provisioned into them and which AWS Accounts should have portfolios shared with them:\n launches: aws-config-rds-storage-encrypted: portfolio: \u0026#34;reinvent-cloud-engineering-governance\u0026#34; product: \u0026#34;aws-config-rds-storage-encrypted\u0026#34; version: \u0026#34;v1\u0026#34; deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34;  regions: \u0026#34;default_region\u0026#34; spoke-local-portfolios: cloud-engineering-self-service: portfolio: \u0026#34;reinvent-cloud-engineering-self-service\u0026#34; deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34;  regions: \u0026#34;default_region\u0026#34;   The Service Catalog Tools looks through the launches and the spoke-local-portfolios. For each launch and spoke-local-portfolio found the framework will look through the tags specified in the tags section. For each tag found the Service Catalog Tools will look through the list of accounts for an account with the same tag. When the tag is found the product is provisioned if the tag was found in the launch section otherwise the portfolio specified will be shared if the tag was found in a spoke-local-portfolio.\nHow you can make best use of them Having the right number of tags is essential. Too few tags will cause you to have less flexibility but having too many may lead to a larger than needed manifest file or feeling overwhelmed.\nTo begin with, we recommend using foundation and additional tags to align to the multi-account strategy best practice:\n outype:foundational outype:additional  We then recommend describing which OU the AWS Accounts are in:\n ou:sharedservices ou:networking ou:securityreadonly  We then recommend the following tags based on the type of the workloads that exist in the AWS account:\n type:prod type:test type:dev type:sandbox type:suspended  We then recommend using a set of scope tags to help explain the governance needs of the accounts:\n scope:pci scope:pii scope:hipaa  We may also want to classify the account by the confidentiality of the data within it\n confidentiality:highly confidentiality:medium confidentiality:public  It may also be convenient to tag the accounts with the team or business unit:\n team:ccoe team:member-services team:mobile-banking businessunit:security  The tags you specify within the manifest are not applied to the accounts using AWS Organizations - they only exist within the manifest file. You can change them at any time and renaming them will not result in changes.\nIf you would like to share your tagging patterns raise a github issue to share\n "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/every-day-use/100-creating-a-product/300-add-the-source-code.html",
	"title": "Add the source code",
	"tags": [],
	"description": "",
	"content": " What are we going to do? We are going to perform the following steps:\n Add the source code for the version of the AWS Service Catalog product we have just created  Step by step guide Here are the steps you need to follow to \u0026ldquo;Add the source code\u0026rdquo;\nAdd the source code for your product When you configured your product version, you specified the following:\n Versions: - Name: \u0026#34;v1\u0026#34; Description: \u0026#34;v1 of aws-config-enable-config\u0026#34; Active: True Source: Provider: \u0026#34;CodeCommit\u0026#34; Configuration: RepositoryName: \u0026#34;aws-config-enable-config\u0026#34; BranchName: \u0026#34;main\u0026#34;   We now need to create the AWS CodeCommit repository and add the AWS CloudFormation template we are going to use for our product into that repository.\n  Navigate to AWS CodeCommit\n  Click Create repository\n     Input the name aws-config-enable-config     Click Create     Scroll down to the bottom of the page and hit the Create file button      Copy the following snippet into the main input field:\n AWSTemplateFormatVersion: 2010-09-09 Description: | This template creates a Config Recorder and an Amazon S3 bucket where logs are published. Resources: ConfigRole: Type: \u0026#39;AWS::IAM::Role\u0026#39; Description: The IAM role used to configure AWS Config Properties: AssumeRolePolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Principal: Service: - config.amazonaws.com Action: - \u0026#39;sts:AssumeRole\u0026#39; ManagedPolicyArns: - arn:aws:iam::aws:policy/service-role/AWSConfigRole Policies: - PolicyName: root PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: \u0026#39;s3:GetBucketAcl\u0026#39; Resource: !Sub arn:aws:s3:::${S3ConfigBucket} - Effect: Allow Action: \u0026#39;s3:PutObject\u0026#39; Resource: !Sub arn:aws:s3:::${S3ConfigBucket}/AWSLogs/${AWS::AccountId}/${AWS::Region} Condition: StringEquals: \u0026#39;s3:x-amz-acl\u0026#39;: bucket-owner-full-control - Effect: Allow Action: \u0026#39;config:Put*\u0026#39; Resource: \u0026#39;*\u0026#39; ConfigRecorder: Type: \u0026#39;AWS::Config::ConfigurationRecorder\u0026#39; DependsOn: ConfigRole Properties: Name: default RoleARN: !GetAtt ConfigRole.Arn DeliveryChannel: Type: \u0026#39;AWS::Config::DeliveryChannel\u0026#39; Properties: ConfigSnapshotDeliveryProperties: DeliveryFrequency: Six_Hours S3BucketName: !Ref S3ConfigBucket S3ConfigBucket: DeletionPolicy: Retain Description: S3 bucket with AES256 Encryption set Type: AWS::S3::Bucket Properties: BucketName: !Sub config-bucket-${AWS::AccountId} PublicAccessBlockConfiguration: BlockPublicAcls: True BlockPublicPolicy: True IgnorePublicAcls: True RestrictPublicBuckets: True BucketEncryption: ServerSideEncryptionConfiguration: - ServerSideEncryptionByDefault: SSEAlgorithm: AES256 S3ConfigBucketPolicy: Type: AWS::S3::BucketPolicy Description: S3 bucket policy Properties: Bucket: !Ref S3ConfigBucket PolicyDocument: Version: 2012-10-17 Statement: - Sid: AWSBucketPermissionsCheck Effect: Allow Principal: Service: - config.amazonaws.com Action: s3:GetBucketAcl Resource: - !Sub \u0026#34;arn:aws:s3:::${S3ConfigBucket}\u0026#34; - Sid: AWSBucketDelivery Effect: Allow Principal: Service: - config.amazonaws.com Action: s3:PutObject Resource: !Sub \u0026#34;arn:aws:s3:::${S3ConfigBucket}/AWSLogs/*/*\u0026#34; Outputs: ConfigRoleArn: Value: !GetAtt ConfigRole.Arn S3ConfigBucketArn: Value: !GetAtt S3ConfigBucket.Arn     Set the File name to product.template.yaml\n  Set your Author name\n  Set your Email address\n  Set your Commit message\n  Using a good / unique commit message will help you understand what is going on later.\n Creating that file should trigger your aws-config-enable-config-v1-pipeline.\nOnce the pipeline has completed it should show the Source, Package and Deploy stages in green to indicate they have completed successfully:\n  You should see your commit message on this screen, it will help you know which version of ServiceCatalogFactory repository the pipeline is processing.\n Once you have verified the pipeline has run you can go to Service Catalog products to view your newly created version.\nYou should see the product you created listed:\n  Click on the product and verify v1 is there\n  You have now successfully created a version for your product!\nSpecifying source code to import If you are using AWS CodeCommit as your SCM you are able to request the framework to create the git repository for you and you can specify an AWS S3 source for where the initial commit should come from:\n - Name: \u0026#34;vpc\u0026#34; Owner: \u0026#34;networking@example.com\u0026#34; Description: \u0026#34;vpc\u0026#34; Distributor: \u0026#34;cloud-engineering\u0026#34; SupportDescription: \u0026#34;Speak to networking@example.com for help\u0026#34; SupportEmail: \u0026#34;networking@example.com\u0026#34; SupportUrl: \u0026#34;https://wiki.example.com/cloud-engineering/networking/vpc\u0026#34; Source: Provider: \u0026#34;CodeCommit\u0026#34; Configuration: RepositoryName: \u0026#34;networking-vpc\u0026#34; Code: S3: Bucket: \u0026#34;service-catalog-tools-product-sets-eu-west-2\u0026#34; Versions: - Name: \u0026#34;v1\u0026#34; Description: \u0026#34;v1 of vpc\u0026#34; Active: True Source: Configuration: BranchName: \u0026#34;v1\u0026#34; Code: S3: Key: \u0026#34;product-sets/networking/vpc/v1/networking--vpc--v1.zip\u0026#34; Portfolios: - \u0026#34;demo-portfolio\u0026#34;   In the example above we are saying the initial source code should come from the S3 bucket named service-catalog-tools-product-sets-eu-west-2 using the key product-sets/networking/vpc/v1/networking\u0026ndash;vpc\u0026ndash;v1.zip\nYou can split the declaration between the product and version as we have in the example above or you could have specified all of the configuration under the version.\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/40-reinvent2019/150-task-2/300-create-the-product.html",
	"title": "Create the product",
	"tags": [],
	"description": "",
	"content": " What are we going to do? We have provisioned a detective control to look for AWS RDS Instances that have don't have encryption enabled. We can do better, and create an AWS Service Catalog product that meets the encryption requirement by default using service catalog tools. When users create a new RDS instance using this product, encryption at rest is enabled by default and no further configuration is required.\nWe are going to perform the following steps:\n define a product with a version and a portfolio add the source code for our product share that portfolio with a spoke account  Step by step guide Here are the steps you need to follow to \u0026ldquo;Create the product\u0026rdquo;\nDefine a product with a version and a portfolio   Navigate to the ServiceCatalogFactory CodeCommit repository again\n  Click on portfolios\n     Click on reinvent.yaml     Click Edit     Add the following to the products section:   - Name: \u0026#34;rds-instance\u0026#34; Owner: \u0026#34;data-governance@example.com\u0026#34; Description: \u0026#34;A compliant RDS Instance you can use that meets data governance standards\u0026#34; Distributor: \u0026#34;cloud-engineering\u0026#34; SupportDescription: \u0026#34;Speak to data-governance@example.com about exceptions and speak to cloud-engineering@example.com about implementation issues\u0026#34; SupportEmail: \u0026#34;cloud-engineering@example.com\u0026#34; SupportUrl: \u0026#34;https://wiki.example.com/cloud-engineering/data-governance/rds-instance\u0026#34; Options: ShouldCFNNag: True Tags: - Key: \u0026#34;type\u0026#34; Value: \u0026#34;governance\u0026#34; - Key: \u0026#34;creator\u0026#34; Value: \u0026#34;cloud-engineering\u0026#34; Versions: - Name: \u0026#34;v1\u0026#34; Description: \u0026#34;v1 of rds-instance\u0026#34; Active: True Source: Provider: \u0026#34;CodeCommit\u0026#34; Configuration: RepositoryName: \u0026#34;rds-instance\u0026#34; BranchName: \u0026#34;main\u0026#34; Portfolios: - \u0026#34;cloud-engineering-self-service\u0026#34;    Add the following to the portfolios section:   - DisplayName: \u0026#34;cloud-engineering-self-service\u0026#34; Description: \u0026#34;Portfolio containing products that you can use to ensure you meet the governance guidelines\u0026#34; ProviderName: \u0026#34;cloud-engineering\u0026#34; Associations: - \u0026#34;arn:aws:iam::${AWS::AccountId}:role/TeamRole\u0026#34; Tags: - Key: \u0026#34;type\u0026#34; Value: \u0026#34;governance\u0026#34; - Key: \u0026#34;creator\u0026#34; Value: \u0026#34;cloud-engineering\u0026#34;    Once completed it should like look this:   Schema: factory-2019-04-01 Products: - Name: \u0026#34;aws-config-desired-instance-types\u0026#34; Owner: \u0026#34;budget-and-cost-governance@example.com\u0026#34; Description: \u0026#34;Enables AWS Config rule - desired-instance-type with our RIs\u0026#34; Distributor: \u0026#34;cloud-engineering\u0026#34; SupportDescription: \u0026#34;Speak to budget-and-cost-governance@example.com about exceptions and speak to cloud-engineering@example.com about implementation issues\u0026#34; SupportEmail: \u0026#34;cloud-engineering@example.com\u0026#34; SupportUrl: \u0026#34;https://wiki.example.com/cloud-engineering/budget-and-cost-governance/aws-config-desired-instance-types\u0026#34; Tags: - Key: \u0026#34;type\u0026#34; Value: \u0026#34;governance\u0026#34; - Key: \u0026#34;creator\u0026#34; Value: \u0026#34;cloud-engineering\u0026#34; - Key: \u0026#34;cost-center\u0026#34; Value: \u0026#34;governance\u0026#34; Versions: - Name: \u0026#34;v1\u0026#34; Description: \u0026#34;v1 of aws-config-desired-instance-types\u0026#34; Active: True Source: Provider: \u0026#34;CodeCommit\u0026#34; Configuration: RepositoryName: \u0026#34;aws-config-desired-instance-types\u0026#34; BranchName: \u0026#34;main\u0026#34; Portfolios: - \u0026#34;cloud-engineering-governance\u0026#34; - Name: \u0026#34;aws-config-rds-storage-encrypted\u0026#34; Owner: \u0026#34;data-governance@example.com\u0026#34; Description: \u0026#34;Enables AWS Config rule - aws-config-rds-storage-encrypted\u0026#34; Distributor: \u0026#34;cloud-engineering\u0026#34; SupportDescription: \u0026#34;Speak to data-governance@example.com about exceptions and speak to cloud-engineering@example.com about implementation issues\u0026#34; SupportEmail: \u0026#34;cloud-engineering@example.com\u0026#34; SupportUrl: \u0026#34;https://wiki.example.com/cloud-engineering/data-governance/aws-config-rds-storage-encrypted\u0026#34; Tags: - Key: \u0026#34;type\u0026#34; Value: \u0026#34;governance\u0026#34; - Key: \u0026#34;creator\u0026#34; Value: \u0026#34;cloud-engineering\u0026#34; - Key: \u0026#34;cost-center\u0026#34; Value: \u0026#34;governance\u0026#34; Versions: - Name: \u0026#34;v1\u0026#34; Description: \u0026#34;v1 of aws-config-rds-storage-encrypted\u0026#34; Active: True Source: Provider: \u0026#34;CodeCommit\u0026#34; Configuration: RepositoryName: \u0026#34;aws-config-rds-storage-encrypted\u0026#34; BranchName: \u0026#34;main\u0026#34; Portfolios: - \u0026#34;cloud-engineering-governance\u0026#34; - Name: \u0026#34;rds-instance\u0026#34; Owner: \u0026#34;data-governance@example.com\u0026#34; Description: \u0026#34;A compliant RDS Instance you can use that meets data governance standards\u0026#34; Distributor: \u0026#34;cloud-engineering\u0026#34; SupportDescription: \u0026#34;Speak to data-governance@example.com about exceptions and speak to cloud-engineering@example.com about implementation issues\u0026#34; SupportEmail: \u0026#34;cloud-engineering@example.com\u0026#34; SupportUrl: \u0026#34;https://wiki.example.com/cloud-engineering/data-governance/rds-instance\u0026#34; Options: ShouldCFNNag: True  Tags: - Key: \u0026#34;type\u0026#34; Value: \u0026#34;governance\u0026#34; - Key: \u0026#34;creator\u0026#34; Value: \u0026#34;cloud-engineering\u0026#34; Versions: - Name: \u0026#34;v1\u0026#34; Description: \u0026#34;v1 of rds-instance\u0026#34; Active: True Source: Provider: \u0026#34;CodeCommit\u0026#34; Configuration: RepositoryName: \u0026#34;rds-instance\u0026#34; BranchName: \u0026#34;main\u0026#34; Portfolios: - \u0026#34;cloud-engineering-self-service\u0026#34; Portfolios: - DisplayName: \u0026#34;cloud-engineering-governance\u0026#34; Description: \u0026#34;Portfolio containing the products needed to govern AWS accounts\u0026#34; ProviderName: \u0026#34;cloud-engineering\u0026#34; Associations: - \u0026#34;arn:aws:iam::${AWS::AccountId}:role/TeamRole\u0026#34; Tags: - Key: \u0026#34;type\u0026#34; Value: \u0026#34;governance\u0026#34; - Key: \u0026#34;creator\u0026#34; Value: \u0026#34;cloud-engineering\u0026#34; - Key: \u0026#34;cost-center\u0026#34; Value: \u0026#34;governance\u0026#34; - DisplayName: \u0026#34;cloud-engineering-self-service\u0026#34; Description: \u0026#34;Portfolio containing products that you can use to ensure you meet the governance guidelines\u0026#34; ProviderName: \u0026#34;cloud-engineering\u0026#34; Associations: - \u0026#34;arn:aws:iam::${AWS::AccountId}:role/TeamRole\u0026#34; Tags: - Key: \u0026#34;type\u0026#34; Value: \u0026#34;governance\u0026#34; - Key: \u0026#34;creator\u0026#34; Value: \u0026#34;cloud-engineering\u0026#34;   Have a look at the highlighted lines. We are using this to turn on cfn-nag, an open source tool by Stelligent that looks for insecure configuration of resources. This will add an extra layer of governance ensuring the AWS CloudFormation templates we are using meets the quality bar set by cfn-nag.\n   Set your Author name\n  Set your Email address\n  Set your Commit message\n  Using a good / unique commit message will help you understand what is going on later.\n  Click the Commit changes button:    What did we just do? The YAML we pasted in the previous step told the framework to perform several actions:\n create a product named rds-instance add a v1 of our product create a portfolio named cloud-engineering-self-service add the product: rds-instance to the portfolio: cloud-engineering-self-service  Verify the change worked Once you have made your changes the ServiceCatalogFactory Pipeline should have run. If you were very quick, the pipeline may still be running. If it has not yet started feel free to the hit the Release change button.\nOnce it has completed it should show the Source and Build stages in green to indicate they have completed successfully:\n  If this is failing please raise your hand for some assistance\n Add the source code for our product When you configured your product version, you specified the following version:\n Versions: - Name: \u0026#34;v1\u0026#34; Description: \u0026#34;v1 of rds-instance\u0026#34; Active: True Source: Provider: \u0026#34;CodeCommit\u0026#34; Configuration: RepositoryName: \u0026#34;rds-instance\u0026#34; BranchName: \u0026#34;main\u0026#34;   This tells the framework the source code for the product comes from the main branch of a CodeCommit repository of the name rds-instance.\nWe now need to create the CodeCommit repository and add the AWS CloudFormation template we are going to use for our product.\n  Navigate to AWS CodeCommit\n  Click Create repository\n     Input the name rds-instance     Click Create     Scroll down to the bottom of the page and hit the Create file button     Copy the following snippet into the main input field:   AWSTemplateFormatVersion: 2010-09-09 Description: \u0026#34;RDS Storage Encrypted\u0026#34; Parameters: RdsDbMasterUsername: Description: RdsDbMasterUsername Type: String Default: someuser RdsDbMasterUserPassword: Description: RdsDbMasterUserPassword Type: String NoEcho: true RdsDbDatabaseName: Description: DbDatabaseName Type: String Default: mysql57_database Resources: VPC: Type: AWS::EC2::VPC Properties: CidrBlock: 10.0.0.0/16 EnableDnsSupport: \u0026#39;false\u0026#39; EnableDnsHostnames: \u0026#39;false\u0026#39; Subnet1: Type: AWS::EC2::Subnet Properties: VpcId: Ref: VPC CidrBlock: 10.0.0.0/24 AvailabilityZone: !Select [0, !GetAZs {Ref: \u0026#39;AWS::Region\u0026#39;}] Subnet2: Type: AWS::EC2::Subnet Properties: VpcId: Ref: VPC CidrBlock: 10.0.1.0/24 AvailabilityZone: !Select [1, !GetAZs {Ref: \u0026#39;AWS::Region\u0026#39;}] RdsDbSubnetGroup: Type: AWS::RDS::DBSubnetGroup Properties: DBSubnetGroupDescription: Database subnets for RDS SubnetIds: - !Ref Subnet1 - !Ref Subnet2 RdsSecurityGroup: Type: AWS::EC2::SecurityGroup Description: Used to grant access to and from the VPC Properties: VpcId: !Ref VPC GroupDescription: Allow MySQL (TCP3306) access to and from the VPC SecurityGroupIngress: - IpProtocol: tcp FromPort: 3306 ToPort: 3306 CidrIp: 10.0.0.0/32 SecurityGroupEgress: - IpProtocol: tcp FromPort: 3306 ToPort: 3306 CidrIp: 10.0.0.0/32 RdsDbClusterParameterGroup: Type: AWS::RDS::DBClusterParameterGroup Properties: Description: CloudFormation Aurora Cluster Parameter Group Family: aurora-mysql5.7 Parameters: server_audit_logging: 0 server_audit_events: \u0026#39;CONNECT,QUERY,QUERY_DCL,QUERY_DDL,QUERY_DML,TABLE\u0026#39; RdsDbCluster: Type: AWS::RDS::DBCluster Properties: DBSubnetGroupName: !Ref RdsDbSubnetGroup MasterUsername: !Ref RdsDbMasterUsername MasterUserPassword: !Ref RdsDbMasterUserPassword DatabaseName: !Ref RdsDbDatabaseName Engine: aurora-mysql VpcSecurityGroupIds: - !Ref RdsSecurityGroup DBClusterIdentifier : !Sub \u0026#39;${AWS::StackName}-dbcluster\u0026#39; DBClusterParameterGroupName: !Ref RdsDbClusterParameterGroup PreferredBackupWindow: 18:05-18:35 RdsDbParameterGroup: Type: AWS::RDS::DBParameterGroup Properties: Description: CloudFormation Aurora Parameter Group Family: aurora-mysql5.7 Parameters: aurora_lab_mode: 0 general_log: 1 slow_query_log: 1 long_query_time: 10 RdsDbInstance: Type: AWS::RDS::DBInstance Properties: DBSubnetGroupName: !Ref RdsDbSubnetGroup DBParameterGroupName: !Ref RdsDbParameterGroup Engine: aurora-mysql DBClusterIdentifier: !Ref RdsDbCluster AutoMinorVersionUpgrade: \u0026#39;true\u0026#39; PubliclyAccessible: \u0026#39;false\u0026#39; PreferredMaintenanceWindow: Thu:19:05-Thu:19:35 AvailabilityZone: !Select [0, !GetAZs {Ref: \u0026#39;AWS::Region\u0026#39;}] DBInstanceClass: \u0026#39;db.t2.small\u0026#39;     Set the File name to product.template.yaml\n  Set your Author name\n  Set your Email address\n  Set your Commit message\n  Click Commit changes\n  Using a good / unique commit message will help you understand what is going on later.\n Creating that file should trigger your rds-instance-v1-pipeline.\nOnce the pipeline has completed it should show the Source stage in green to indicate it has completed successfully but it should show the CFNNag action within the Tests stage as failing:\n  Clicking the Details link within the CFNNag box will bring you to the AWS CodeBuild project. When you scroll near to the bottom of that page you should see an error:\n·[0;31;49m| FAIL F26·[0m ·[0;31;49m|·[0m ·[0;31;49m| Resources: [\u0026#34;RdsDbCluster\u0026#34;]·[0m ·[0;31;49m| Line Numbers: [84]·[0m ·[0;31;49m|·[0m ·[0;31;49m| RDS DBCluster should have StorageEncrypted enabled·[0m CFNNag has determined you are not applying encryption to your DBCluster. This is a violation of the data governance guidelines and so we need to fix it.\n  Go to AWS CodeCommit\n  Click on the rds-instance repository\n  Click on product.template.yaml\n  Click on edit\n  Replace the contents with this:\n   AWSTemplateFormatVersion: 2010-09-09 Description: \u0026#34;RDS Storage Encrypted\u0026#34; Parameters: RdsDbMasterUsername: Description: RdsDbMasterUsername Type: String Default: someuser RdsDbMasterUserPassword: Description: RdsDbMasterUserPassword Type: String NoEcho: true RdsDbDatabaseName: Description: DbDatabaseName Type: String Default: mysql57_database Resources: VPC: Type: AWS::EC2::VPC Properties: CidrBlock: 10.0.0.0/16 EnableDnsSupport: \u0026#39;false\u0026#39; EnableDnsHostnames: \u0026#39;false\u0026#39; Subnet1: Type: AWS::EC2::Subnet Properties: VpcId: Ref: VPC CidrBlock: 10.0.0.0/24 AvailabilityZone: !Select [0, !GetAZs {Ref: \u0026#39;AWS::Region\u0026#39;}] Subnet2: Type: AWS::EC2::Subnet Properties: VpcId: Ref: VPC CidrBlock: 10.0.1.0/24 AvailabilityZone: !Select [1, !GetAZs {Ref: \u0026#39;AWS::Region\u0026#39;}] RdsDbSubnetGroup: Type: AWS::RDS::DBSubnetGroup Properties: DBSubnetGroupDescription: Database subnets for RDS SubnetIds: - !Ref Subnet1 - !Ref Subnet2 RdsSecurityGroup: Type: AWS::EC2::SecurityGroup Description: Used to grant access to and from the VPC Properties: VpcId: !Ref VPC GroupDescription: Allow MySQL (TCP3306) access to and from the VPC SecurityGroupIngress: - IpProtocol: tcp FromPort: 3306 ToPort: 3306 CidrIp: 10.0.0.0/32 SecurityGroupEgress: - IpProtocol: tcp FromPort: 3306 ToPort: 3306 CidrIp: 10.0.0.0/32 RdsDbClusterParameterGroup: Type: AWS::RDS::DBClusterParameterGroup Properties: Description: CloudFormation Aurora Cluster Parameter Group Family: aurora-mysql5.7 Parameters: server_audit_logging: 0 server_audit_events: \u0026#39;CONNECT,QUERY,QUERY_DCL,QUERY_DDL,QUERY_DML,TABLE\u0026#39; RdsDbCluster: Type: AWS::RDS::DBCluster Properties: DBSubnetGroupName: !Ref RdsDbSubnetGroup MasterUsername: !Ref RdsDbMasterUsername MasterUserPassword: !Ref RdsDbMasterUserPassword DatabaseName: !Ref RdsDbDatabaseName Engine: aurora-mysql VpcSecurityGroupIds: - !Ref RdsSecurityGroup DBClusterIdentifier : !Sub \u0026#39;${AWS::StackName}-dbcluster\u0026#39; DBClusterParameterGroupName: !Ref RdsDbClusterParameterGroup PreferredBackupWindow: 18:05-18:35 StorageEncrypted: True  RdsDbParameterGroup: Type: AWS::RDS::DBParameterGroup Properties: Description: CloudFormation Aurora Parameter Group Family: aurora-mysql5.7 Parameters: aurora_lab_mode: 0 general_log: 1 slow_query_log: 1 long_query_time: 10 RdsDbInstance: Type: AWS::RDS::DBInstance Properties: DBSubnetGroupName: !Ref RdsDbSubnetGroup DBParameterGroupName: !Ref RdsDbParameterGroup Engine: aurora-mysql DBClusterIdentifier: !Ref RdsDbCluster AutoMinorVersionUpgrade: \u0026#39;true\u0026#39; PubliclyAccessible: \u0026#39;false\u0026#39; PreferredMaintenanceWindow: Thu:19:05-Thu:19:35 AvailabilityZone: !Select [0, !GetAZs {Ref: \u0026#39;AWS::Region\u0026#39;}] DBInstanceClass: \u0026#39;db.t2.small\u0026#39; StorageEncrypted: True    Please observe the highlighted lines showing where we have made a change. We have added:\nStorageEncrypted: True  Set your Author name Set your Email address Set your Commit message Click Commit changes  Using a good / unique commit message will help you understand what is going on later.\n Creating that file should trigger your rds-instance-v1-pipeline.\nOnce the pipeline has completed it should show the Source and Tests stages in green to indicate they have completed successfully:\n  You should see your commit message on this screen, it will help you know which version of ServiceCatalogFactory repository the pipeline is processing.\n If this is failing please raise your hand for some assistance\n Once you have verified the pipeline has run you can go to Service Catalog products to view your newly created version.\nYou should see the product you created listed:\n  Click on the product and verify v1 is there\n  If you cannot see your version please raise your hand for some assistance\n You have now successfully created a version for your product!\nVerify that the product was added to the portfolio Now that you have verified the pipeline has run you can go to Service Catalog portfolios to view your portfolio.\n Click on cloud-engineering-self-service      Click on the product rds-instance\n  Click on the version v1\n    Share portfolio with a spoke account   Navigate to the ServiceCatalogPuppet CodeCommit repository again\n  Click on manifest.yaml\n  Click Edit\n     Append the following snippet to the YAML document in the main input field (be careful with your indentation):   spoke-local-portfolios: cloud-engineering-self-service: portfolio: \u0026#34;reinvent-cloud-engineering-self-service\u0026#34; deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;    The main input field should look like this:   accounts: - account_id: \u0026#34;\u0026lt;YOUR_ACCOUNT_ID_WITHOUT_HYPHENS\u0026gt;\u0026#34; name: \u0026#34;puppet-account\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34; launches: aws-config-desired-instance-types: portfolio: \u0026#34;reinvent-cloud-engineering-governance\u0026#34; product: \u0026#34;aws-config-desired-instance-types\u0026#34; version: \u0026#34;v1\u0026#34; parameters: InstanceType: default: \u0026#34;t2.medium, t2.large, t2.xlarge\u0026#34; deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34; aws-config-rds-storage-encrypted: portfolio: \u0026#34;reinvent-cloud-engineering-governance\u0026#34; product: \u0026#34;aws-config-rds-storage-encrypted\u0026#34; version: \u0026#34;v1\u0026#34; deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34; spoke-local-portfolios: cloud-engineering-self-service: portfolio: \u0026#34;reinvent-cloud-engineering-self-service\u0026#34; deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;   Committing the manifest file   Set your Author name\n  Set your Email address\n  Set your Commit message\n  Using a good / unique commit message will help you understand what is going on later.\n  Click the Commit changes button:    Verifying the sharing Once you have made your changes the ServiceCatalogPuppet Pipeline should have run. If you were quick may still be running. If it has not yet started feel free to the hit the Release change button.\nOnce it has completed it should show the Source and Build stages in green to indicate they have completed successfully:\n  If this is failing please raise your hand for some assistance\n Once you have verified the pipeline has run you can go to Service Catalog portfolios to view your shared product.\nWhen you share a portfolio the framework will decide if it should share the portfolio. If the target account is the same as the factory account it will not share the portfolio as it is not needed.\nIf you cannot see your product please raise your hand for some assistance\n "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/every-day-use/110-deleting-a-product/300-deleting-the-product.html",
	"title": "Deleting the product",
	"tags": [],
	"description": "",
	"content": " What are we going to do? We are going to perform the following steps:\n delete a product version delete a product  Step by step guide Here are the steps you need to follow to \u0026ldquo;Deleting the product\u0026rdquo;\nDelete a product version When you are ready to delete a product version you will need to edit its definition in the portfolio yaml.\n  Navigate to the ServiceCatalogFactory CodeCommit repository\n  Click on portfolios\n      Click on the portfolio yaml containing your product\n  Click Edit\n  Add or set the attribute Status for the version you want to delete to terminated:\n Schema: factory-2019-04-01 Products: - Name: account-vending-machine Owner: central-it@customer.com Description: The iam roles needed for you to do your jobs Distributor: central-it-team SupportDescription: Contact us on Chime for help #central-it-team SupportEmail: central-it-team@customer.com SupportUrl: https://wiki.customer.com/central-it-team/self-service/account-iam  Tags: - Key: product-type Value: iam Versions: - Name: v1 Description: The iam roles needed for you to do your jobs Status: terminated  Source: Provider: CodeCommit Configuration: RepositoryName: account-vending-machine BranchName: v1     Set your Author name\n  Set your Email address\n  Set your Commit message\n  Click the Commit changes button:\n    When the framework runs the product version will be deleted. This change will only affect the version of the product in your factory account. If you are using the imported product in your spoke accounts it will have affect there otherwise you will need to run service-catalog-puppet to cascade the change.\nYou can verify this by navigating to Service Catalog and checking your disabled version is removed.\nDelete a product When you are ready to delete a product you will need to edit its definition in the portfolio yaml.\n  Navigate to the ServiceCatalogFactory CodeCommit repository\n  Click on portfolios\n      Click on the portfolio yaml containing your product\n  Click Edit\n  Add or set the attribute Status for the product you want to delete to terminated:\n Schema: factory-2019-04-01 Products: - Name: account-vending-machine Status: terminated  Owner: central-it@customer.com Description: The iam roles needed for you to do your jobs Distributor: central-it-team SupportDescription: Contact us on Chime for help #central-it-team SupportEmail: central-it-team@customer.com SupportUrl: https://wiki.customer.com/central-it-team/self-service/account-iam  Tags: - Key: product-type Value: iam Versions: - Name: v1 Description: The iam roles needed for you to do your jobs Source: Provider: CodeCommit Configuration: RepositoryName: account-vending-machine BranchName: v1     Set your Author name\n  Set your Email address\n  Set your Commit message\n  Click the Commit changes button:\n    When the framework runs the product will be deleted. This change will only affect the version of the product in your factory account. If you are using the imported product in your spoke accounts it will have affect there otherwise you will need to run service-catalog-puppet to cascade the change. If you are using imported products in your spokes then the product will be deleted there.\nYou can verify this by navigating to Service Catalog and checking your disabled version is removed.\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/2021-workshop/300-provisioning-subnets.html",
	"title": "Provisioning Subnets",
	"tags": [],
	"description": "",
	"content": " What are we going to do? By now, you will have removed the default networking resources and provisioned a new VPC. You are now ready to start creating some subnets.\nWe are going to create a subnet by provisioning an AWS Service Catalog product using a launch and then we are going to provision another subnet using Hashicorp Terraform using a workspace.\nHere are the steps to do this:\n Using AWS Service Catalog   Using Terraform   "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/every-day-use/300-sharing-a-portfolio.html",
	"title": "Sharing a portfolio",
	"tags": [],
	"description": "",
	"content": " What are we going to do? This tutorial will walk you through \u0026ldquo;Sharing a portfolio\u0026rdquo; with a spoke account.\nWe will assume you have:\n installed Service Catalog Puppet correctly bootstrapped a spoke you have created a product you have created a portfolio  We are going to perform the following steps:\n create a manifest file add an account to the manifest file add a spoke-local-portfolios to the manifest file  During this process you will check your progress by verifying what the framework is doing at each step.\nStep by step guide Here are the steps you need to follow to \u0026ldquo;Sharing a portfolio\u0026rdquo;\nCreating the manifest file   Navigate to the ServiceCatalogPuppet CodeCommit repository\n  Scroll down to the bottom of the page and hit the Create file button\n    Adding an account to the manifest file We will start out by adding your account to the manifest file.\n  Navigate to the ServiceCatalogPuppet CodeCommit repository\n  Scroll down to the bottom of the page and hit the Create file button\n      Copy the following snippet into the main input field:\n accounts: - account_id: \u0026#34;\u0026lt;YOUR_ACCOUNT_ID_WITHOUT_HYPHENS\u0026gt;\u0026#34; name: \u0026#34;puppet-account\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; - \u0026#34;eu-west-2\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34;     Update account_id on line to show your account id\n  Adding spoke-local-portfolio to the manifest Now we are ready to add a product to the manifest file.\n Add the following snippet to the end of the main input field:   spoke-local-portfolios: account-vending-for-spokes: portfolio: reinvent-cloud-engineering-governance deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;    The main input field should look like this:   accounts: - account_id: \u0026#34;\u0026lt;YOUR_ACCOUNT_ID_WITHOUT_HYPHENS\u0026gt;\u0026#34; name: \u0026#34;puppet-account\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; - \u0026#34;eu-west-2\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34; spoke-local-portfolios: account-vending-for-spokes: portfolio: reinvent-cloud-engineering-governance deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;   Committing the manifest file Now that we have written the manifest file we are ready to commit it.\n  Set the File name to manifest.yaml\n  Set your Author name\n  Set your Email address\n  Set your Commit message\n  Using a good / unique commit message will help you understand what is going on later.\n  Click the Commit changes button:    Verifying the sharing Once you have made your changes the ServiceCatalogPuppet Pipeline should have run or if you were quick may still be running. If it has not yet started feel free to the hit the Release change button.\nOnce it has completed it should show the Source and Deploy stages in green to indicate they have completed successfully:\n  Once you have verified the pipeline has run you can go to Service Catalog portfolios to view your shared product.\nWhen you share a portfolio the framework will decide if it should share the portfolio. If the target account is the same as the factory account it will not share the portfolio as it is not needed.\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/every-day-use/100-creating-a-product/305-creating-codestar-pipelines.html",
	"title": "Creating CodeStar pipelines",
	"tags": [],
	"description": "",
	"content": " What are we going to do? We are going to perform the following steps:\n How to use AWS CodeStar Connections as a source for your pipelines (for Github.com/Github Enterprise and BitBucket Cloud)  Step by step guide Here are the steps you need to follow to \u0026ldquo;Creating CodeStar pipelines\u0026rdquo;\nAdd the source code for your product When you configured your product version, you may have specified the following:\n Versions: - Name: \u0026#34;v1\u0026#34; Description: \u0026#34;v1 of aws-config-enable-config\u0026#34; Source: Provider: \u0026#34;CodeCommit\u0026#34; Configuration: RepositoryName: \u0026#34;aws-config-enable-config\u0026#34; BranchName: \u0026#34;v1\u0026#34;   This would have taken your products source code from CodeCommit. If you are using BitBucket Cloud, Github.com or Github Enterprise (where you can use AWS CodeStar Connections) you may want to use CodeStar Connections to simplify the creation of product version pipelines.\nTo do so, you can specify the following:\n Versions: - Name: \u0026#34;v1\u0026#34; Description: \u0026#34;v1 of aws-config-enable-config\u0026#34; Source: Provider: \u0026#34;CodeStarSourceConnection\u0026#34; Configuration: BranchName: \u0026#34;v1\u0026#34; ConnectionArn: \u0026#34;arn:aws:codestar-connections:eu-west-1:0123456789010:connection/eb6703af-6407-0522dc6a6\u0026#34; FullRepositoryId: \u0026#34;exampleorg/aws-config-enable-config\u0026#34;   "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/every-day-use/100-creating-a-product/310-creating-s3-pipelines.html",
	"title": "Creating S3 pipelines",
	"tags": [],
	"description": "",
	"content": " What are we going to do? We are going to perform the following steps:\n How to use S3 as a source for your pipelines  Step by step guide Here are the steps you need to follow to \u0026ldquo;Creating S3 pipelines\u0026rdquo;\nAdd the source code for your product When you configured your product version, you may have specified the following:\n Versions: - Name: \u0026#34;v1\u0026#34; Description: \u0026#34;v1 of aws-config-enable-config\u0026#34; Source: Provider: \u0026#34;CodeCommit\u0026#34; Configuration: RepositoryName: \u0026#34;aws-config-enable-config\u0026#34; BranchName: \u0026#34;v1\u0026#34;   This would have taken your products source code from CodeCommit. If you are using BitBucket Server or an SCM solution where you cannot use AWS CodeStar Connections you may want to configure your pipelines source your products from AWS S3 and then use another solution to put the source code there. To do so, you can specify the following:\n Versions: - Name: \u0026#34;v1\u0026#34; Description: \u0026#34;v1 of aws-config-enable-config\u0026#34; Source: Provider: \u0026#34;S3\u0026#34; Configuration: BucketName: \u0026#34;incomingproductchanges\u0026#34; S3ObjectKey: \u0026#34;aws-config-enable-config/v1/product.zip\u0026#34;   Please note, if you create S3 sourced pipelines you will be responsible for the creation of the AWS S3 Bucket.\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/every-day-use/350-adding-a-region.html",
	"title": "Adding a region",
	"tags": [],
	"description": "",
	"content": " What are we going to do? This tutorial will walk you through \u0026ldquo;Adding a region\u0026rdquo;.\nWe will assume you have:\n installed Service Catalog Factory correctly installed Service Catalog Puppet correctly  The steps you will take depend on how you installed the tooling. Please follow the appropriate section.\nIf you installed the tools by creating an AWS CloudFormation stack then please follow the AWS CloudFormation steps, otherwise please follow the Python steps. If you cannot remember how you installed the tools please follow the Python steps.\nIf you have switched to GitHub as the source of your ServiceCatalogFactory and ServiceCatalogPuppet repos you will need to follow the Python steps.\nWhichever steps you follow you will need to follow the steps in Populating the New Regions section\nStep by step guide Here are the steps you need to follow to \u0026ldquo;Adding a region\u0026rdquo;\nAWS CloudFormation steps Factory If you installed factory using the AWS CloudFormation way then you can update the stack you created, changing the parameters. This will update your config and bootstrap again for you.\n  In the AWS Console navigate to the AWS CloudFormation service where you created your initialisation stack - the recommended name for the stack was factory-initialization-stack.\n  Select the stack and click Update, then Use current template should be selected and you can click Next\n  In the EnabledRegions input specify the new list of every region you want to target.\n  Once you have done this click Next on the following two screens and then check the box I acknowledge that AWS CloudFormation might create IAM resources with custom names. and click Update Stack\n  Once this has completed you can run the servicecatalog-product-factory-initialiser AWS CodeBuild project and your install will be updated.\n  Puppet If you installed puppet using the AWS CloudFormation way then you can update the stack you created, changing the parameters. This will update your config and bootstrap again for you.\n  In the AWS Console navigate to the AWS CloudFormation service where you created your initialisation stack - the recommended name for the stack was puppet-initialization-stack.\n  Select the stack and click Update, then Use current template should be selected and you can click Next\n  In the EnabledRegions input specify the new list of every region you want to target.\n  Once you have done this click Next on the following two screens and then check the box I acknowledge that AWS CloudFormation might create IAM resources with custom names. and click Update Stack\n  Once this has completed you can run the servicecatalog-product-puppet-initialiser AWS CodeBuild project and your install will be updated.\n  Python steps Factory  You will need to install aws-service-catalog-factory via pip:   pip install aws-service-catalog-factory    You can then set your regions:   servicecatalog-factory set-regions eu-west-1,eu-west-2,eu-west-3    You will then need to bootstrap with the same settings you used when initially bootstrapping:   servicecatalog-factory bootstrap ...   If you are using GitHub as your ServiceCatalogFactory repo you will need to specify this whenever you bootstrap.\nTo get a list of the parameters for bootstrapping you can run:\n servicecatalog-factory bootstrap --help   Puppet  You will need to install aws-service-catalog-puppet via pip:   pip install aws-service-catalog-puppet    You can then set your regions:   servicecatalog-puppet set-regions eu-west-1,eu-west-2,eu-west-3    You will then need to bootstrap with the same settings you used when initially bootstrapping:   servicecatalog-puppet bootstrap ...   If you are using GitHub as your ServiceCatalogPuppet repo you will need to specify this whenever you bootstrap.\nTo get a list of the parameters for bootstrapping you can run:\n servicecatalog-puppet bootstrap --help   Populating the New Regions Once you have followed the instructions above your product pipelines have been reconfigured to add your products to the newly specified regions. For your products to appear in those regions you will need to run their pipelines again.\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/every-day-use/360-aws-organizations-integration.html",
	"title": "AWS Organizations Integration",
	"tags": [],
	"description": "",
	"content": " This tutorial will walk you through \u0026ldquo;AWS Organizations Integration\u0026rdquo; with Service Catalog Tools.\nWe will assume you have:\n installed Service Catalog Puppet correctly setup AWS Organizations including:  a Management Account Member Account(s) in an Organizational Unit (OU)   setup an IAM Role for AWS Organizations bootstrapped a spoke created a product created a portfolio  In the tutorial you will look at:\n Adding Accounts using Organizational Units (OU)   Sharing a portfolio using AWS Organizations   During this process you will check your progress by verifying what the framework is doing.\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/every-day-use/360-aws-organizations-integration/361-adding-accounts-in-ou.html",
	"title": "Adding Accounts using Organizational Units (OU)",
	"tags": [],
	"description": "",
	"content": " What are we going to do? This tutorial will walk you through \u0026ldquo;Adding Accounts using Organizational Units (OU)\u0026quot;.\nWe are going to perform the following steps:\n create a manifest file add accounts to the manifest file using AWS Organizations OU add a spoke-local-portfolios to the manifest file  During this process you will check your progress by verifying what the framework is doing at each step.\nStep by step guide Here are the steps you need to follow to \u0026ldquo;Adding Accounts using Organizational Units (OU)\u0026rdquo;\nCreating the manifest file   Navigate to the ServiceCatalogPuppet CodeCommit repository\n  Scroll down to the bottom of the page and hit the Create file button\n    Adding an OU to the manifest file This will allow you to provision AWS Service Catalog Products into multiple AWS Accounts and Regions across your AWS estate without listing individual AWS 12-digit Account IDs. Instead we will supply the AWS Organizations OU Path. Service Catalog Products will then be provisioned in AWS Accounts that are a member of this OU.\nWe will start out by adding your OU to the manifest file.\n  Navigate to the ServiceCatalogPuppet CodeCommit repository\n  Scroll down to the bottom of the page and hit the Create file button\n      Copy the following snippet into the main input field:\n accounts: - ou: \u0026#34;\u0026lt;YOUR_OU_OR_PATH\u0026gt;\u0026#34; name: \u0026#34;application-accounts\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; - \u0026#34;eu-west-2\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34;     Update \u0026lt;YOUR_OU_OR_PATH\u0026gt; to be your OU or OU Path which contains member accounts\n for example: /production/application-accounts/    The framework will list the AWS Accounts in your OU and expand the manifest automatically.\nFor example, if your OU were to contain AWS Accounts: 0123456789010 and 0109876543210, then the expanded manifest file will look like this:\n accounts: - account_id: 0123456789010 name: \u0026#39;\u0026lt;YOUR_ACCOUNT_NAME\u0026gt;\u0026#39; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; - \u0026#34;eu-west-2\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34; - account_id: 0109876543210 name: \u0026#39;\u0026lt;YOUR_ACCOUNT_NAME\u0026gt;\u0026#39; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; - \u0026#34;eu-west-2\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34;   Adding spoke-local-portfolio to the manifest Now we are ready to add a product to the manifest file.\n Add the following snippet to the end of the main input field:   spoke-local-portfolios: account-vending-for-spokes: portfolio: \u0026#34;reinvent-cloud-engineering-governance\u0026#34; deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;    The main input field should look like this:   accounts: - ou: \u0026#34;\u0026lt;YOUR_OU_OR_PATH\u0026gt;\u0026#34; name: \u0026#34;application-accounts\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; - \u0026#34;eu-west-2\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34; spoke-local-portfolios: account-vending-for-spokes: portfolio: \u0026#34;reinvent-cloud-engineering-governance\u0026#34; deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;   Committing the manifest file Now that we have written the manifest file we are ready to commit it.\n  Set the File name to manifest.yaml\n  Set your Author name\n  Set your Email address\n  Set your Commit message\n  Using a good / unique commit message will help you understand what is going on later.\n  Click the Commit changes button:    Verifying the sharing Once you have made your changes the ServiceCatalogPuppet Pipeline should have run or if you were quick may still be running. If it has not yet started feel free to the hit the Release change button.\nOnce it has completed it should show the Source and Deploy stages in green to indicate they have completed successfully:\n  Once you have verified the pipeline has run you can go to Service Catalog portfolios in the member account to view your shared product.\nWhen you share a portfolio the framework will decide if it should share the portfolio. If the target account is the same as the factory account it will not share the portfolio as it is not needed.\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/every-day-use/360-aws-organizations-integration/362-sharing-a-portfolio-organizations.html",
	"title": "Sharing a portfolio using AWS Organizations",
	"tags": [],
	"description": "",
	"content": " What are we going to do? This tutorial will walk you through \u0026ldquo;Sharing a portfolio using AWS Organizations\u0026rdquo; to OU member accounts.\nDoing this will allow the framework to share Service Catalog Portfolios by using AWS Organizations OUs, rather than account-to-account sharing. This will reduce the time required to share Portfolios.\nWe are going to perform the following steps:\n enable organizational sharing create a manifest file add an account to the manifest file add a spoke-local-portfolios to the manifest file  During this process you will check your progress by verifying what the framework is doing at each step.\nStep by step guide Here are the steps you need to follow to \u0026ldquo;Sharing a portfolio using AWS Organizations\u0026rdquo;\nEnable Organizational Sharing This is action is only required once.\n Using the AWS Console:\n Navigate to Service Catalog in the AWS Console Select Portfolios from the sidebar. Select previously created Portfolio, i.e reinvent-cloud-engineering-governance Select Actions, then Share. Select the Organizations button.  You'll see a warning message: \u0026ldquo;Organizational sharing is not available\u0026rdquo;   Select the Enable button.    Creating the manifest file   Navigate to the ServiceCatalogPuppet CodeCommit repository\n  Scroll down to the bottom of the page and hit the Create file button\n    Adding an OU to the manifest file We will start out by adding your OU to the manifest file.\n  Navigate to the ServiceCatalogPuppet CodeCommit repository\n  Scroll down to the bottom of the page and hit the Create file button\n      Copy the following snippet into the main input field:\n accounts: - ou: \u0026#34;\u0026lt;YOUR_OU_OR_PATH\u0026gt;\u0026#34; name: \u0026#34;application-accounts\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; - \u0026#34;eu-west-2\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34;     Update \u0026lt;YOUR_OU_OR_PATH\u0026gt; to show your OU or OU Path which contains member accounts\n for example /production/application-accounts/    Adding spoke-local-portfolio to the manifest Now we are ready to add a product, which we will share via AWS Organizations, to the manifest file.\n Add the following snippet to the end of the main input field:   spoke-local-portfolios: account-vending-for-spokes: portfolio: \u0026#34;reinvent-cloud-engineering-governance\u0026#34; sharing_mode: AWS_ORGANIZATIONS deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;   Notice that we have included sharing_mode: AWS_ORGANIZATIONS.\nHere, the Portfolio Share will be accepted in the default_region of accounts that are type:prod.\n The main input field should look like this:   accounts: - ou: \u0026#34;\u0026lt;YOUR_OU_OR_PATH\u0026gt;\u0026#34; name: \u0026#34;application-accounts\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; - \u0026#34;eu-west-2\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34; spoke-local-portfolios: account-vending-for-spokes: portfolio: \u0026#34;reinvent-cloud-engineering-governance\u0026#34; sharing_mode: AWS_ORGANIZATIONS deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;   Committing the manifest file Now that we have written the manifest file we are ready to commit it.\n  Set the File name to manifest.yaml\n  Set your Author name\n  Set your Email address\n  Set your Commit message\n  Using a good / unique commit message will help you understand what is going on later.\n  Click the Commit changes button:    Verifying the sharing Once you have made your changes the ServiceCatalogPuppet Pipeline should have run or if you were quick may still be running. If it has not yet started feel free to the hit the Release change button.\nOnce it has completed it should show the Source and Deploy stages in green to indicate they have completed successfully:\n  Once you have verified the pipeline has run you can go to Service Catalog portfolios in the member account to view your shared product.\nWhen you share a portfolio the framework will decide if it should share the portfolio. If the target account is the same as the factory account it will not share the portfolio as it is not needed.\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/every-day-use/380-provisioning-a-cloudformation-stack.html",
	"title": "Provisioning CloudFormation",
	"tags": [],
	"description": "",
	"content": " What are we going to do? This tutorial will walk you through how to use the \u0026ldquo;Provisioning CloudFormation\u0026rdquo; feature.\nWe will assume you have:\n installed Service Catalog Puppet correctly bootstrapped a spoke created a manifest file added an account to the manifest file  We will assume you are comfortable:\n making changes your manifest file  We are going to perform the following steps to \u0026ldquo;Provisioning CloudFormation\u0026rdquo;:\n upload a template to AWS CloudFormation specify an AWS CloudFormation template that should be provisioned  Step by step guide Here are the steps you need to follow to \u0026ldquo;Provisioning CloudFormation\u0026rdquo;\nThings to note, before we start  This feature was added to version 0.108.0. You will need to be using this version (or later) Stacks can use parameters, deploy_to and outputs Stacks can be used in spoke execution mode Stacks can be used in dry-runs Stacks do not appear in list-launches (they are not a launch)  Upload a template to AWS CloudFormation When you upgrade to version 0.108.0 or newer you will see a bucket named sc-puppet-stacks-repository-xxx where xxx is your AWS account id. You should upload a template into that bucket and get the version id of the template:\n  Specify an AWS CloudFormation template that should be provisioned Now we are ready to add a stack to the manifest file.\n  Navigate to the ServiceCatalogPuppet CodeCommit repository\n  Click the ServiceCatalogPuppet repository\n  Click the link to the manifest.yaml file, and then click the Edit button\n  Add the following snippet to the end of the main input field:\n   stacks: basic-vpc: key: product.template.yaml version_id: 1tPCvNHLEw8fsARqJ2RFouBfebRpURS7 depends_on: - name: basic-vpc type: launches affinity: account deploy_to: tags: - tag: group:spoke regions: regions_enabled outputs: ssm: - stack_output: VPCId param_name: \u0026#34;/vpcs/${AWS::AccountId}/${AWS::Region}/VPCId\u0026#34;   Committing the manifest file Now that we have updated the manifest file we are ready to commit our changes.\n Set your Author name Set your Email address Set your Commit message  Using a good / unique commit message will help you understand what is going on later.\n  Click the Commit changes button:    "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/every-day-use/385-provisioning-a-stack.html",
	"title": "Provisioning a Stack",
	"tags": [],
	"description": "",
	"content": " What are we going to do? This tutorial will walk you through how to use the \u0026ldquo;Provisioning a Stack\u0026rdquo; feature.\nWe will assume you have:\n installed Service Catalog Factory correctly installed Service Catalog Puppet correctly bootstrapped a spoke created a manifest file added an account to the manifest file  We will assume you are comfortable:\n making changes your portfolios files making changes your manifest file  We are going to perform the following steps to \u0026ldquo;Provisioning a Stack\u0026rdquo;:\n creating a stack using Service Catalog Factory provision a stack using Service Catalog Puppet  Step by step guide Here are the steps you need to follow for \u0026ldquo;Provisioning a Stack\u0026rdquo;\nThings to note, before we start  This feature was added in version 0.63.0 of Service Catalog Factory. You will need to be using this version (or later) This feature was added in version 0.109.0 of Service Catalog Puppet. You will need to be using this version (or later) Stacks can use parameters, deploy_to and outputs Stacks can be used in spoke execution mode Stacks can be used in dry-runs Stacks can be provisioned in spoke execution mode (since Service Catalog Puppet version 0.109.0) Stacks do not appear in list-launches (they are not a launch)  Creating a stack using Service Catalog Factory   Navigate to the ServiceCatalogFactory CodeCommit repository\n  click Add file and then Create file\n  Paste the following into the main input window:\n  In the File name field enter the following stacks/example.yaml\n   Schema: factory-2019-04-01 Stacks: - Name: \u0026#34;ssm-parameter\u0026#34; Versions: - Name: \u0026#34;v1\u0026#34; Source: Provider: \u0026#34;CodeCommit\u0026#34; Configuration: RepositoryName: \u0026#34;ssm-parameter-stack\u0026#34; BranchName: \u0026#34;main\u0026#34;     Please note the file name is not significant but it must have the extension .yaml and it must be in a directory named stacks\n  Fill in the other fields and save the file. Once you do the pipeline servicecatalog-factory-pipeline will run.\n  Once the pipeline is complete you will have a new pipeline named stack--ssm-parameter-v1-pipeline\n  You should navigate to AWS CodeCommit, create a repo named ssm-parameter-stack and add a file named stack.template.yaml with the following content on the main branch:\n   Parameters: Name: Type: String Value: Type: String Resources: Parameter: Type: AWS::SSM::Parameter Properties: Name: !Ref Name Type: String Value: !Ref Value Outputs: Value: Value: !GetAtt Parameter.Value    Once you have added the file the pipeline stack--ssm-parameter-v1-pipeline will run. It will take the source code and add will add it to Amazon S3 so you can use it in Service Catalog Puppet in the step below.  Provision a stack using Service Catalog Puppet Now we are ready to provision the stack using the manifest file.\n  Navigate to the ServiceCatalogPuppet CodeCommit repository\n  Click the ServiceCatalogPuppet repository\n  Click the link to the manifest.yaml file, and then click the Edit button\n  Add the following snippet to the end of the main input field:\n   stacks: ssm-parameter: name: ssm-parameter version: v1 parameters: Name: default: \u0026#34;hello\u0026#34; Value: default: \u0026#34;world\u0026#34; deploy_to: tags: - tag: type:prod regions: regions_enabled   Capabilities In some cases, you must explicitly acknowledge that your stack template contains certain capabilities in order for CloudFormation to create the stack.\nWhen you need to do this you can specify the capabilities as a list per stack:\n stacks: ssm-parameter: name: ssm-parameter version: v1 capabilities: - CAPABILITY_IAM parameters: Name: default: \u0026#34;hello\u0026#34; Value: default: \u0026#34;world\u0026#34; deploy_to: tags: - tag: type:prod regions: regions_enabled   Committing the manifest file Now that we have updated the manifest file we are ready to commit our changes.\n Set your Author name Set your Email address Set your Commit message  Using a good / unique commit message will help you understand what is going on later.\n  Click the Commit changes button:    "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/every-day-use/400-invoking-a-lambda-function.html",
	"title": "Invoking a Lambda Function",
	"tags": [],
	"description": "",
	"content": " What are we going to do? This tutorial will walk you through how to use the \u0026ldquo;Invoking a Lambda Function\u0026rdquo; feature.\nWe will assume you have:\n installed Service Catalog Puppet correctly bootstrapped a spoke created a manifest file added an account to the manifest file  We are going to perform the following steps to \u0026ldquo;Invoking a Lambda Function\u0026rdquo;:\n set up an AWS IAM role in the spoke account create a sample lambda function add a lambda invoke to the manifest file  During this process you will check your progress by verifying what the framework is doing at each step.\nStep by step guide Here are the steps you need to follow to \u0026ldquo;Invoking a Lambda Function\u0026rdquo;\nSet Up an AWS IAM Role in the Spoke Account This guide assumes that a role exists within the spoke account that can be assumed by the Service Catalog Tools account. The name of this role would need to be the same across all spoke accounts, and the role would need permissions appropriate for your lambda function(s) to be able to complete its tasks. For the purpose of this example, a CloudFormation template has been provided that you can use to create the role in the spoke account that will allow our sample lambda function to run successfully.\n AWSTemplateFormatVersion: \u0026#39;2010-09-09\u0026#39; Description: \u0026#34;IAM Role in spoke accounts that will have trust relationship with Service Catalog Tools account.\u0026#34; Parameters: ToolsAccountId: Description: \u0026#34;The Service Catalog Tools AWS Account ID\u0026#34; Type: String ToolsAccountAccessRole: Description: \u0026#34;Name of the IAM role that the management account will be allowed to assume\u0026#34; Default: ToolsAccountAccessRole Type: String Resources: AssumedRole: Type: AWS::IAM::Role Description: | IAM Role needed by the Service Catalog Tools Account Properties: RoleName: !Ref ToolsAccountAccessRole Policies: - PolicyName: ToolsAccountTrustPolicy PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: \u0026#39;iam:*\u0026#39; Resource: \u0026#39;*\u0026#39; AssumeRolePolicyDocument: Version: \u0026#34;2012-10-17\u0026#34; Statement: - Effect: \u0026#34;Allow\u0026#34; Principal: AWS: !Sub \u0026#34;arn:aws:iam::${ToolsAccountId}:root\u0026#34; Action: - \u0026#34;sts:AssumeRole\u0026#34;   Creating the sample lambda function We will need to create the AWS Lambda function that will be executed by the framework. This function will exist in the account where you have installed the Service Catalog Tools. When you want to perform an action in a spoke account you should read the account_id and region properties from the event object. If you want to use parameters they are available using the parameters attribute in the event object.\n  You should save the following into a file named create-iam-group-lambda.yaml\n AWSTemplateFormatVersion: \u0026#39;2010-09-09\u0026#39; Description: Creates a Lambda function that assumes a role into spoke accounts and creates an IAM group Resources: rLambdaCustomRole: Type: AWS::IAM::Role Properties: AssumeRolePolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Principal: Service: [lambda.amazonaws.com] Action: - sts:AssumeRole ManagedPolicyArns: - !Ref rLambdaCustomPolicy rLambdaCustomPolicy: Type: AWS::IAM::ManagedPolicy Properties: PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: [\u0026#39;logs:CreateLogGroup\u0026#39;, \u0026#39;logs:CreateLogStream\u0026#39;, \u0026#39;logs:PutLogEvents\u0026#39;] Resource: !Sub arn:${AWS::Partition}:logs:${AWS::Region}:${AWS::AccountId}:log-group:* - Effect: Allow Action: - sts:AssumeRole Resource: \u0026#34;*\u0026#34; rLambda: Type: AWS::Lambda::Function Properties: FunctionName: create-iam-group Description: Creates an IAM Group Handler: index.lambda_handler Code: ZipFile: | import json import boto3 # Set up clients sts = boto3.client(\u0026#39;sts\u0026#39;) def get_session_info(event): acct_id = event[\u0026#39;account_id\u0026#39;] role_name = event[\u0026#39;parameters\u0026#39;][\u0026#39;RoleName\u0026#39;] role_arn = \u0026#39;arn:aws:iam::\u0026#39; + acct_id + \u0026#39;:role/\u0026#39; + role_name sts_response = sts.assume_role( RoleArn=role_arn, RoleSessionName=\u0026#39;LambdaInvokeSession\u0026#39; ) return sts_response def lambda_handler(event, context): print(event) sts_response = get_session_info(event) access_key = sts_response[\u0026#34;Credentials\u0026#34;][\u0026#34;AccessKeyId\u0026#34;] secret_key = sts_response[\u0026#34;Credentials\u0026#34;][\u0026#34;SecretAccessKey\u0026#34;] session_token = sts_response[\u0026#34;Credentials\u0026#34;][\u0026#34;SessionToken\u0026#34;] iam = boto3.client( \u0026#39;iam\u0026#39;, aws_access_key_id=access_key, aws_secret_access_key=secret_key, aws_session_token=session_token ) group_name = \u0026#39;sc-tools-invoke-lambda-test-group\u0026#39; response = iam.create_group( GroupName=group_name ) MemorySize: 128 Role: !GetAtt rLambdaCustomRole.Arn Runtime: python3.7 Timeout: 300 Outputs: LambdaName: Value: !Ref rLambda     You should then use AWS CloudFormation to create a stack named create-iam-group-lambda using the template you just created\n  What did we just do?  You created an AWS CloudFormation template You used the template to create a stack in CloudFormation The stack created a sample lambda function  Adding a lambda-invocation to the manifest Now we are ready to add a lambda invocation to the manifest file.\n  Navigate to the ServiceCatalogPuppet CodeCommit repository\n  Click the ServiceCatalogPuppet repository\n  Click the link to the manifest.yaml file, and then click the Edit button\n  Add the following snippet to the end of the main input field:\n   lambda-invocations: create-iam-group: function_name: create-iam-group qualifier: $LATEST invocation_type: Event parameters: RoleName: default: \u0026#34;ToolsAccountAccessRole\u0026#34; invoke_for: tags: - regions: \u0026#34;default_region\u0026#34; tag: \u0026#34;type:prod\u0026#34;    The main input field should look like this:   accounts: - account_id: \u0026#34;\u0026lt;YOUR_SPOKE_ACCOUNT_ID_WITHOUT_HYPHENS\u0026gt;\u0026#34; name: \u0026#34;puppet-account\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; - \u0026#34;eu-west-2\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34; lambda-invocations: create-iam-group: function_name: create-iam-group qualifier: $LATEST invocation_type: Event parameters: RoleName: default: \u0026#34;ToolsAccountAccessRole\u0026#34; invoke_for: tags: - regions: \u0026#34;default_region\u0026#34; tag: \u0026#34;type:prod\u0026#34;   Committing the manifest file Now that we have updated the manifest file we are ready to commit our changes.\n Set your Author name Set your Email address Set your Commit message  Using a good / unique commit message will help you understand what is going on later.\n  Click the Commit changes button:    Verifying the lambda invocation Once you have made your changes the ServiceCatalogPuppet Pipeline should have run or if you were quick may still be running. If it has not yet started feel free to the hit the Release change button.\nOnce it has completed it should show the Source and Deploy stages in green to indicate they have completed successfully:\n  Once you have verified the pipeline has run you can go to IAM Groups Console in the spoke account to view the IAM Group created by the lambda invoke labeled sc-tools-invoke-lambda-test-group.\n  You have now successfully invoked a lambda function!\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/design-considerations/product-sdlc.html",
	"title": "Product SDLC",
	"tags": [],
	"description": "",
	"content": " What are we going to do? This article will explain how productive teams have been structuring their AWS Accounts and how they have been managing their source code whilst working with the Service Catalog Tools.\nAWS Account structure Testing the provisioning of a single resource in AWS can be achieved using CloudFormation RSpec, awspec and others. Testing the provisioning of a single resource or even a single AWS CloudFormation stack is valuable and will give you confidence that your change will behave in the way you think without any side effects. When you have multiple developers working on the same code base, or develop over time or have different developers provisioning different stacks that must work with each other it can be difficult to manage and changes that introduce unexpected side effects can easily sneak through.\nWe do not recommend you create a whole test installation of the Service Catalog Tools for the development of products.\nThe management overhead of the installation and the complexity and delay it creates in your SDLC should be avoided and instead you should invest that effort into better automated testing so you can scale.\nWe recommend you have at least a single canary account where you can provision products that will work together. You can use actions from the Service Catalog Tools to automate the testing and promotion of products across your AWS Accounts including a mandatory provisioning into the canary account.\nManaging source code We recommend you do not modify a product version that you have shared or provisioned. If you make a change to a product then you should change the version name.\nWe have seen gitflow working well with product development. Whilst building a product the developer uses a develop branch in git. That develop version gets provisioned into the canary account for testing. Once the testing is complete the branch in merged to main and then rebranched from main to create a new version. This worked well for teams where a single developer was building a single product. If you have a product that is too big for a single developer then maybe the product needs to be split into smaller pieces. There are features in the tools that allow you split products into smaller pieces.\nIf you would like to share your SDLC process please raise a github issue to share\n "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/2021-workshop/400-self-service.html",
	"title": "Self service",
	"tags": [],
	"description": "",
	"content": " What are we going to do? We have now removed the default networking resources, created a VPC and two subnets.\nAs we build out a multi account environment we may want to allow our end user to decide how many subnets they need, what sizes they are or which VPC they should be attached to. AWS Service Catalog is a great way to achieve this. You can create a product, add it to a portfolio and then allow your users to provision the product with limited options.\nThe following steps will show you how to make a product and share it with an account:\n Using AWS Service Catalog   "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/design-considerations/using-iam-and-scp-effectively.html",
	"title": "Using IAM and SCP Effectively",
	"tags": [],
	"description": "",
	"content": " What are we going to do? This article will explain how to restrict access to the AWS services for your users but still allow them to provision resources using launch constraints.\nIt will also cover how you can use resource naming conventions and conditional IAM and SCP statements to ensure users cannot modify AWS Service Catalog products or the resources within them.\nUsing launch constraints You may wish to use a service catalog to provide a set of approved resource templates for consumption. In this instance, you may want to allow consumers to provision a product that contains an AWS S3 bucket but you may not want to allow these users to provision AWS S3 buckets directly via the console or cli.\nTo achieve this, you could use a launch constraint and an IAM role assumable by the AWS Service Catalog service. The IAM role you provide as a launch constraint is used to provision the resources in the product. The role provisioning the product must have the iam:passrole permission.\nResource naming conventions along with conditional IAM and SCP statements When writing your AWS Service Catalog products you can have a parameter to all of your products that is used as a prefix to the names of the resources provisioned. You can then use IAM boundaries to prohibit the modifying of all resources that have the given prefix or you can use a conditional SCP to prohibit the modifying of all resources that have the given prefix except for the PuppetRole.\nWithin the Service Catalog Tools all provisioning occurs using an IAM role /servicecatalog-puppet/PuppetRole.\nWhen writing your products you should use globally unique parameter names to avoid clashes. If you use the same name for the resource prefixes you can set the parameter value once in the manifest file as a global parameter.\nIf you would like to share your experiences please raise a github issue\n "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/every-day-use/600-starting-a-code-build-project.html",
	"title": "Starting a CodeBuild project",
	"tags": [],
	"description": "",
	"content": " What are we going to do? This tutorial will walk you through how to use the \u0026ldquo;Starting a CodeBuild project\u0026rdquo; feature.\nWe will assume you have:\n installed Service Catalog Puppet correctly bootstrapped a spoke created a manifest file added an account to the manifest file  We are going to perform the following steps to \u0026ldquo;Starting a CodeBuild project\u0026rdquo;:\n Create an AWS CodeBuild project in your puppet account that will be triggered Adding the codebuild run to your manifest file that will do the triggering  Step by step guide Here are the steps you need to follow to \u0026ldquo;Starting a CodeBuild project\u0026rdquo;\nCreate an AWS CodeBuild project in your puppet account Log into the account where you installed service catalog puppet and navigate to the AWS CodeBuild console in the region where you installed service catalog puppet.\nCreate a new project named ping-host. It should have three environmental variables named:\n TARGET_ACCOUNT_ID TARGET_REGION HOST_TO_PING  For the build spec you should have the following command:\necho \u0026quot;I have been asked to ping ${HOST_TO_PING} from ${TARGET_REGION} of ${TARGET_ACCOUNT_ID}\u0026quot;\nAdding the codebuild run to your manifest file Add the following snippet to your manifest file:\n code-build-runs: ping-host: project_name: ping-some-host parameters: HOST_TO_PING: default: 10.0.0.2 run_for: tags: - regions: regions_enabled tag: role:all   If you already had a code-build-runs please append the ping-host declaration to the existing code-build-runs section.\nYou will most likely need to update the tag from role:all to whatever you are using in your environment.\nThe parameters section supports all capabilities that launches supports. You can read more in the \u0026ldquo;Using parameters in the real world\u0026rdquo; section under the \u0026ldquo;Design considerations\u0026rdquo; heading.\nWhat did we just do? You told service catalog puppet to get a list of all regions for all accounts with the tag you specified. For each item in the list you told service catalog puppet to run a codebuild project. If you have 2 regions and 4 accounts the codebuild project will be triggered 8 times. The codebuild projects will run in serial to avoid hitting service or api throttling limits.\nVerifying the codebuild run To verify the codebuild runs you can use the codebuild console. Look at the history for the ping-host project and verify the number of runs was correct. For each run you can verify the environmental variables that were overridden.\nYou have now successfully run a codebuild project!\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/2021-workshop.html",
	"title": "2021 Workshop",
	"tags": [],
	"description": "",
	"content": "Welcome to the Service Catalog tools network capability workshop - 2021 edition.\nThe average completion time for the workshop is between 2 - 3 hours.\nWhat are we going to be doing? In this workshop you will be building out the networking capability for your multi-account environment.\nYou will learn about some of the essential building blocks the Service Catalog tools provide you when creating your multi account workflow:\n provisioning resources into accounts using AWS CloudFormation, AWS Service Catalog and Hashicorp Terraform executing AWS Lambda functions Sharing Service Catalog portfolios  You will be using these building blocks to:\n Remove the default networking resources from your AWS accounts Provision a new AWS VPC using AWS CloudFormation Provision a new subnet using AWS Service Catalog Provision a new subnet using Hashicorp Terraform Share a Service Catalog portfolio so end users can create their own subnets  Let's get going! When you are ready, click the right arrow to begin!\nYou can use the left and right arrows to navigate through the Workshop\n "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/every-day-use/700-using-assertions.html",
	"title": "Using assertions",
	"tags": [],
	"description": "",
	"content": " What are we going to do? With assertions, you tell the framework to compare expected results to actual results. If they do not match then the framework sees this as a failure and anything depending on your assertion will not execute.\nYou can declare the expected results object in your manifest file where you can also tell the framework how to build up an actual results.\nThis tutorial will walk you through how to use the \u0026ldquo;Using assertions\u0026rdquo; feature.\nWe will assume you have:\n installed Service Catalog Puppet correctly bootstrapped a spoke created a manifest file added an account to the manifest file  We are going to perform the following steps to \u0026ldquo;Using assertions\u0026rdquo;:\n Adding an assertion to your manifest file  Step by step guide Here are the steps you need to follow to \u0026ldquo;Using assertions\u0026rdquo;\nAdding an assertion to your manifest file Add the following snippet to your manifest file:\n assertions: assert-no-default-vpcs: expected: source: manifest config: value: - \u0026#34;\u0026#34; actual: source: boto3 config: client: \u0026#39;ec2\u0026#39; call: describe_vpcs arguments: {} use_paginator: true filter: Vpcs[?IsDefault==`true`].State assert_for: tags: - regions: regions_enabled tag: role:all   If you already had an assertions please append the assert-puppet-role-path declaration to the existing assertions section.\nYou will most likely need to update the tag from role:all to whatever you are using in your environment.\nWhat did we just do? In each region of each account in your assert_for you asked service catalog puppet to do the following:\n assume role into the region of the account create an iam client using boto3 using the client, call the describe_vpcs command using a paginator and providing no arguments you then told service catalog puppet to use a filter to remove items from the results of the describe_vpcs command - this is useful to remove things like CreateDate, Arns and other properties that vary by region / account.  Recommendations  It is recommended to add assertions verifying default VPCs are removed. It is recommended to use fine-grained depends_on statements when using assertions to reduce choke points.  You have now successfully executed an assertion!\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/reinvent2019/",
	"title": "2019 AWS re:Invent Workshop",
	"tags": [],
	"description": "",
	"content": "Welcome to the Service Catalog tools workshop at re:Invent 2019. In this workshop, we will walk through using Service Catalog tools to build controls for governance. At the end of the session, we hope that you will go away with tools and techniques to help you build for security and governance requirements using AWS Service Catalog.\nHouse keeping Please check through the following to help you get started.\nWhat to do if you need help Raise your hand and a helper will be with you as soon as possible.\nMeet the Team In today's workshop you have the following team\nPresenters:\n Eamonn Faherty Jamie McKay  Workshop helpers:\n Ritesh Sinha Thivan Visvanathan Alex Nicot Charles Roberts  Lets get going When you are ready, click the right arrow to begin!\nYou can use the left and right arrows to navigate through the Workshop\n "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/999-further_reading.html",
	"title": "Further Reading",
	"tags": [],
	"description": "",
	"content": "From here you can find out more about the Service Catalog Tools the AWS managed services used in the tutorials and workshops presented on this site.\nService Catalog Tools The following links provide more information on the open source tools used in the tutorials and workshops presented on this site:\nService Catalog Factory  Github project page for Service Catalog Factory Documentation for Service Catalog Factory  Service Catalog Puppet  Github project page for Service Catalog Puppet Documentation for Service Catalog Puppet  Shared products  We have Service Catalog products that you can use with these tools. These products perform actions like setting up multi account AWS Config or AWS Security Hub and more. You can view them on their Github project page.  AWS Services The following links provide more information on the AWS managed services used in the tutorials and workshops presented on this site:\nAWS Service Catalog  AWS Service Catalog home page  "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/categories.html",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/tags.html",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]