[
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/managing-your-environments/dont-repeat-yourself/accounts-section.html",
	"title": "Accounts section",
	"tags": [],
	"description": "",
	"content": " Accounts section As the accounts section of your manifest file increases you can find some duplication. The following sections explain how to deal with this\nDuplicate account details You may find you have many accounts and OUs with the same default region and enabled regions:\n accounts: - account_id: \u0026#39;012345678910\u0026#39; default_region: eu-west-1 email: someone-1@example.com name: example account 1 regions_enabled: - eu-west-1 - eu-west-2 - eu-west-3 - us-east-1 tags: - outype:foundational - ou:sharedservices - type:prod - team:ccoe - role:sct - role:account - account_id: \u0026#39;109876543210\u0026#39; default_region: eu-west-1 email: someone-2@example.com name: example account 2 regions_enabled: - eu-west-1 - eu-west-2 - eu-west-3 - us-east-1 tags: - outype:foundational - ou:sharedservices - type:prod - team:ccoe - role:account   If you want to roll out a new region you will have to update each of these entries. Instead you can use the accounts sub section of the defaults section of the manifest file to declare the defaults for each account:\n defaults: accounts: default_region: eu-west-1 regions_enabled: - eu-west-1 - eu-west-2 - eu-west-3 - us-east-1   You can now omit the default_region and regions_enabled sections of your accounts:\n accounts: - account_id: \u0026#39;012345678910\u0026#39; email: someone-1@example.com name: example account 1 tags: - outype:foundational - ou:sharedservices - type:prod - team:ccoe - role:sct - role:account - account_id: \u0026#39;109876543210\u0026#39; email: someone-2@example.com name: example account 2 tags: - outype:foundational - ou:sharedservices - type:prod - team:ccoe - role:account   If you specify a default_region wihtin the defaults.accounts section and an account within the accounts section specifies its own default_region the accounts section value will be used. The same is true for any values set within the defaults.accounts section - there is no append or merge behaviour.\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/",
	"title": "AWS Service Catalog Tools Intro Workshop",
	"tags": [],
	"description": "",
	"content": "Welcome Builders! This site is a collection of guidance, tutorials and workshops focusing on helping AWS customers build and manage a multi account environment using the Service Catalog tools.\nUse the headings on the left to find what you are looking for.\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/installation/50-bootstrapping/10-using-cfn-and-codebuild.html",
	"title": "CloudFormation/CodeBuild",
	"tags": [],
	"description": "",
	"content": "What are we going to do? You will need to bootstrap spoke accounts so you can configure them using the Service Catalog Tools.\nBootstrapping a spoke account will create an AWS CloudFormation stack in it. This stack will contain the Puppet IAM Role (PuppetRole) which is needed by framework to perform actions in the spoke account.\nThe following steps should be executed using the provided AWS CloudFormation templates and AWS CodeBuild projects which are linked to here or available in the home region of your puppet hub account.\nBootstrapping a spoke You should log into the account you want to bootstrap as a spoke and navigate to the AWS CloudFormation console in your home region using your web browser.\nYou should create an AWS CloudFormation stack with the name servicecatalog-puppet-spoke\nusing the template\nhttps://service-catalog-tools.s3.eu-west-2.amazonaws.com/puppet/latest/servicecatalog-puppet-spoke.template.yaml\nYou should set parameter with the name of PuppetAccountId to the 12 digit AWS Account Id of your puppet hub account.\nBootstrapping spokes in OU If you have enabled AWS Organizations support you may want to bootstrap all spokes within an organizational unit.\nFollowing these steps will allow you to bootstrap all AWS Accounts that exist within the same organizational unit.\nIn your AWS Account you can find an AWS CodeBuild project named: servicecatalog-puppet-bootstrap-an-ou\n  Click Start Build\n  Before you select Start Build again, expand the Environment variables override section.\n  Set OU_OR_PATH to the OU path or AWS Organizational Unit ID that contains all of the spokes you want to bootstrap\n  Set IAM_ROLE_NAME to the name of the IAM Role that is assumable in the spoke accounts - this must be the same name in all accounts\n  Set IAM_ROLE_ARNS to the ARNs you want to assume before assuming before the IAM_ROLE_NAME. This is should you need to assume a role with cross account permissions.\n  Click Start Build again\n  "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/2021-workshop/10-prerequisites.html",
	"title": "Pre-requisites",
	"tags": [],
	"description": "",
	"content": "This workshop assumes you have no experience using the Service Catalog tools. The expected completion time is between 2 - 3 hours. You can complete this in multiple sittings and if you have previous experience using the Service Catalog tools you can expect to complete it sooner.\nIn order to complete this workshop you will need:\n a web browser to access the AWS console AWS account to use the Service Catalog tools installed into the account  If you are using your own AWS account you will need to be able to use the AWS Console to provision resources.\n  Installation   "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/tools/factory.html",
	"title": "Service Catalog Factory",
	"tags": [],
	"description": "",
	"content": " What is Service Catalog Factory Service Catalog Factory is the first of the Service Catalog tools. It is an AWS Solution designed to accelerate the creation of AWS CodePipelines. The pipelines it creates take source code, allow you to run static analysis and functional tests before packaging and preparing for later use when using Service Catalog Puppet\nThe pipelines you create can package AWS Cloudformation templates to be used in stacks, launches or spoke local portfolios when using Service Catalog Puppet. You can also package source code to be used in a workspace or an app when using Service Catalog Puppet. Using Service Catalog Factory you can package AWS CloudFormation, AWS CDK or Hashicorp based git repositories.\nOverview When installing Service Catalog Factory you create a pipeline named servicecatalog-factory-pipeline. This pipeline can use AWS CodeStar connections, Amazon S3 or AWS CodeCommit as its source. The source contains descriptions of the different pipelines you want to create.\nWhen the pipeline runs it will verify all existing pipelines that were previously created are configured correctly and it will create pipelines for the newly declared pipelines:\n  When you are using portfolios in your git repository the pipeline will also create the AWS Service Catalog products in each of your enabled regions. If you add an additional region you will need to rerun the pipeline for changes to be made.\nWhat Do The Pipelines Look Like The pipelines comprise of the following stages:\n  Source The source stage listens to changes in your specified source code management system and triggers the pipeline when there are changes. The source git repository can be AWS CodeStar connections, Amazon S3, AWS CodeCommit or a custom source action.\nBuild The build stage is optional. You can specify a build stage by providing your own buildspec and specifying which AWS CodeBuild environment you would like to use. This is useful when you are using code generation tools to generate your AWS Cloudformation template - eg. when using Troposphere.\nTest The test stage fans out to make use of parallel steps in AWS CodePipeline. Each step in the phase needs to complete successfully for the operation to fan back in and continue to the package stage.\nThe framework provided test stage steps vary depending on which type of source code you are using:\nStacks  Validate - An AWS Cloudformation validate is performed on the provided template. This cannot be disabled. CFN Nag - CFN Nag is run on the provided template. This is disabled by default. CloudFormation RSpec - the tests you provide in your git repo are executed using CloudFormation RSpec. This is disabled by default.  Portfolios  Validate - An AWS Cloudformation validate is performed on the provided template. This cannot be disabled. CFN Nag - CFN Nag is run on the provided template. This is disabled by default. CloudFormation RSpec - the tests you provide in your git repo are executed using CloudFormation RSpec. This is disabled by default.  Workspaces  There are no framework provided steps for workspaces.  Apps  There are no framework provided steps for apps.  Package The package stage takes your input and makes it available for each region you operate in. If you are using local resources like source code in an AWS Lambda function the package stage can be used to execute an AWS Cloudformation package command for each region you operate in. If you are using workspaces or apps the package stage will zip your source code. A default package stage is provided for you.\nDeploy The deploy stage takes your prepared artefact and submits it to the corresponding repository:\n   Type Repository     stacks Amazon S3   portfolios AWS Service Catalog   workspaces Amazon S3   apps Amazon S3    Regions without AWS Codepipeline support If you are using a region without AWS Codepipeline support the framework will detect this and use an AWS CodeBuild project to perform the cross region deployment.\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/design-considerations/multi-account-strategy/starter-framework.html",
	"title": "Starter framework",
	"tags": [],
	"description": "",
	"content": " What are we going to do? This article will explain the starter multi-account framework\nStarter framework Within your AWS Organization there are two types of Organizational Units (OUs) - foundational and additional.\nThe Foundational OUs group the shared accounts needed to manage the your overall AWS environment. The areas considered foundational are security, networking and logs. Each of the AWS Accounts within the foundational OUs are grouped into production and non-production environments in order to clearly distinguish between production and non production policies.\nThe additional OUs group the accounts directly related to the software development lifecycle. This includes the accounts for development (development sandboxes, source code and continuous delivery) and the accounts for the staging process from earliest testing to production.\n  Foundational Security This OUs where the accounts for security are grouped. This OU and the accounts within it is the main responsibility of the security organization.\nLog Archive Account The log archive account would be home to AWS CloudTrail logs and other security logs. We would expect to see versioned, and encrypted Amazon S3 buckets with restrictive bucket policies and MFA on delete. There should be limited access to the account and there should be alarms on user login. The log archive account should be the single source of truth.\nRead only This account would typically be owned by the security team and used to enable security operations. It would have cross account roles for viewing / scanning resources in other accounts. It would be used for exploratory security testing.\nBreak glass This account would typically be owned by the security team and used to enable security operations. It would have cross account roles for making changes to resources in other accounts. It would be used in case of an event. It should have extremely limited access, almost never be used and have an alert on login.\nTooling This account / these accounts would typically be owned by the security team and used to enable security operations. It would would be used for tools such as AWS Guard Duty, AWS Security Hub, AWS Config aggregation or Cloud Custodian.\nThere could be more than one tooling account.\nInfrastructure Infrastructure is intended for shared infrastructure services such as networking and optionally any shared hosted infrastructure services.\nAccounts in other foundational and additional OUs should only depend on the infrastructure/prod OU accounts.\nShared Services This account / these accounts would typically host the following shared services consumed by others:\n LDAP/Active Directory Deployment tools  Golden AMI Pipeline    Network This account / these accounts would typically host the following networking services consumed by others:\n DNS Shared Services VPC  Additional The Additional OUs group the accounts directly related to the SDLC. This includes the accounts for development (development sandboxes, source code and continuous delivery) and the accounts for the staging process from earliest testing to production.\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/installation/30-service-catalog-puppet/10-using-aws-organizations.html",
	"title": "Using AWS Organizations",
	"tags": [],
	"description": "",
	"content": "You can use Service Catalog Puppet with AWS Organizations. Using it will allow you describe all accounts within an organizational unit as a single entity. This provides a quicker way to get started and an easier way of managing multiple account environments. You can also use AWS Service Catalog's support for AWS Organizations delegated administrator to reduce the number of invites and accepts for portfolio sharing.\nWhat are we going to do? When enabling AWS Organizations you will need to provision an IAM Role in the Organization's management account and you will then need to provide the ARN of that role to your puppet account as an AWS SSM parameter.\nThe role provisioned in the Organizations management account is only used to list accounts. It has no write access.\nYou can use the AWS CloudFormation to enable AWS Organizations.\nUsing AWS CloudFormation to create the IAM Role Within your AWS Organizations management account you should create an AWS CloudFormation stack with the following name:\npuppet-organizations-initialization-stack\nUsing the template of the URL:\nhttps://service-catalog-tools.s3.eu-west-2.amazonaws.com/puppet/latest/servicecatalog-puppet-org-master.template.yaml\nThis stack will have an output named PuppetOrgRoleForExpandsArn. Take a note of this Arn as you will need it in the next step.\nSetting the Org IAM role ARN Once you have created the IAM Role, you need to tell the framework which role you want to use. You do this by creating an AWS Systems Manager Parameter Store parameter named /servicecatalog-puppet/org-iam-role-arn in the region of the account where you will install puppet and use as your hub account. You can do this via the console or via the cli.\n aws ssm put-parameter --name /servicecatalog-puppet/org-iam-role-arn --type String --value \u0026lt;VALUE-FROM-STEP-ABOVE\u0026gt;  "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/tools.html",
	"title": "Service Catalog Tools",
	"tags": [],
	"description": "",
	"content": " What are the Service Catalog Tools The Service Catalog Tools are a collection of open source tools authored to help you manage a multi account AWS environment.\nTo find out more about the tools please read through the following:\n Service Catalog Factory   Service Catalog Puppet   "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/2021-workshop/300-provisioning-subnets/15-using-service-catalog.html",
	"title": "Using AWS Service Catalog",
	"tags": [],
	"description": "",
	"content": " What are we going to do? We are going to perform the following steps:\n Define a product with a version and a portfolio Add the source code for our product Provision the product subnet into a spoke account  Step by step guide Here are the steps you need to follow to \u0026ldquo;Using AWS Service Catalog\u0026rdquo;\nDefine a product with a version and a portfolio   Navigate to the ServiceCatalogFactory CodeCommit repository\n  Open the Add file menu and click the Create file button\n  Copy the following snippet into the main input field:\n Schema: factory-2019-04-01 Products: - Name: \u0026#34;subnet\u0026#34; Owner: \u0026#34;networking@example.com\u0026#34; Description: \u0026#34;subnet for networking\u0026#34; Distributor: \u0026#34;networking team\u0026#34; SupportDescription: \u0026#34;Speak to networking@example.com about exceptions and speak to cloud-engineering@example.com about implementation issues\u0026#34; SupportEmail: \u0026#34;cloud-engineering@example.com\u0026#34; SupportUrl: \u0026#34;https://wiki.example.com/cloud-engineering/networking/subnet\u0026#34; Tags: - Key: \u0026#34;type\u0026#34; Value: \u0026#34;governance\u0026#34; - Key: \u0026#34;creator\u0026#34; Value: \u0026#34;cloud-engineering\u0026#34; - Key: \u0026#34;cost-center\u0026#34; Value: \u0026#34;governance\u0026#34; Versions: - Name: \u0026#34;v1\u0026#34; Description: \u0026#34;v1 of subnet\u0026#34; Active: True Source: Provider: \u0026#34;CodeCommit\u0026#34; Configuration: RepositoryName: \u0026#34;subnet\u0026#34; BranchName: \u0026#34;main\u0026#34; Portfolios: - \u0026#34;mandatory\u0026#34; Portfolios: - DisplayName: \u0026#34;mandatory\u0026#34; Description: \u0026#34;Portfolio containing the mandatory networking components\u0026#34; ProviderName: \u0026#34;cloud-engineering\u0026#34; Associations: - \u0026#34;arn:aws:iam::${AWS::AccountId}:role/\u0026lt;INSERT YOUR ROLE NAME HERE\u0026gt;\u0026#34; Tags: - Key: \u0026#34;type\u0026#34; Value: \u0026#34;governance\u0026#34; - Key: \u0026#34;creator\u0026#34; Value: \u0026#34;cloud-engineering\u0026#34; - Key: \u0026#34;cost-center\u0026#34; Value: \u0026#34;governance\u0026#34;     Update the Associations in the pasted text to include the IAM role name you are assuming in your account.\n  Set the File name to portfolios/networking.yaml\n  Set your Author name\n  Set your Email address\n  Set your Commit message\n  Using a good / unique commit message will help you understand what is going on later.\n  Click the Commit changes button:    What did we just do? The YAML file we created in the CodeCommit repository told the framework to perform several actions:\n create a product named subnet add a v1 of our product create a portfolio named networking-mandatory add the product: subnet to the portfolio: networking-mandatory  Verify the change worked Once you have made your changes the ServiceCatalogFactory Pipeline should have run. If you were very quick in making the change, the pipeline may still be running. If it has not yet started feel free to the hit the Release change button.\nOnce it has completed it should show the Source and Build stages in green to indicate they have completed successfully:\n  Add the source code for our product When you configured your product version, you specified the following version:\n Versions: - Name: \u0026#34;v1\u0026#34; Description: \u0026#34;v1 of subnet\u0026#34; Active: True Source: Provider: \u0026#34;CodeCommit\u0026#34; Configuration: RepositoryName: \u0026#34;subnet\u0026#34; BranchName: \u0026#34;main\u0026#34;   This tells the framework the source code for the product comes from the main branch of a CodeCommit repository of the name subnet.\nWe now need to create the CodeCommit repository and add the AWS CloudFormation template we are going to use for our product.\n  Navigate to AWS CodeCommit\n  Click Create repository\n     Input the name subnet     Click Create     Scroll down to the bottom of the page and hit the Create file button     Copy the following snippet into the main input field:   AWSTemplateFormatVersion: \u0026#39;2010-09-09\u0026#39; Description: | Builds out a VPC for use Parameters: SubnetCIDR: Type: String Description: | Subnet to use for the Subnet VPCID: Type: String Description: | VPC to create Subnet in Resources: Subnet: Type: AWS::EC2::Subnet Properties: VpcId: Ref: VPCID CidrBlock: Ref: SubnetCIDR AvailabilityZone: !Select - 0 - !GetAZs Ref: \u0026#39;AWS::Region\u0026#39;     Set the File name to product.template.yaml\n  Set your Author name\n  Set your Email address\n  Set your Commit message\n  Using a good / unique commit message will help you understand what is going on later.\n Creating that file should trigger your subnet-v1-pipeline.\nOnce the pipeline has completed it should show the stages in green to indicate they have completed successfully:\n  You should see your commit message on this screen, it will help you know which version of ServiceCatalogFactory repository the pipeline is processing.\n Once you have verified the pipeline has run you can go to Service Catalog products to view your newly created version.\nYou should see the product you created listed:\n  Click on the product and verify v1 is there\n  If you cannot see your version please raise your hand for some assistance\n You have now successfully created a version for your product!\nVerify the product was added to the portfolio Now that you have verified the pipeline has run you can go to Service Catalog portfolios to view your portfolio.\n Click on networking-mandatory      Click on the product subnet\n  Click on the version v1\n    Provision the product subnet into a spoke account   Navigate to the ServiceCatalogPuppet CodeCommit repository again\n  Click on manifest.yaml\n  Click Edit\n  Append the following snippet to the end of the main input field:\n launches: subnet: portfolio: \u0026#34;networking-mandatory\u0026#34; product: \u0026#34;subnet\u0026#34; version: \u0026#34;v1\u0026#34; depends_on: - name: vpc type: stack affinity: stack parameters: VPCID: ssm: name: \u0026#34;/networking/vpc/account-parameters/${AWS::AccountId}/${AWS::Region}/VPCId\u0026#34; SubnetCIDR: default: \u0026#39;10.0.0.0/24\u0026#39; deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;     The main input field should look like this (remember to set your account_id):\n   accounts: - account_id: \u0026#34;\u0026lt;YOUR_ACCOUNT_ID_WITHOUT_HYPHENS\u0026gt;\u0026#34; name: \u0026#34;puppet-account\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34; stacks: delete-default-networking-function: name: \u0026#34;delete-default-networking-function\u0026#34; version: \u0026#34;v1\u0026#34; capabilities: - CAPABILITY_NAMED_IAM deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34; vpc: name: \u0026#34;vpc\u0026#34; version: \u0026#34;v1\u0026#34; depends_on: - name: \u0026#34;delete-default-networking\u0026#34; type: \u0026#34;lambda-invocation\u0026#34; affinity: \u0026#34;lambda-invocation\u0026#34; deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34; outputs: ssm: - param_name: \u0026#34;/networking/vpc/account-parameters/${AWS::AccountId}/${AWS::Region}/VPCId\u0026#34; stack_output: VPCId lambda-invocations: delete-default-networking: function_name: DeleteDefaultNetworking qualifier: $LATEST invocation_type: Event depends_on: - name: \u0026#34;delete-default-networking-function\u0026#34; type: \u0026#34;stack\u0026#34; affinity: \u0026#34;stack\u0026#34; invoke_for: tags: - regions: \u0026#34;default_region\u0026#34; tag: \u0026#34;type:prod\u0026#34; assertions: assert-no-default-vpcs: expected: source: manifest config: value: [] actual: source: boto3 config: client: \u0026#39;ec2\u0026#39; call: describe_vpcs arguments: {} use_paginator: true filter: Vpcs[?IsDefault==`true`].State depends_on: - name: \u0026#34;delete-default-networking\u0026#34; type: \u0026#34;lambda-invocation\u0026#34; affinity: \u0026#34;lambda-invocation\u0026#34; assert_for: tags: - regions: regions_enabled tag: type:prod launches: subnet: portfolio: \u0026#34;networking-mandatory\u0026#34; product: \u0026#34;subnet\u0026#34; version: \u0026#34;v1\u0026#34; depends_on: - name: vpc type: stack affinity: stack parameters: VPCID: ssm: name: \u0026#34;/networking/vpc/account-parameters/${AWS::AccountId}/${AWS::Region}/VPCId\u0026#34; SubnetCIDR: default: \u0026#39;10.0.0.0/24\u0026#39; deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;   Committing the manifest file  Set your Author name Set your Email address Set your Commit message  Using a good / unique commit message will help you understand what is going on later.\n  Click the Commit changes button:    What did we just do? The YAML file we created in the previous step told the framework to perform the following actions:\n provision a product named subnet into the default region of the account  When you added the following:\n launches: subnet: portfolio: \u0026#34;networking-mandatory\u0026#34; product: \u0026#34;subnet\u0026#34; version: \u0026#34;v1\u0026#34; depends_on: - name: vpc type: stack affinity: stack parameters: VPCID: ssm: name: \u0026#34;/networking/vpc/account-parameters/${AWS::AccountId}/${AWS::Region}/VPCId\u0026#34; SubnetCIDR: default: \u0026#39;10.0.0.0/24\u0026#39; deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;   You told the framework to provision v1 of subnet from the portfolio networking-mandatory into every account that has the tag type:prod\n accounts: - account_id: \u0026#34;\u0026lt;YOUR_ACCOUNT_ID_WITHOUT_HYPHENS\u0026gt;\u0026#34; name: \u0026#34;puppet-account\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34;    Within each account there will be a copy of the product provisioned into the default region:\n accounts: - account_id: \u0026#34;\u0026lt;YOUR_ACCOUNT_ID_WITHOUT_HYPHENS\u0026gt;\u0026#34; name: \u0026#34;puppet-account\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34;  regions_enabled: - \u0026#34;eu-west-1\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34;   Verifying the provisioned product Once you have made your changes the ServiceCatalogPuppet Pipeline should have run. If you were quick in making the change, the pipeline may still be running. If it has not yet started feel free to the hit the Release change button.\nOnce it has completed it should show the stages in green to indicate they have completed successfully:\n  Once you have verified the pipeline has run you can go to Service Catalog provisioned products to view your provisioned product. Please note when you arrive at the provisioned product page you will need to select account from the filter by drop down in the top right:\n  You have now successfully provisioned a product\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/2021-workshop/400-self-service/15-using-service-catalog.html",
	"title": "Using AWS Service Catalog",
	"tags": [],
	"description": "",
	"content": " What are we going to do? We are going to perform the following steps:\n Create a portfolio for sharing products Share the portfolio networking-optional into a spoke account  Step by step guide Here are the steps you need to follow to \u0026ldquo;Using AWS Service Catalog\u0026rdquo;\nCreate a portfolio for sharing products   Navigate to the ServiceCatalogFactory CodeCommit repository\n  Click on portfolios, then networking.yaml and click \u0026ldquo;Edit\u0026rdquo;.\n  Append the following snippet into the portfolios section whilst updating the role name to the one you are using in the associations section:\n - DisplayName: \u0026#34;optional\u0026#34; Description: \u0026#34;Portfolio containing the optional networking components\u0026#34; ProviderName: \u0026#34;cloud-engineering\u0026#34; Associations: - \u0026#34;arn:aws:iam::${AWS::AccountId}:role/\u0026lt;INSERT YOUR ROLE NAME HERE\u0026gt;\u0026#34; Tags: - Key: \u0026#34;type\u0026#34; Value: \u0026#34;governance\u0026#34; - Key: \u0026#34;creator\u0026#34; Value: \u0026#34;cloud-engineering\u0026#34; - Key: \u0026#34;cost-center\u0026#34; Value: \u0026#34;governance\u0026#34;     Add the portfolio of - \u0026quot;optional\u0026quot; to the portfolios list for subnet product.\n  The file should look like the following:\n Schema: factory-2019-04-01 Products: - Name: \u0026#34;subnet\u0026#34; Owner: \u0026#34;networking@example.com\u0026#34; Description: \u0026#34;subnet for networking\u0026#34; Distributor: \u0026#34;networking team\u0026#34; SupportDescription: \u0026#34;Speak to networking@example.com about exceptions and speak to cloud-engineering@example.com about implementation issues\u0026#34; SupportEmail: \u0026#34;cloud-engineering@example.com\u0026#34; SupportUrl: \u0026#34;https://wiki.example.com/cloud-engineering/networking/subnet\u0026#34; Tags: - Key: \u0026#34;type\u0026#34; Value: \u0026#34;governance\u0026#34; - Key: \u0026#34;creator\u0026#34; Value: \u0026#34;cloud-engineering\u0026#34; - Key: \u0026#34;cost-center\u0026#34; Value: \u0026#34;governance\u0026#34; Versions: - Name: \u0026#34;v1\u0026#34; Description: \u0026#34;v1 of subnet\u0026#34; Active: True Source: Provider: \u0026#34;CodeCommit\u0026#34; Configuration: RepositoryName: \u0026#34;subnet\u0026#34; BranchName: \u0026#34;main\u0026#34; Portfolios: - \u0026#34;mandatory\u0026#34; - \u0026#34;optional\u0026#34; Portfolios: - DisplayName: \u0026#34;mandatory\u0026#34; Description: \u0026#34;Portfolio containing the mandatory networking components\u0026#34; ProviderName: \u0026#34;cloud-engineering\u0026#34; Associations: - \u0026#34;arn:aws:iam::${AWS::AccountId}:role/\u0026lt;INSERT YOUR ROLE NAME HERE\u0026gt;\u0026#34; Tags: - Key: \u0026#34;type\u0026#34; Value: \u0026#34;governance\u0026#34; - Key: \u0026#34;creator\u0026#34; Value: \u0026#34;cloud-engineering\u0026#34; - Key: \u0026#34;cost-center\u0026#34; Value: \u0026#34;governance\u0026#34; - DisplayName: \u0026#34;optional\u0026#34; Description: \u0026#34;Portfolio containing the optional networking components\u0026#34; ProviderName: \u0026#34;cloud-engineering\u0026#34; Associations: - \u0026#34;arn:aws:iam::${AWS::AccountId}:role/\u0026lt;INSERT YOUR ROLE NAME HERE\u0026gt;\u0026#34; Tags: - Key: \u0026#34;type\u0026#34; Value: \u0026#34;governance\u0026#34; - Key: \u0026#34;creator\u0026#34; Value: \u0026#34;cloud-engineering\u0026#34; - Key: \u0026#34;cost-center\u0026#34; Value: \u0026#34;governance\u0026#34;     Set your Author name\n  Set your Email address\n  Set your Commit message\n  Using a good / unique commit message will help you understand what is going on later.\n  Click the Commit changes button:    What did we just do? The YAML file we created in the CodeCommit repository told the framework to create a new portfolio and to add the subnet product to that new portfolio.\nVerify the change worked Once you have made your changes the ServiceCatalogFactory Pipeline should have run. If you were very quick in making the change, the pipeline may still be running. If it has not yet started feel free to the hit the Release change button.\nOnce it has completed it should show the Source and Build stages in green to indicate they have completed successfully:\n  Verify the product was added to the portfolio Now that you have verified the pipeline has run you can go to Service Catalog portfolios to view your portfolio.\n Click on networking-optional      Click on the product subnet\n  Click on the version v1\n    Share the portfolio networking-optional into a spoke account   Navigate to the ServiceCatalogPuppet CodeCommit repository again\n  Click on manifest.yaml\n  Click Edit\n  Append the following snippet to the end of the main input field:\n spoke-local-portfolios: networking-optional: portfolio: \u0026#34;networking-optional\u0026#34; product_generation_method: copy depends_on: - name: vpc type: stack affinity: stack deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;     The main input field should look like this (remember to set your account_id):\n   accounts: - account_id: \u0026#34;\u0026lt;YOUR_ACCOUNT_ID_WITHOUT_HYPHENS\u0026gt;\u0026#34; name: \u0026#34;puppet-account\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34; stacks: delete-default-networking-function: name: \u0026#34;delete-default-networking-function\u0026#34; version: \u0026#34;v1\u0026#34; capabilities: - CAPABILITY_NAMED_IAM deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34; vpc: name: \u0026#34;vpc\u0026#34; version: \u0026#34;v1\u0026#34; depends_on: - name: \u0026#34;delete-default-networking\u0026#34; type: \u0026#34;lambda-invocation\u0026#34; affinity: \u0026#34;lambda-invocation\u0026#34; deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34; outputs: ssm: - param_name: \u0026#34;/networking/vpc/account-parameters/${AWS::AccountId}/${AWS::Region}/VPCId\u0026#34; stack_output: VPCId lambda-invocations: delete-default-networking: function_name: DeleteDefaultNetworking qualifier: $LATEST invocation_type: Event depends_on: - name: \u0026#34;delete-default-networking-function\u0026#34; type: \u0026#34;stack\u0026#34; affinity: \u0026#34;stack\u0026#34; invoke_for: tags: - regions: \u0026#34;default_region\u0026#34; tag: \u0026#34;type:prod\u0026#34; assertions: assert-no-default-vpcs: expected: source: manifest config: value: [] actual: source: boto3 config: client: \u0026#39;ec2\u0026#39; call: describe_vpcs arguments: {} use_paginator: true filter: Vpcs[?IsDefault==`true`].State depends_on: - name: \u0026#34;delete-default-networking\u0026#34; type: \u0026#34;lambda-invocation\u0026#34; affinity: \u0026#34;lambda-invocation\u0026#34; assert_for: tags: - regions: regions_enabled tag: type:prod launches: subnet: portfolio: \u0026#34;networking-mandatory\u0026#34; product: \u0026#34;subnet\u0026#34; version: \u0026#34;v1\u0026#34; depends_on: - name: vpc type: stack affinity: stack parameters: VPCID: ssm: name: \u0026#34;/networking/vpc/account-parameters/${AWS::AccountId}/${AWS::Region}/VPCId\u0026#34; SubnetCIDR: default: \u0026#39;10.0.0.0/24\u0026#39; deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34; workspaces: subnet: name: \u0026#34;subnet\u0026#34; version: \u0026#34;v1\u0026#34; depends_on: - name: vpc type: stack affinity: stack parameters: VPCID: ssm: name: \u0026#34;/networking/vpc/account-parameters/${AWS::AccountId}/${AWS::Region}/VPCId\u0026#34; SubnetCIDR: default: \u0026#39;10.0.1.0/24\u0026#39; deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34; spoke-local-portfolios: networking-optional: portfolio: \u0026#34;networking-optional\u0026#34; product_generation_method: copy depends_on: - name: vpc type: stack affinity: stack deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;   Committing the manifest file  Set your Author name Set your Email address Set your Commit message  Using a good / unique commit message will help you understand what is going on later.\n  Click the Commit changes button:    What did we just do? The changes we made told the framework to make a portfolio in the default region of each spoke with the tag type:prod. This portfolio will contain copies of the products that exist in the hub account.\nVerifying the portfolio share Once you have made your changes the ServiceCatalogPuppet Pipeline should have run. If you were quick in making the change, the pipeline may still be running. If it has not yet started feel free to the hit the Release change button.\nAs this workshop has been designed to run in a single region of a single account you cannot verify this step. If the pipeline ran and each stage has succeeded the share should have taken place. When sharing a portfolio with the same account it was created in the framework does not perform any actions.\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/2021-workshop/100-removing-the-default-networking/20-creating-the-lambda.html",
	"title": "Creating the Lambda",
	"tags": [],
	"description": "",
	"content": " What are we going to do? We are going to perform the following steps:\n Define a stack Add the source code for our product  The hub AWS Account is the source of truth for our stacks. Spoke AWS accounts are consumers of these stacks, you can think of them as accounts that need governance controls applied. For this workshop, we are using the same account as both the hub and spoke for simplicity; in a multi-account setup, these could be separate AWS Accounts and Regions.\nStep by step guide Here are the steps you need to follow to \u0026ldquo;Creating the Lambda\u0026rdquo;\nDefine a stack   Navigate to the ServiceCatalogFactory CodeCommit repository\n  Scroll down to the bottom of the page and hit the Create file button\n      Copy the following snippet into the main input field:\n Schema: factory-2019-04-01 Stacks: - Name: \u0026#34;delete-default-networking-function\u0026#34; Versions: - Name: \u0026#34;v1\u0026#34; Active: True Source: Provider: \u0026#34;CodeCommit\u0026#34; Configuration: RepositoryName: \u0026#34;delete-default-networking-function\u0026#34; BranchName: \u0026#34;main\u0026#34;     Set the File name to stacks/network-workshop.yaml\n  Set your Author name\n  Set your Email address\n  Set your Commit message\n  Using a good / unique commit message will help you understand what is going on later.\n  Click the Commit changes button:    What did we just do? The YAML file we created in the CodeCommit repository told the framework to:\n create a pipeline that will take source code from a branch named main of CodeCommit repo named delete-default-networking-function  Verify the change worked Once you have made your changes the ServiceCatalogFactory Pipeline should have run. If you were very quick in making the change, the pipeline may still be running. If it has not yet started feel free to the hit the Release change button.\nOnce it has completed it should show the Source and Build stages in green to indicate they have completed successfully:\n  The screenshots may differ slightly as the design of AWS CodePipeline changes. You should see a pipeline where each stage is green.\n Add the source code for our product When you configured your product version, you specified the following version:\n Versions: - Name: \u0026#34;v1\u0026#34; Active: True Source: Provider: \u0026#34;CodeCommit\u0026#34; Configuration: RepositoryName: \u0026#34;delete-default-networking-function\u0026#34; BranchName: \u0026#34;main\u0026#34;   This tells the framework the source code for the product comes from the main branch of a CodeCommit repository of the name delete-default-networking-function.\nWe now need to create the CodeCommit repository and add the AWS CloudFormation template we are going to use for our product.\n  Navigate to AWS CodeCommit\n  Click Create repository\n      Input the repository name delete-default-networking-function\n  Click Create\n     Scroll down to the bottom of the page and hit the Create file button     Copy the following snippet into the main input field:   AWSTemplateFormatVersion: \u0026#34;2010-09-09\u0026#34; Description: | Deletes the following default networking components from AWS Accounts: 1) Deletes the internet gateway 2) Deletes the subnets 4) Deletes the network access lists 5) Deletes the security groups 6) Deletes the default VPC {\u0026#34;framework\u0026#34;: \u0026#34;servicecatalog-products\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;product\u0026#34;, \u0026#34;product-set\u0026#34;: \u0026#34;delete-default-vpc\u0026#34;, \u0026#34;product\u0026#34;: \u0026#34;delete-default-vpc\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;v1\u0026#34;} Parameters: DeleteDefaultNetworkingRoleNameToAssume: Description: \u0026#34;Name of the IAM Role that will be assumed in the spoke account to remove networking\u0026#34; Type: String Default: \u0026#34;servicecatalog-puppet/PuppetRole\u0026#34; DeleteDefaultVPCLambdaExecutionIAMRoleName: Description: \u0026#34;Name of the IAM Role that will be created to execute the lambda function\u0026#34; Type: String Default: \u0026#34;DeleteDefaultVPCLambdaExecution\u0026#34; DeleteDefaultVPCLambdaExecutionIAMRolePath: Description: \u0026#34;The path for the IAM Role that will be created to execute the lambda function\u0026#34; Type: String Default: \u0026#34;/DeleteDefaultVPCLambdaExecution/\u0026#34; DeleteDefaultNetworkingLambdaFunctionName: Description: \u0026#34;The name to give the function that deletes the default networking resources\u0026#34; Type: String Default: \u0026#34;DeleteDefaultNetworking\u0026#34; Resources: DefaultVpcDeletionRole: Type: AWS::IAM::Role Properties: RoleName: !Ref DeleteDefaultVPCLambdaExecutionIAMRoleName Path: !Ref DeleteDefaultVPCLambdaExecutionIAMRolePath AssumeRolePolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: sts:AssumeRole Principal: Service: lambda.amazonaws.com Policies: - PolicyName: Organizations PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: - \u0026#34;sts:AssumeRole\u0026#34; Resource: !Sub \u0026#39;arn:${AWS::Partition}:iam::*:role/${DeleteDefaultNetworkingRoleNameToAssume}\u0026#39; - Effect: Allow Action: - \u0026#34;ec2:DescribeInternetGateways\u0026#34; - \u0026#34;ec2:DetachInternetGateway\u0026#34; - \u0026#34;ec2:DeleteInternetGateway\u0026#34; - \u0026#34;ec2:DescribeSubnets\u0026#34; - \u0026#34;ec2:DeleteSubnet\u0026#34; - \u0026#34;ec2:DescribeRouteTables\u0026#34; - \u0026#34;ec2:DeleteRouteTable\u0026#34; - \u0026#34;ec2:DescribeNetworkAcls\u0026#34; - \u0026#34;ec2:DeleteNetworkAcl\u0026#34; - \u0026#34;ec2:DeleteSecurityGroup\u0026#34; - \u0026#34;ec2:DeleteVpc\u0026#34; - \u0026#34;ec2:DescribeRegions\u0026#34; - \u0026#34;ec2:DescribeAccountAttributes\u0026#34; - \u0026#34;ec2:DescribeNetworkInterfaces\u0026#34; - \u0026#34;ec2:DescribeSecurityGroups\u0026#34; - \u0026#34;ec2:DeleteSecurityGroup\u0026#34; - \u0026#34;logs:CreateLogGroup\u0026#34; - \u0026#34;logs:CreateLogStream\u0026#34; - \u0026#34;logs:PutLogEvents\u0026#34; Resource: \u0026#39;*\u0026#39; Function: Type: AWS::Lambda::Function Properties: FunctionName: !Ref DeleteDefaultNetworkingLambdaFunctionName Handler: index.lambda_handler Runtime: python3.7 Timeout: 600 Role: !GetAtt DefaultVpcDeletionRole.Arn Environment: Variables: DeleteDefaultNetworkingRoleNameToAssume: !Ref DeleteDefaultNetworkingRoleNameToAssume Partition: !Sub \u0026#39;${AWS::Partition}\u0026#39; Code: ZipFile: | import boto3, logging, traceback, os from boto3 import Session logger = logging.getLogger() logger.setLevel(logging.INFO) logging.basicConfig( format=\u0026#39;%(levelname)s %(threadName)s [%(filename)s:%(lineno)d] %(message)s\u0026#39;, datefmt=\u0026#39;%Y-%m-%d:%H:%M:%S\u0026#39;, level=logging.INFO ) def delete_igw(client, vpc_id): fltr = [{\u0026#39;Name\u0026#39;: \u0026#39;attachment.vpc-id\u0026#39;, \u0026#39;Values\u0026#39;: [vpc_id]}] try: igw = client.describe_internet_gateways(Filters=fltr)[\u0026#39;InternetGateways\u0026#39;] if igw: igw_id = igw[0][\u0026#39;InternetGatewayId\u0026#39;] client.detach_internet_gateway(InternetGatewayId=igw_id, VpcId=vpc_id) client.delete_internet_gateway(InternetGatewayId=igw_id) return except Exception as ex: logger.error(ex) traceback.print_tb(ex.__traceback__) raise def delete_subnets(client): try: subs = client.describe_subnets()[\u0026#39;Subnets\u0026#39;] if subs: for sub in subs: sub_id = sub[\u0026#39;SubnetId\u0026#39;] client.delete_subnet(SubnetId=sub_id) return except Exception as ex: logger.error(ex) traceback.print_tb(ex.__traceback__) raise def delete_rtbs(client): try: rtbs = client.describe_route_tables()[\u0026#39;RouteTables\u0026#39;] if rtbs: for rtb in rtbs: main = False for assoc in rtb[\u0026#39;Associations\u0026#39;]: main = assoc[\u0026#39;Main\u0026#39;] if main: continue rtb_id = rtb[\u0026#39;RouteTableId\u0026#39;] client.delete_route_table(RouteTableId=rtb_id) return except Exception as ex: logger.error(ex) traceback.print_tb(ex.__traceback__) raise def delete_acls(client): try: acls = client.describe_network_acls()[\u0026#39;NetworkAcls\u0026#39;] if acls: for acl in acls: default = acl[\u0026#39;IsDefault\u0026#39;] if default: continue acl_id = acl[\u0026#39;NetworkAclId\u0026#39;] client.delete_network_acl(NetworkAclId=acl_id) return except Exception as ex: logger.error(ex) traceback.print_tb(ex.__traceback__) raise def delete_sgps(client): try: sgps = client.describe_security_groups()[\u0026#39;SecurityGroups\u0026#39;] if sgps: for sgp in sgps: default = sgp[\u0026#39;GroupName\u0026#39;] if default == \u0026#39;default\u0026#39;: continue sg_id = sgp[\u0026#39;GroupId\u0026#39;] client.delete_security_group(GroupId=sg_id) return except Exception as ex: logger.error(ex) traceback.print_tb(ex.__traceback__) raise def delete_vpc(client, vpc_id, region): try: client.delete_vpc(VpcId=vpc_id) logger.info(\u0026#39;VPC {} has been deleted from the {} region.\u0026#39;.format(vpc_id, region)) return except Exception as ex: logger.error(ex) traceback.print_tb(ex.__traceback__) raise def lambda_handler(e, c): account_id=e.get(\u0026#34;account_id\u0026#34;) region=e.get(\u0026#34;region\u0026#34;) role_arn = f\u0026#34;arn:{os.getenv(\u0026#39;Partition\u0026#39;)}:iam::{account_id}:role/{os.getenv(\u0026#39;DeleteDefaultNetworkingRoleNameToAssume\u0026#39;)}\u0026#34; sts = Session().client(\u0026#39;sts\u0026#39;) assumed_role_object = sts.assume_role( RoleArn=role_arn, RoleSessionName=\u0026#34;spoke\u0026#34;, ) credentials = assumed_role_object[\u0026#39;Credentials\u0026#39;] kwargs = { \u0026#34;service_name\u0026#34;: \u0026#34;ec2\u0026#34;, \u0026#34;aws_access_key_id\u0026#34;: credentials[\u0026#39;AccessKeyId\u0026#39;], \u0026#34;aws_secret_access_key\u0026#34;: credentials[\u0026#39;SecretAccessKey\u0026#39;], \u0026#34;aws_session_token\u0026#34;: credentials[\u0026#39;SessionToken\u0026#39;], } ec2 = Session().client(**kwargs, region_name=region) try: attribs = ec2.describe_account_attributes(AttributeNames=[\u0026#39;default-vpc\u0026#39;])[\u0026#39;AccountAttributes\u0026#39;] vpc_id = attribs[0][\u0026#39;AttributeValues\u0026#39;][0][\u0026#39;AttributeValue\u0026#39;] if vpc_id == \u0026#39;none\u0026#39;: logger.info(\u0026#39;Default VPC not found in {}\u0026#39;.format(region)) return # Since most resources are attached an ENI, this checks for additional resources f = [{\u0026#39;Name\u0026#39;: \u0026#39;vpc-id\u0026#39;, \u0026#39;Values\u0026#39;: [vpc_id]}] eni = ec2.describe_network_interfaces(Filters=f)[\u0026#39;NetworkInterfaces\u0026#39;] if eni: logger.error(\u0026#39;VPC {} has existing resources in the {} region.\u0026#39;.format(vpc_id, region)) return delete_igw(ec2, vpc_id) delete_subnets(ec2) delete_rtbs(ec2) delete_acls(ec2) delete_sgps(ec2) delete_vpc(ec2, vpc_id, region) except Exception as ex: logger.error(ex) traceback.print_tb(ex.__traceback__)     Set the File name to stack.template.yaml\n  Set your Author name\n  Set your Email address\n  Set your Commit message\n  Using a good / unique commit message will help you understand what is going on later.\n Creating that file should trigger your stack\u0026ndash;delete-default-networking-function-v1-pipeline.\nOnce the pipeline has completed it should show the stages in green to indicate they have completed successfully:\n  You should see your commit message on this screen, it will help you know which version of ServiceCatalogFactory repository the pipeline is processing.\n The screenshots may differ slightly as the design of AWS CodePipeline changes. You should see a pipeline where each stage is green.\n You have now successfully created a stack!\nVerify the stack is present in Amazon S3 Now that you have verified the pipeline has run correctly you can go to Amazon S3 to view the stack.\n  Navigate to https://s3.console.aws.amazon.com/s3/home\n  Select the bucket named sc-puppet-stacks-repository-\u0026lt;account_id\u0026gt;\n  Navigate to stack/delete-default-networking-function/v1 where you should see an object named stack.template.yaml\n  "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/installation.html",
	"title": "Installation",
	"tags": [],
	"description": "",
	"content": "Prerequisites In order to install these tools you will need:\n A single AWS Account which you can log into A web browser where to access the AWS console.  You will also need to decide which account to install these tools into.\nThis account will contain the AWS CodePipelines and will need to be accessible to any accounts you would like to share products with. If you want to use the optional AWS Organizations support you will need to install the tools into an AWS Account where there is (or can be) a trust relationship with the AWS Organizations management account. You can install these tools into your management account but this is not recommended.\nBoth tools should be installed into the same region of the same account.\nTask list Here:\n Service Catalog Factory   Service Catalog Puppet   Onboarding spokes   Securing the installation   Installing specific versions   "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/2021-workshop/10-prerequisites/100-installation/20-service-catalog-factory.html",
	"title": "Service Catalog Factory",
	"tags": [],
	"description": "",
	"content": "Select the pre-configured CloudFormation Template Service Catalog Factory can be installed via a pre-created AWS CloudFormation template stored in Amazon S3. There are many configuration options for you to customise your installation. For this workshop we will be using the following quick link which has the settings already preconfigured for you:\n Create the initialisation stack Check the box labeled \u0026ldquo;I acknowledge that AWS CloudFormation might create IAM resources with custom names.\u0026rdquo; Hit Create stack Wait for the stack status to go to CREATE_COMPLETE    What have we deployed? The following AWS resources have just been deployed into your AWS Account:\nAWS CloudFormation stacks The AWS CodeBuild job created two AWS CloudFormation stacks which in turn deployed the resources listed below:\n URL: https://eu-west-1.console.aws.amazon.com/cloudformation/home?region=eu-west-1\n   Factory AWS CodeCommit repository This repository holds the Service Catalog Factory YAML files which are used to configure AWS Service Catalog portfolios and products.\n URL: https://eu-west-1.console.aws.amazon.com/codesuite/codecommit/repositories?region=eu-west-1\n   Factory AWS CodePipeline This AWS CodePipeline is triggered by updates to the AWS CodeCommit repository. When run, it will create the AWS Service Catalog portfolios and products defined in the portfolio files.\n URL: https://eu-west-1.console.aws.amazon.com/codesuite/codepipeline/pipelines?region=eu-west-1\n   The pipeline execution will show as failing at this point. This is expected.\n Amazon S3 Buckets An Amazon S3 Bucket was created to store artifacts for Service Catalog factory.\n URL: https://s3.console.aws.amazon.com/s3/home?region=eu-west-1\n   "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/installation/20-service-catalog-factory.html",
	"title": "Service Catalog Factory",
	"tags": [],
	"description": "",
	"content": "Create a new AWS CloudFormation stack  Select the AWS CloudFormation Service.    If you are installing Service Catalog Puppet it will need to be installed into the same account as Service Catalog Factory.\n  Select \u0026lsquo;Create Stack\u0026rsquo;  Service Catalog Factory can be installed via a pre-created AWS CloudFormation template stored in Amazon S3 under the following URL:\n https://service-catalog-tools.s3.eu-west-2.amazonaws.com/factory/latest/servicecatalog-factory-initialiser.template.yaml\n  Paste this URL under \u0026lsquo;Amazon S3 URL\u0026rsquo;: Hit Next  Specify Stack details  Specify the Stack details as follows:  Stack Name: factory-initialization-stack    You should fill in the details depending on which source code management system you want to use:\nCodeCommit You need to set SCMSourceProvider to CodeCommit.\nYou should also set the following:\n SCMRepositoryName - this is the name of the git repo to use SCMBranchName - this is the branch you want to use SCMShouldCreateRepo - set this to true if you want the tools to create the repo for you  Github.com / Github Enterprise / Bitbucket Cloud You should have a read through the following guide: https://docs.aws.amazon.com/dtconsole/latest/userguide/connections.html\nYou need to set SCMSourceProvider to CodeStarSourceConnection.\nYou should also set the following:\n SCMConnectionArn - this is the Arn of the connection you created in the console SCMFullRepositoryId - the value for this is dependant on which SCM you use SCMBranchName - this is the branch you want to use  S3 You need to set SCMSourceProvider to S3.\nYou should also set the following:\n SCMBucketName - this is the name of the S3 bucket you want to use SCMObjectKey - this is the name of the object key you will be uploading your zip file as to trigger pipeline runs SCMShouldCreateRepo - set this to true if you want the tools to create the repo for you  Configure Stack Options  Set the common tags you want to use for the resources created by the framework. These may be cost management tags or RBAC/ABAC required tags. Hit Next Acknowledge that the Stack will create an IAM Role Hit \u0026lsquo;Create Stack\u0026rsquo;     You will now see the stack status as CREATE_IN_PROGRESS     Wait for the stack status to go to CREATE_COMPLETE    What have we deployed? The following AWS resources have just been deployed into your AWS Account:\nAWS CloudFormation stacks The AWS CodeBuild job created two AWS CloudFormation stacks which in turn deployed the resources listed below:\n URL: https://eu-west-1.console.aws.amazon.com/cloudformation/home?region=eu-west-1\n   Factory AWS CodeCommit repository This repository holds the Service Catalog Factory YAML files which are used to configure AWS Service Catalog portfolios and products.\n URL: https://eu-west-1.console.aws.amazon.com/codesuite/codecommit/repositories?region=eu-west-1\n   Factory AWS CodePipeline This AWS CodePipeline is triggered by updates to the AWS CodeCommit repository. When run, it will create the AWS Service Catalog portfolios and products defined in the portfolio files.\n URL: https://eu-west-1.console.aws.amazon.com/codesuite/codepipeline/pipelines?region=eu-west-1\n   Amazon S3 Buckets An Amazon S3 Bucket was created to store artifacts for Service Catalog factory.\n URL: https://s3.console.aws.amazon.com/s3/home?region=eu-west-1\n   "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/upgrading/service-catalog-factory.html",
	"title": "Service Catalog Factory",
	"tags": [],
	"description": "",
	"content": "Before you update This guide applies to all users who installed using the AWS CloudFormation template. If you did not install using the template then you need to follow the install guide. When following the install guide, ensure you provision the initialiser stack into the same region you installed the tools using the CLI method. Installing this way will perform an update and will not break your install.\nIf you have added any regions since your initial install or modified any settings provided by the parameters in the install template ensure you specify the values you want for them as the install process will overwrite any previous settings you configured in your existing install.\nNavigate to CloudFormation  Select the AWS CloudFormation Service.      Select the initialization stack you created when installing Service Catalog Factory. The recommended stack name was factory-initialization-stack\n  Select \u0026lsquo;Update\u0026rsquo;\n  Select \u0026lsquo;Replace current template\u0026rsquo;\n  For Template source select \u0026lsquo;Amazon S3 URL\u0026rsquo; and paste in the following: https://service-catalog-tools.s3.eu-west-2.amazonaws.com/factory/latest/servicecatalog-factory-initialiser.template.yaml\n  Then hit \u0026lsquo;Next\u0026rsquo;\n  Make any changes you want to the parameters you provided and then hit \u0026lsquo;Next\u0026rsquo; again\n  Follow the rest of the steps to update the stack\n  As the stack updates the AWS CodeBuild project named servicecatalog-product-factory-initialiser is started. Once it is completed the stack update will complete. The project will update your installation to the latest version\n  "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/tools/puppet.html",
	"title": "Service Catalog Puppet",
	"tags": [],
	"description": "",
	"content": " What is Service Catalog Puppet Service Catalog Puppet is the second of the Service Catalog tools. It is an AWS Solution designed to help you manage a multi account environment. Using the solution you can provision resources, share portfolios, execute functions and execute assertions on the configuration of your environment. The configuration for the solution is stored in git and changes made to the configuration trigger a run of the solution.\nOverview When installing Service Catalog Puppet you create a pipeline named servicecatalog-puppet-pipeline. This pipeline can use AWS CodeStar connections, Amazon S3 or AWS CodeCommit as its source. The source contains descriptions of the actions you want to happen.\nWhen the pipeline runs it will verify all existing provisioning and sharing is configured as expected. Any manual actions applied since the last run are overridden and any new changes are applied. If you change the value of a parameter used for provisioning the provisioned resources will be updated.\n  Spoke Execution Mode When your pipeline takes 45+ mins to run we recommend switching to spoke execution mode. Instead of all operations occurring in the hub account some of the operations are delegated to the spokes where they run in parallel across each spoke. When using spoke execution mode the solution will still check if each action was performed correctly.\n  When using spoke execution mode the hub account generates a manifest file for the spoke - which is very similar to the one used in the hub. The hub account also generates a cache of data to share with the spoke - this contains any AWS Systems Manager parameters stored in the hub and used in the spoke as well as the AWS Service Catalog portfolio, product and provisioning artefact ids. The cache is stored in the hub within an Amazon S3 bucket and a signed url is shared with the spoke so it can retrieve the artefact.\nWhat Can I Do With The Solution The solution allows you to easily build out a workflow. You specify (using YAML) how you want your multi account environment to be configured and the solution will configure it as such. The solution will ensure the right actions are performed in the right order and that no API throttling limits are exceeded. Using the solution you can perform the following actions:\nStacks You can provision a stack in one or more regions of one or more accounts:\n  Launches You can provision a product in one or more regions of one or more accounts:\n  Spoke Local Portfolios You can share a portfolio in one or more regions of one or more accounts:\n  AWS Lambda Invokes You can invoke a lambda function for one or more regions of one or more accounts:\n  AWS CodeBuild Runs You can start a Code Build project for one or more regions of one or more accounts:\n  Assertions You can create an assertion for one or more regions of one or more accounts:\n  "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/2021-workshop/300-provisioning-subnets/20-using-terraform.html",
	"title": "Using Terraform",
	"tags": [],
	"description": "",
	"content": " What are we going to do? We are going to perform the following steps:\n Define a workspace Add the source code for our product Update the manifest file  Step by step guide Here are the steps you need to follow to \u0026ldquo;Using Terraform\u0026rdquo;\nDefine a workspace   Navigate to the ServiceCatalogFactory CodeCommit repository\n  Open the Add file menu and click the Create file button\n  Paste the following snippet to the main input field:\n Schema: factory-2019-04-01 Workspaces: - Name: \u0026#34;subnet\u0026#34; Versions: - Name: \u0026#34;v1\u0026#34; Active: True Source: Provider: \u0026#34;CodeCommit\u0026#34; Configuration: RepositoryName: \u0026#34;subnet-terraform\u0026#34; BranchName: \u0026#34;main\u0026#34;     Set your filename to workspaces/networking.yaml\n  Set your Author name\n  Set your Email address\n  Set your Commit message\n  Using a good / unique commit message will help you understand what is going on later.\n  Click the Commit changes button:    What did we just do? The YAML file we created in the CodeCommit repository told the framework to:\n create a pipeline that will take source code from a branch named main of CodeCommit repo named subnet-terraform  Verify the change worked Once you have made your changes the ServiceCatalogFactory Pipeline should have run. If you were very quick in making the change, the pipeline may still be running. If it has not yet started feel free to the hit the Release change button.\nOnce it has completed it should show the Source and Build stages in green to indicate they have completed successfully:\n  The screenshots may differ slightly as the design of AWS CodePipeline changes. You should see a pipeline where each stage is green.\n Add the source code for our product When you configured your product version, you specified the following version:\n Versions: - Name: \u0026#34;v1\u0026#34; Active: True Source: Provider: \u0026#34;CodeCommit\u0026#34; Configuration: RepositoryName: \u0026#34;subnet-terraform\u0026#34; BranchName: \u0026#34;main\u0026#34;   This tells the framework the source code for the product comes from the main branch of a CodeCommit repository of the name subnet-terraform.\nWe now need to create the CodeCommit repository and add the AWS CloudFormation template we are going to use for our workspace.\n  Navigate to AWS CodeCommit\n  Click Create repository\n     Input the name subnet-terraform     Click Create     Scroll down to the bottom of the page and hit the Create file button     Copy the following snippet into the main input field:   variable \u0026#34;VPCID\u0026#34; { type = string } variable \u0026#34;SubnetCIDR\u0026#34; { type = string } resource \u0026#34;aws_subnet\u0026#34; \u0026#34;main\u0026#34; { vpc_id = var.VPCID cidr_block = var.SubnetCIDR }     Set the File name to subnet.tf\n  Set your Author name\n  Set your Email address\n  Set your Commit message\n  Using a good / unique commit message will help you understand what is going on later.\n The name or number of files does not matter when you are creating your own workspaces using Terraform.\n Creating that file should trigger your workspace\u0026ndash;subnet-v1-pipeline.\nOnce the pipeline has completed it should show the stages in green to indicate they have completed successfully:\n  You should see your commit message on this screen, it will help you know which version of ServiceCatalogFactory repository the pipeline is processing.\n The screenshots may differ slightly as the design of AWS CodePipeline changes. You should see a pipeline where each stage is green.\n You have now successfully created a stack!\nVerify the stack is present in Amazon S3 Now that you have verified the pipeline has run correctly you can go to Amazon S3 to view the stack.\n  Navigate to https://s3.console.aws.amazon.com/s3/home\n  Select the bucket named sc-puppet-stacks-repository-\u0026lt;account_id\u0026gt;\n  Navigate to workspace/subnet/v1 where you should see an object named workspace.zip\n  Update the manifest file   Navigate to the ServiceCatalogPuppet CodeCommit repository again\n  Click on manifest.yaml\n  Click Edit\n  Append the following snippet to the end of the file in the input field:\n workspaces: subnet: name: \u0026#34;subnet\u0026#34; version: \u0026#34;v1\u0026#34; depends_on: - name: vpc type: stack affinity: stack parameters: VPCID: ssm: name: \u0026#34;/networking/vpc/account-parameters/${AWS::AccountId}/${AWS::Region}/VPCId\u0026#34; SubnetCIDR: default: \u0026#39;10.0.1.0/24\u0026#39; deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;     The main input field should look like this (remember to set your account_id):\n   accounts: - account_id: \u0026#34;\u0026lt;YOUR_ACCOUNT_ID_WITHOUT_HYPHENS\u0026gt;\u0026#34; name: \u0026#34;puppet-account\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34; stacks: delete-default-networking-function: name: \u0026#34;delete-default-networking-function\u0026#34; version: \u0026#34;v1\u0026#34; capabilities: - CAPABILITY_NAMED_IAM deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34; vpc: name: \u0026#34;vpc\u0026#34; version: \u0026#34;v1\u0026#34; depends_on: - name: \u0026#34;delete-default-networking\u0026#34; type: \u0026#34;lambda-invocation\u0026#34; affinity: \u0026#34;lambda-invocation\u0026#34; deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34; outputs: ssm: - param_name: \u0026#34;/networking/vpc/account-parameters/${AWS::AccountId}/${AWS::Region}/VPCId\u0026#34; stack_output: VPCId lambda-invocations: delete-default-networking: function_name: DeleteDefaultNetworking qualifier: $LATEST invocation_type: Event depends_on: - name: \u0026#34;delete-default-networking-function\u0026#34; type: \u0026#34;stack\u0026#34; affinity: \u0026#34;stack\u0026#34; invoke_for: tags: - regions: \u0026#34;default_region\u0026#34; tag: \u0026#34;type:prod\u0026#34; assertions: assert-no-default-vpcs: expected: source: manifest config: value: [] actual: source: boto3 config: client: \u0026#39;ec2\u0026#39; call: describe_vpcs arguments: {} use_paginator: true filter: Vpcs[?IsDefault==`true`].State depends_on: - name: \u0026#34;delete-default-networking\u0026#34; type: \u0026#34;lambda-invocation\u0026#34; affinity: \u0026#34;lambda-invocation\u0026#34; assert_for: tags: - regions: regions_enabled tag: type:prod launches: subnet: portfolio: \u0026#34;networking-mandatory\u0026#34; product: \u0026#34;subnet\u0026#34; version: \u0026#34;v1\u0026#34; depends_on: - name: vpc type: stack affinity: stack parameters: VPCID: ssm: name: \u0026#34;/networking/vpc/account-parameters/${AWS::AccountId}/${AWS::Region}/VPCId\u0026#34; SubnetCIDR: default: \u0026#39;10.0.0.0/24\u0026#39; deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34; workspaces: subnet: name: \u0026#34;subnet\u0026#34; version: \u0026#34;v1\u0026#34; depends_on: - name: vpc type: stack affinity: stack parameters: VPCID: ssm: name: \u0026#34;/networking/vpc/account-parameters/${AWS::AccountId}/${AWS::Region}/VPCId\u0026#34; SubnetCIDR: default: \u0026#39;10.0.1.0/24\u0026#39; deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;   Committing the manifest file Now that we have updated the manifest file we are ready to commit it.\n Set your Author name Set your Email address Set your Commit message  Using a good / unique commit message will help you understand what is going on later.\n  Click the Commit changes button:    What did we just do? When you added the following:\n workspaces: subnet: name: \u0026#34;subnet\u0026#34; version: \u0026#34;v1\u0026#34; depends_on: - name: vpc type: stack affinity: stack parameters: VPCID: ssm: name: \u0026#34;/networking/vpc/account-parameters/${AWS::AccountId}/${AWS::Region}/VPCId\u0026#34; SubnetCIDR: default: \u0026#39;10.0.1.0/24\u0026#39; deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;   You told the framework to provision v1 of subnet into the default region of each account that has the tag type:prod\nVerifying the provisioned stack Once you have made your changes the ServiceCatalogPuppet Pipeline should have run. If you were quick in making the change, the pipeline may still be running. If it has not yet started feel free to the hit the Release change button.\nOnce it has completed it should show the stages in green to indicate they have completed successfully:\n  The screenshots may differ slightly as the design of AWS CodePipeline changes. You should see a pipeline where each stage is green.\n You have now successfully provisioned a workspace.\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/upgrading.html",
	"title": "Upgrading",
	"tags": [],
	"description": "",
	"content": "Prerequisites In order to upgrade these tools you will need:\n to have installed the tools  Task list  Service Catalog Factory   Service Catalog Puppet   "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/2021-workshop/200-provisioning-a-vpc/20-creating-the-stack.html",
	"title": "Creating the Stack",
	"tags": [],
	"description": "",
	"content": " What are we going to do? We are going to perform the following steps:\n Define a stack Add the source code for our product  Step by step guide Here are the steps you need to follow to \u0026ldquo;Creating the Stack\u0026rdquo;\nDefine a stack   Navigate to the ServiceCatalogFactory CodeCommit repository\n  Click on stacks\n  Click on network-workshop.yaml\n  Click on edit\n  Append the following snippet to the main input field:\n - Name: \u0026#34;vpc\u0026#34; Versions: - Name: \u0026#34;v1\u0026#34; Active: True Source: Provider: \u0026#34;CodeCommit\u0026#34; Configuration: RepositoryName: \u0026#34;vpc\u0026#34; BranchName: \u0026#34;main\u0026#34;     The full file should look like this:\n Schema: factory-2019-04-01 Stacks: - Name: \u0026#34;delete-default-networking-function\u0026#34; Versions: - Name: \u0026#34;v1\u0026#34; Active: True Source: Provider: \u0026#34;CodeCommit\u0026#34; Configuration: RepositoryName: \u0026#34;delete-default-networking-function\u0026#34; BranchName: \u0026#34;main\u0026#34; - Name: \u0026#34;vpc\u0026#34; Versions: - Name: \u0026#34;v1\u0026#34; Active: True Source: Provider: \u0026#34;CodeCommit\u0026#34; Configuration: RepositoryName: \u0026#34;vpc\u0026#34; BranchName: \u0026#34;main\u0026#34;     Set your Author name\n  Set your Email address\n  Set your Commit message\n  Using a good / unique commit message will help you understand what is going on later.\n  Click the Commit changes button:    What did we just do? The YAML file we created in the CodeCommit repository told the framework to:\n create a pipeline that will take source code from a branch named main of CodeCommit repo named vpc  Verify the change worked Once you have made your changes the ServiceCatalogFactory Pipeline should have run. If you were very quick in making the change, the pipeline may still be running. If it has not yet started feel free to the hit the Release change button.\nOnce it has completed it should show the Source and Build stages in green to indicate they have completed successfully:\n  The screenshots may differ slightly as the design of AWS CodePipeline changes. You should see a pipeline where each stage is green.\n Add the source code for our product When you configured your product version, you specified the following version:\n Versions: - Name: \u0026#34;v1\u0026#34; Active: True Source: Provider: \u0026#34;CodeCommit\u0026#34; Configuration: RepositoryName: \u0026#34;vpc\u0026#34; BranchName: \u0026#34;main\u0026#34;   This tells the framework the source code for the product comes from the main branch of a CodeCommit repository of the name vpc.\nWe now need to create the CodeCommit repository and add the AWS CloudFormation template we are going to use for our product.\n  Navigate to AWS CodeCommit\n  Click Create repository\n     Input the name vpc     Click Create     Scroll down to the bottom of the page and hit the Create file button     Copy the following snippet into the main input field:   AWSTemplateFormatVersion: \u0026#39;2010-09-09\u0026#39; Description: | Builds out a VPC for use Parameters: VPCCIDR: Type: String Default: \u0026#39;10.0.0.0/16\u0026#39; Description: | Subnet to use for the VPC Resources: VPC: Type: AWS::EC2::VPC Description: The vpc being created Properties: EnableDnsSupport: true EnableDnsHostnames: true CidrBlock: !Ref VPCCIDR Outputs: VPCId: Description: The ID of the VPC that was created Value: !Ref VPC     Set the File name to stack.template.yaml\n  Set your Author name\n  Set your Email address\n  Set your Commit message\n  Using a good / unique commit message will help you understand what is going on later.\n Creating that file should trigger your stack\u0026ndash;vpc-v1-pipeline.\nOnce the pipeline has completed it should show the stages in green to indicate they have completed successfully:\n  You should see your commit message on this screen, it will help you know which version of ServiceCatalogFactory repository the pipeline is processing.\n The screenshots may differ slightly as the design of AWS CodePipeline changes. You should see a pipeline where each stage is green.\n You have now successfully created a stack!\nVerify the stack is present in Amazon S3 Now that you have verified the pipeline has run correctly you can go to Amazon S3 to view the stack.\n  Navigate to https://s3.console.aws.amazon.com/s3/home\n  Select the bucket named sc-puppet-stacks-repository-\u0026lt;account_id\u0026gt;\n  Navigate to stack/vpc/v1 where you should see an object named stack.template.yaml\n  "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/2021-workshop/100-removing-the-default-networking/30-deploying-the-lambda.html",
	"title": "Deploying the Lambda",
	"tags": [],
	"description": "",
	"content": " What are we going to do? We are going to perform the following steps:\n Create a manifest file with our account in it Provision the stack delete-default-networking-function into an account  Step by step guide Here are the steps you need to follow to provision the stack.\nCreate a manifest file with our account in it   Navigate to the ServiceCatalogPuppet CodeCommit repository\n  Scroll down to the bottom of the page and hit the Create file button\n    The screenshots may differ slightly as the design of AWS CodePipeline changes. You should see a pipeline where each stage is green.\n  Copy the following snippet into the main input field:   accounts: - account_id: \u0026#34;${AWS::PuppetAccountId}\u0026#34; name: \u0026#34;puppet-account\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34;   Provision the stack delete-default-networking-function into an account   Append the following snippet to the end of the main input field:\n stacks: delete-default-networking-function: name: \u0026#34;delete-default-networking-function\u0026#34; version: \u0026#34;v1\u0026#34; capabilities: - CAPABILITY_NAMED_IAM deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;     The main input field should look like this:\n   accounts: - account_id: \u0026#34;${AWS::PuppetAccountId}\u0026#34; name: \u0026#34;puppet-account\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34; stacks: delete-default-networking-function: name: \u0026#34;delete-default-networking-function\u0026#34; version: \u0026#34;v1\u0026#34; capabilities: - CAPABILITY_NAMED_IAM deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;   Committing the manifest file Now that we have written the manifest file we are ready to commit it.\n  Set the File name to manifest.yaml\n  Set your Author name\n  Set your Email address\n  Set your Commit message\n  Using a good / unique commit message will help you understand what is going on later.\n  Click the Commit changes button:    What did we just do? When you added the following:\n stacks: delete-default-networking-function: name: \u0026#34;delete-default-networking-function\u0026#34; version: \u0026#34;v1\u0026#34; capabilities: - CAPABILITY_NAMED_IAM deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;   You told the framework to provision v1 of delete-default-networking-function into the default region of each account that has the tag type:prod\n accounts: - account_id: \u0026#34;\u0026lt;YOUR_ACCOUNT_ID_WITHOUT_HYPHENS\u0026gt;\u0026#34; name: \u0026#34;puppet-account\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34;   Verifying the provisioned stack Once you have made your changes the ServiceCatalogPuppet Pipeline should have run. If you were quick in making the change, the pipeline may still be running. If it has not yet started feel free to the hit the Release change button.\nOnce it has completed it should show the stages in green to indicate they have completed successfully:\n  The screenshots may differ slightly as the design of AWS CodePipeline changes. You should see a pipeline where each stage is green.\n Once you have verified the pipeline has run you can go to the AWS CloudFormation console in the default region of the account you specified to view your provisioned stack.\nYou have now successfully provisioned a stack.\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/installation/30-service-catalog-puppet/30-installing-puppet.html",
	"title": "Installing Puppet",
	"tags": [],
	"description": "",
	"content": "Navigate to CloudFormation  Select the AWS CloudFormation service.    Create a new AWS CloudFormation stack  Select \u0026lsquo;Create Stack\u0026rsquo;    Note you must have installed factory into this account already. If you have not done this already navigate to \u0026lsquo;Install Factory Process\u0026rsquo;\n Select the pre-configured AWS CloudFormation template Service Catalog Puppet can be installed via a pre-created AWS CloudFormation template stored in Amazon S3 under the following URL:\n https://service-catalog-tools.s3.eu-west-2.amazonaws.com/puppet/latest/servicecatalog-puppet-initialiser.template.yaml\n  Paste this URL under \u0026lsquo;Amazon S3 URL\u0026rsquo;: Hit Next    Specify AWS CloudFormation stack details You will need to fill in the parameters for the template. We recommend the Stack Name: of puppet-initialization-stack\nYou should fill in the details depending on which source code management system you want to use:\nCodeCommit You need to set SCMSourceProvider to CodeCommit.\nYou should also set the following:\n SCMRepositoryName - this is the name of the git repo to use SCMBranchName - this is the branch you want to use SCMShouldCreateRepo - set this to true if you want the tools to create the repo for you  Github.com / Github Enterprise / Bitbucket Cloud You should have a read through the following guide: https://docs.aws.amazon.com/dtconsole/latest/userguide/connections.html\nYou need to set SCMSourceProvider to CodeStarSourceConnection.\nYou should also set the following:\n SCMConnectionArn - this is the Arn of the connection you created in the console SCMFullRepositoryId - the value for this is dependant on which SCM you use SCMBranchName - this is the branch you want to use  S3 You need to set SCMSourceProvider to S3.\nYou should also set the following:\n SCMBucketName - this is the name of the S3 bucket you want to use SCMObjectKey - this is the name of the object key you will be uploading your zip file as to trigger pipeline runs SCMShouldCreateRepo - set this to true if you want the tools to create the repo for you  Continuing Once you have done this, hit Next\n  Create the AWS CloudFormation stack  Leave Defaults for \u0026lsquo;Configure Stack Options\u0026rsquo; Hit Next Acknowledge that the Stack will create an IAM Role Hit \u0026lsquo;Create Stack\u0026rsquo;     You will now see the stack status as CREATE_IN_PROGRESS     Wait for the stack status to go to CREATE_COMPLETE    What have we deployed? The following AWS resources have just been deployed into your AWS Account:\nAWS CloudFormation stacks The AWS CodeBuild job created two AWS CloudFormation stacks which in turn deployed the resources listed below\n URL: https://eu-west-1.console.aws.amazon.com/cloudformation/home?region=eu-west-1\n   Puppet AWS CodeCommit repository This respository holds the Service Catalog Puppet manifest YAML file which is used to configure provisioning and sharing.\n URL: https://eu-west-1.console.aws.amazon.com/codesuite/codecommit/repositories?region=eu-west-1\n   Puppet AWS CodePipeline This AWS CodePipeline is triggered by updates to the AWS CodeCommit repository. When run, it will create the Service Catalog portfolios and products defined in the portfolio files.\n URL: https://eu-west-1.console.aws.amazon.com/codesuite/codepipeline/pipelines?region=eu-west-1\n   Amazon S3 buckets Three Amazon S3 buckets were created to store artifacts for Service Catalog Puppet.\n URL: https://s3.console.aws.amazon.com/s3/home?region=eu-west-1\n   "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/40-reinvent2019/30-prerequisites.html",
	"title": "Pre-requisites",
	"tags": [],
	"description": "",
	"content": "In order to complete this workshop you will need:\n A single AWS Account which you can log into A web browser to access the AWS console  If you are taking this workshop at re:Invent 2019 you should have been given a note when you entered the workshop. This note contains all you need to log into a AWS account we have created for you.\nWe have installed the tools needed for you to get going.\n If you want to run through this workshop in your own account please check the next section titled running-yourself.\n  Running yourself   "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/2021-workshop/200-provisioning-a-vpc/30-provisioning-the-stack.html",
	"title": "Provisioning the Stack",
	"tags": [],
	"description": "",
	"content": " What are we going to do? We are going to perform the following steps:\n Update the manifest file  Step by step guide Here are the steps you need to follow to provision the stack.\nUpdate the manifest file   Navigate to the ServiceCatalogPuppet CodeCommit repository again\n  Click on manifest.yaml\n  Click Edit\n  Append the following snippet to the end of the stacks section in the input field:\n vpc: name: \u0026#34;vpc\u0026#34; version: \u0026#34;v1\u0026#34; depends_on: - name: \u0026#34;delete-default-networking\u0026#34; type: \u0026#34;lambda-invocation\u0026#34; affinity: \u0026#34;lambda-invocation\u0026#34; deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34; outputs: ssm: - param_name: \u0026#34;/networking/vpc/account-parameters/${AWS::AccountId}/${AWS::Region}/VPCId\u0026#34; stack_output: VPCId     The main input field should look like this (remember to set your account_id):\n   accounts: - account_id: \u0026#34;\u0026lt;YOUR_ACCOUNT_ID_WITHOUT_HYPHENS\u0026gt;\u0026#34; name: \u0026#34;puppet-account\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34; stacks: delete-default-networking-function: name: \u0026#34;delete-default-networking-function\u0026#34; version: \u0026#34;v1\u0026#34; capabilities: - CAPABILITY_NAMED_IAM deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34; vpc: name: \u0026#34;vpc\u0026#34; version: \u0026#34;v1\u0026#34; depends_on: - name: \u0026#34;delete-default-networking\u0026#34; type: \u0026#34;lambda-invocation\u0026#34; affinity: \u0026#34;lambda-invocation\u0026#34; deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34; outputs: ssm: - param_name: \u0026#34;/networking/vpc/account-parameters/${AWS::AccountId}/${AWS::Region}/VPCId\u0026#34; stack_output: VPCId lambda-invocations: delete-default-networking: function_name: DeleteDefaultNetworking qualifier: $LATEST invocation_type: Event depends_on: - name: \u0026#34;delete-default-networking-function\u0026#34; type: \u0026#34;stack\u0026#34; affinity: \u0026#34;stack\u0026#34; invoke_for: tags: - regions: \u0026#34;default_region\u0026#34; tag: \u0026#34;type:prod\u0026#34; assertions: assert-no-default-vpcs: expected: source: manifest config: value: [] actual: source: boto3 config: client: \u0026#39;ec2\u0026#39; call: describe_vpcs arguments: {} use_paginator: true filter: Vpcs[?IsDefault==`true`].State depends_on: - name: \u0026#34;delete-default-networking\u0026#34; type: \u0026#34;lambda-invocation\u0026#34; affinity: \u0026#34;lambda-invocation\u0026#34; assert_for: tags: - regions: regions_enabled tag: type:prod   Committing the manifest file Now that we have updated the manifest file we are ready to commit it.\n Set your Author name Set your Email address Set your Commit message  Using a good / unique commit message will help you understand what is going on later.\n  Click the Commit changes button:    What did we just do? When you added the following:\n vpc: name: \u0026#34;vpc\u0026#34; version: \u0026#34;v1\u0026#34; depends_on: - name: \u0026#34;delete-default-networking\u0026#34; type: \u0026#34;lambda-invocation\u0026#34; affinity: \u0026#34;lambda-invocation\u0026#34; deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;   You told the framework to provision v1 of vpc into the default region of each account that has the tag type:prod\nVerifying the provisioned stack Once you have made your changes the ServiceCatalogPuppet Pipeline should have run. If you were quick in making the change, the pipeline may still be running. If it has not yet started feel free to the hit the Release change button.\nOnce it has completed it should show the stages in green to indicate they have completed successfully:\n  The screenshots may differ slightly as the design of AWS CodePipeline changes. You should see a pipeline where each stage is green.\n Once you have verified the pipeline has run you can go to the AWS CloudFormation console in the default region of the account you specified to view your provisioned stack.\nYou have now successfully provisioned a stack.\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/2021-workshop/10-prerequisites/100-installation/30-service-catalog-puppet.html",
	"title": "Service Catalog Puppet",
	"tags": [],
	"description": "",
	"content": "Select the pre-configured AWS CloudFormation template Service Catalog Puppet can be installed via a pre-created AWS CloudFormation template stored in Amazon S3. There are many configuration options for you to customise your installation. For this workshop we will be using the following quick link which has the settings already preconfigured for you:\n Create stack Check the box labeled \u0026ldquo;I acknowledge that AWS CloudFormation might create IAM resources with custom names.\u0026rdquo; Hit Create stack Wait for the stack status to go to CREATE_COMPLETE    What have we deployed? The following AWS resources have just been deployed into your AWS Account:\nAWS CloudFormation stacks The AWS CodeBuild job created two AWS CloudFormation stacks which in turn deployed the resources listed below\n URL: https://eu-west-1.console.aws.amazon.com/cloudformation/home?region=eu-west-1\n   Puppet AWS CodeCommit repository This respository holds the Service Catalog Puppet manifest YAML file which is used to configure provisioning and sharing.\n URL: https://eu-west-1.console.aws.amazon.com/codesuite/codecommit/repositories?region=eu-west-1\n   Puppet AWS CodePipeline This AWS CodePipeline is triggered by updates to the AWS CodeCommit repository. When run, it will create the Service Catalog portfolios and products defined in the portfolio files.\n URL: https://eu-west-1.console.aws.amazon.com/codesuite/codepipeline/pipelines?region=eu-west-1\n   The pipeline execution will show as failing at this point. This is expected.\n Amazon S3 buckets Three Amazon S3 buckets were created to store artifacts for Service Catalog Puppet.\n URL: https://s3.console.aws.amazon.com/s3/home?region=eu-west-1\n   "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/installation/30-service-catalog-puppet.html",
	"title": "Service Catalog Puppet",
	"tags": [],
	"description": "",
	"content": "Service Catalog Puppet does not depend on AWS Organizations.\nUsing AWS Organizations provides performance and productivity improvements but it is optional. If you would like to use it please follow the \u0026ldquo;Using AWS Organizations\u0026rdquo; step below before you continue with the \u0026ldquo;Installing Puppet\u0026rdquo; step.\n Using AWS Organizations   Installing Puppet   "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/upgrading/service-catalog-puppet.html",
	"title": "Service Catalog Puppet",
	"tags": [],
	"description": "",
	"content": "Before you update This guide applies to all users who installed using the AWS CloudFormation template. If you did not install using the template then you need to follow the install guide. When following the install guide, ensure you provision the initialiser stack into the same region you installed the tools using the CLI method. Installing this way will perform an update and will not break your install.\nIf you have added any regions since your initial install or modified any settings provided by the parameters in the install template ensure you specify the values you want for them as the install process will overwrite any previous settings you configured in your existing install.\nNavigate to CloudFormation  Select the AWS CloudFormation Service.      Select the initialization stack you created when installing Service Catalog Factory. The recommended stack name was puppet-initialization-stack\n  Select \u0026lsquo;Update\u0026rsquo;\n  Select \u0026lsquo;Replace current template\u0026rsquo;\n  For Template source select \u0026lsquo;Amazon S3 URL\u0026rsquo; and paste in the following: https://service-catalog-tools.s3.eu-west-2.amazonaws.com/puppet/latest/servicecatalog-puppet-initialiser.template.yaml\n  Then hit \u0026lsquo;Next\u0026rsquo;\n  Make any changes you want to the parameters you provided and then hit \u0026lsquo;Next\u0026rsquo; again\n  Follow the rest of the steps to update the stack\n  As the stack updates the AWS CodeBuild project named servicecatalog-product-puppet-initialiser is started. Once it is completed the stack update will complete. The project will update your installation to the latest version\n  "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/design-considerations.html",
	"title": "Design considerations",
	"tags": [],
	"description": "",
	"content": "Following the best practices for the Service Catalog Tools will ensure you get the most out of these tools with the most minimal amount of effort.\nThe following articles will help you design your solution. It is recommended you read through each - ideally before you install the tools or start provisioning / sharing products and portfolios.\n Multi Account Strategy   Selecting a factory account   Using parameters in the real world   Portfolio Management   Account Tagging   Using IAM and SCP Effectively   "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/2021-workshop/100-removing-the-default-networking/40-invoking-the-lambda.html",
	"title": "Invoking the Lambda",
	"tags": [],
	"description": "",
	"content": " What are we going to do? We are going to perform the following steps:\n Invoke the lambda  Step by step guide Here are the steps you need to follow to \u0026ldquo;Invoking the Lambda\u0026rdquo;\nInvoke the lambda   Navigate to the ServiceCatalogPuppet CodeCommit repository again\n  Click on manifest.yaml\n  Click Edit\n  Append the following snippet to the end of the main input field:\n   lambda-invocations: delete-default-networking: function_name: DeleteDefaultNetworking qualifier: $LATEST invocation_type: Event depends_on: - name: \u0026#34;delete-default-networking-function\u0026#34; type: \u0026#34;stack\u0026#34; affinity: \u0026#34;stack\u0026#34; invoke_for: tags: - regions: \u0026#34;default_region\u0026#34; tag: \u0026#34;type:prod\u0026#34;    The main input field should look like this:   accounts: - account_id: \u0026#34;${AWS::PuppetAccountId}\u0026#34; name: \u0026#34;puppet-account\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34; stacks: delete-default-networking-function: name: \u0026#34;delete-default-networking-function\u0026#34; version: \u0026#34;v1\u0026#34; capabilities: - CAPABILITY_NAMED_IAM deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34; lambda-invocations: delete-default-networking: function_name: DeleteDefaultNetworking qualifier: $LATEST invocation_type: Event depends_on: - name: \u0026#34;delete-default-networking-function\u0026#34; type: \u0026#34;stack\u0026#34; affinity: \u0026#34;stack\u0026#34; invoke_for: tags: - regions: \u0026#34;default_region\u0026#34; tag: \u0026#34;type:prod\u0026#34;   AWS Committing the manifest file Now that we have written the manifest file we are ready to commit it.\n Set your Author name Set your Email address Set your Commit message  Using a good / unique commit message will help you understand what is going on later.\n  Click the Commit changes button:    What did we just do? The YAML we pasted in the previous step told the framework to perform the following actions:\n Invoke a lambda in the hub account named DeleteDefaultNetworking. It will be invoked each time with parameters account_id and region. It will be invoked one time for each account tagged type:prod using the account_id and default region specified for it.  Verifying the invoke Once the pipeline has completed you can verify the lambda was invoked by verifying there is no default VPC in the default region of your account or you can check the AWS CloudWatch logs for the AWS Lambda function or you could check the execution history for the Lambda function.\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/managing-your-environments.html",
	"title": "Managing your environments",
	"tags": [],
	"description": "",
	"content": "Following the best practices for the Service Catalog Tools will ensure you get the most out of these tools with the most minimal amount of effort.\nThe following articles will help you design your solution. It is recommended you read through each - ideally before you install the tools or start provisioning / sharing products and portfolios.\n GitOps   Don\u0026#39;t Repeat Yourself   Dealing With Variance   Fault Tolerance   Workflow Tracing   Fine grained depends_on   Product SDLC   Multi Organization usage   "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/2021-workshop/100-removing-the-default-networking/50-asserting-the-resources-are-removed.html",
	"title": "Asserting the resources are removed",
	"tags": [],
	"description": "",
	"content": " What are we going to do? We are going to perform the following steps:\n Define an assertion  Step by step guide Here are the steps you need to follow to \u0026ldquo;Asserting the resources are removed\u0026rdquo;\nDefine an assertion   Navigate to the ServiceCatalogPuppet CodeCommit repository again\n  Click on manifest.yaml\n  Click Edit\n  Append the following snippet to the end of the main input field:\n   assertions: assert-no-default-vpcs: expected: source: manifest config: value: [] actual: source: boto3 config: client: \u0026#39;ec2\u0026#39; call: describe_vpcs arguments: {} use_paginator: true filter: Vpcs[?IsDefault==`true`].State depends_on: - name: \u0026#34;delete-default-networking\u0026#34; type: \u0026#34;lambda-invocation\u0026#34; affinity: \u0026#34;lambda-invocation\u0026#34; assert_for: tags: - regions: regions_enabled tag: type:prod    The main input field should look like this (remember to set your account_id):   accounts: - account_id: \u0026#34;${AWS::PuppetAccountId}\u0026#34; name: \u0026#34;puppet-account\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34; stacks: delete-default-networking-function: name: \u0026#34;delete-default-networking-function\u0026#34; version: \u0026#34;v1\u0026#34; capabilities: - CAPABILITY_NAMED_IAM deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34; lambda-invocations: delete-default-networking: function_name: DeleteDefaultNetworking qualifier: $LATEST invocation_type: Event depends_on: - name: \u0026#34;delete-default-networking-function\u0026#34; type: \u0026#34;stack\u0026#34; affinity: \u0026#34;stack\u0026#34; invoke_for: tags: - regions: \u0026#34;default_region\u0026#34; tag: \u0026#34;type:prod\u0026#34; assertions: assert-no-default-vpcs: expected: source: manifest config: value: [] actual: source: boto3 config: client: \u0026#39;ec2\u0026#39; call: describe_vpcs arguments: {} use_paginator: true filter: Vpcs[?IsDefault==`true`].State depends_on: - name: \u0026#34;delete-default-networking\u0026#34; type: \u0026#34;lambda-invocation\u0026#34; affinity: \u0026#34;lambda-invocation\u0026#34; assert_for: tags: - regions: regions_enabled tag: type:prod   AWS Committing the manifest file Now that we have written the manifest file we are ready to commit it.\n Set your Author name Set your Email address Set your Commit message  Using a good / unique commit message will help you understand what is going on later.\n  Click the Commit changes button:    What did we just do? The YAML we pasted in the previous step told the framework to use the ec2 client from boto3 to describe vpcs. We told the framework not to use any arguments when invoking describe vpcs and we told the framework to use a paginator ensuring we find every page of results. We then used a filter to include only VPCs where the IsDefault attribute is set to true\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/installation/50-bootstrapping/50-using-the-cli.html",
	"title": "Command Line Interface",
	"tags": [],
	"description": "",
	"content": "What are we going to do? You will need to bootstrap spoke accounts so you can configure them using the Service Catalog Tools.\nBootstrapping a spoke account will create an AWS CloudFormation stack in it. This stack will contain the Puppet IAM Role (PuppetRole) which is needed by framework to perform actions in the spoke account.\nThe following steps should be executed using the Service Catalog Puppet CLI which is an application built using Python 3.7.\nIf you have not already installed the framework you can do so by following these steps:\nInstalling It is good practice to install Python libraries in isolated environments.\nYou can create the a virtual environment using the following command:\nvirtualenv --python=python3.7 venv source venv/bin/activate Once you have decided where to install the library you can install the package:\npip install aws-service-catalog-puppet This will install the library and all of the dependencies.\nBootstrapping a spoke You should export the credentials for the spoke account or set your profile so that AWS CLI commands will execute as a role in the spoke account.\nThen you can run the following command:\nWithout a permission boundary servicecatalog-puppet bootstrap-spoke \u0026lt;ACCOUNT_ID_OF_YOUR_PUPPET\u0026gt;\nWith a permission boundary servicecatalog-puppet bootstrap-spoke \u0026lt;ACCOUNT_ID_OF_YOUR_PUPPET\u0026gt; --permission-boundary arn:aws:iam::aws:policy/AdministratorAccess\nEnsure you replace \u0026lt;ACCOUNT_ID_OF_YOUR_PUPPET\u0026gt; with the AWS Account id of the AWS Account you will be using as your puppet account.\nBootstrapping a spoke as If you want to assume a role into the spoke from your currently active role you can use the following command.\nWithout a boundary servicecatalog-puppet bootstrap-spoke-as \u0026lt;ACCOUNT_ID_OF_YOUR_PUPPET\u0026gt; \u0026lt;ARN_OF_ASSUMABLE_ROLE_IN_SPOKE\u0026gt;\nWith a boundary servicecatalog-puppet bootstrap-spoke-as \u0026lt;ACCOUNT_ID_OF_YOUR_PUPPET\u0026gt; \u0026lt;ARN_OF_ASSUMABLE_ROLE_IN_SPOKE\u0026gt; --permission-boundary arn:aws:iam::aws:policy/AdministratorAccess\nThis will assume the role \u0026lt;ARN_OF_ASSUMABLE_ROLE_IN_SPOKE\u0026gt; before running boostrap-spoke. This is useful if you do not want to perform the AWS STS assume-role yourself.\nEnsure you replace \u0026lt;ACCOUNT_ID_OF_YOUR_PUPPET\u0026gt; with the account id of the account you will be using as your puppet account.\nYou can use the following AWS CloudFormation template to provision the needed role:\n# Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. # SPDX-License-Identifier: Apache-2.0 AWSTemplateFormatVersion: \u0026#39;2010-09-09\u0026#39; Description: IAM Role needed to use AWS Organizations to assume role into member AWS Accounts. Parameters: ServiceCatalogFactoryAccountId: Description: The account you will be installing AWS Service Catalog Factory into Type: String OrganizationAccountAccessRole: Description: Name of the IAM role used to access cross accounts for AWS Orgs usage Default: OrganizationAccountAccessRole Type: String Resources: RoleForBootstrappingSpokes: Type: AWS::IAM::Role Description: | IAM Role needed by the account vending machine so it can create and move accounts Properties: Path: /servicecatalog-puppet/ Policies: - PolicyName: Organizations PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: - sts:AssumeRole Resource: !Sub \u0026#34;arn:aws:iam::*:role/${OrganizationAccountAccessRole}\u0026#34; AssumeRolePolicyDocument: Version: \u0026#34;2012-10-17\u0026#34; Statement: - Effect: \u0026#34;Allow\u0026#34; Principal: AWS: !Sub \u0026#34;arn:aws:iam::${ServiceCatalogFactoryAccountId}:root\u0026#34; Action: - \u0026#34;sts:AssumeRole\u0026#34; Outputs: RoleForBootstrappingSpokesArn: Description: The ARN for your Assumable role in root account Value: !GetAtt RoleForBootstrappingSpokes.Arn Bootstrapping spokes in OU You should export the credentials for the account that allows you to list accounts in the org and assume an IAM Role in each of the spokes.\nThen you can run the following command:\nWithout a Permission Boundary servicecatalog-puppet bootstrap-spokes-in-ou /dev DevOpsAdminRole\nIn this example /dev is the ou path and DevOpsAdminRole is the name of the assumable role in each spoke account.\nWith a Permission Boundary servicecatalog-puppet bootstrap-spokes-in-ou /dev DevOpsAdminRole --permission-boundary arn:aws:iam::aws:policy/AdministratorAccess\nIf your current role does not allow you to list accounts in the AWS Organization or allow you to assume-role across AWS accounts you can specify an ARN of an IAM role that does. When you do so the framework will assume that IAM Role first and then perform the bootstrapping.\nservicecatalog-puppet bootstrap-spokes-in-ou /dev DevOpsAdminRole arn:aws:iam::0123456789010:role/OrgRoleThatAllowsListAndAssumeRole You can use the following AWS CloudFormation template to provision the needed role:\n# Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. # SPDX-License-Identifier: Apache-2.0 AWSTemplateFormatVersion: \u0026#39;2010-09-09\u0026#39; Description: IAM Role needed to use AWS Organizations to assume role into member AWS Accounts. Parameters: ServiceCatalogFactoryAccountId: Description: The account you will be installing AWS Service Catalog Factory into Type: String OrganizationAccountAccessRole: Description: Name of the IAM role used to access cross accounts for AWS Orgs usage Default: OrganizationAccountAccessRole Type: String Resources: RoleForBootstrappingSpokes: Type: AWS::IAM::Role Description: | IAM Role needed by the account vending machine so it can create and move accounts Properties: Path: /servicecatalog-puppet/ Policies: - PolicyName: Organizations PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: - sts:AssumeRole Resource: !Sub \u0026#34;arn:aws:iam::*:role/${OrganizationAccountAccessRole}\u0026#34; AssumeRolePolicyDocument: Version: \u0026#34;2012-10-17\u0026#34; Statement: - Effect: \u0026#34;Allow\u0026#34; Principal: AWS: !Sub \u0026#34;arn:aws:iam::${ServiceCatalogFactoryAccountId}:root\u0026#34; Action: - \u0026#34;sts:AssumeRole\u0026#34; Outputs: RoleForBootstrappingSpokesArn: Description: The ARN for your Assumable role in root account Value: !GetAtt RoleForBootstrappingSpokes.Arn "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/every-day-use.html",
	"title": "Every day use",
	"tags": [],
	"description": "",
	"content": "Welcome builders From here you can see a list of our how to articles\nYou can use the left and right arrows to navigate\n  Using a mono repo   Creating a product   Deleting a product   Creating a portfolio   Adding a product to a portfolio   Creating a manifest   Adding an account   Provisioning a product   Sharing a portfolio   Imported Portfolios   Spoke Local Portfolios   Using tag options   Using resource update constraints   Adding a region   AWS Organizations Integration   Provisioning CloudFormation   Migrating from launches to stacks   Migrating from stacksets to stacks   Provisioning a Stack   Using stack and launch outputs   Terminating provisioned resources   Invoking a Lambda Function   Starting a CodeBuild project   Using policy simulations   Using assertions   Managing Organizational Units   Applying Service Control Policies   Applying Tag Policies   Applying Cloud Custodian policies   "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/design-considerations/multi-account-strategy.html",
	"title": "Multi Account Strategy",
	"tags": [],
	"description": "",
	"content": " What are we going to do? This article will help you align your multi-account strategy with best practices.\nAWS Organizations AWS Organizations helps you centrally govern your environment as you grow and scale your workloads on AWS. Whether you are a growing startup or a large enterprise, AWS Organizations helps you to centrally manage billing; control access, compliance, and security; and share resources across your AWS accounts.\nUsing AWS Organizations, you can automate account creation, create groups of accounts to reflect your business needs, and apply policies for these groups for governance. You can also simplify billing by setting up a single payment method for all of your AWS accounts. Through integrations with other AWS services, you can use AWS Organizations to define central configurations and resource sharing across accounts in your organization. AWS Organizations is available to all AWS customers at no additional charge.\nGetting started You can read through the following pages to understand best practices:\n Starter framework   "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/installation/50-bootstrapping.html",
	"title": "Onboarding spokes",
	"tags": [],
	"description": "",
	"content": "As part of the installation process of installing puppet into your hub account that account is bootstrapped as a spoke so you can use it via automation.\nIf you want to configure other accounts using the Service Catalog Tools you will need to bootstrap those too.\nYou can use the following options to bootstrap accounts:\n CloudFormation/CodeBuild   Command Line Interface   Restricting Spokes   "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/installation/50-bootstrapping/80-restricting-spokes.html",
	"title": "Restricting Spokes",
	"tags": [],
	"description": "",
	"content": "Restricting spokes The PuppetRole created by the framework has the AdministratorAccess IAM managed policy attached to it. It is reccommended that you can define an IAM Permission Boundary for the PuppetRole for any production applications of this framework.\nThe IAM Permission Boundary you provide should permit the PuppetRole to interact with AWS Service Catalog to accept shares, manage portfolios and to add, provision and terminate products. In addition the IAM Role should allow the use of AWS SNS, AWS EventBridge, AWS OpsCenter if you are making use of those features.\nIn order to use an IAM Permission Boundary you will need to append the following to your commands:\n--permission-boundary arn:aws:iam::aws:policy/AdministratorAccess There will be an example of this for each command in these how tos.\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/every-day-use/using-a-mono-repo.html",
	"title": "Using a mono repo",
	"tags": [],
	"description": "",
	"content": " What are we going to do? This tutorial will walk you through how to use a mono repo\nWe will assume you have:\n installed Service Catalog Factory correctly  We will assume you are comfortable:\n making changes your stacks/apps/workspaces files  Things to note, before we start  This capability was introduced in version 0.71.0 of factory. To use this feature you will need to use this version (or later). You can only use a mono repo for products, stacks, apps or workspaces.  Recommended folder structure If you are using a mono repo we recommend you use the root directory for the type of thing you are going to build, we then recommend a directory for each thing followed by a directory for each version of that thing:\n ➜ mono-repo tree . . ├── apps │ └── k8s │ ├── v1 │ └── v2 ├── stacks │ ├── subnet │ │ ├── v1 │ │ └── v2 │ └── vpc │ ├── v1 │ ├── v2 │ └── v3 └── workspaces ├── guard-duty-enabler-hub │ ├── v1 │ └── v2 └── guard-duty-enabler-spoke ├── v1 ├── v2 └── v3 20 directories, 0 files  Enabling the source directory To tell the tools you want to use a source directory you can provide a path attribute:\n Schema: factory-2019-04-01 Stacks: - Name: \u0026#34;vpc\u0026#34; Versions: - Name: \u0026#34;v3\u0026#34; Source: Provider: \u0026#34;CodeCommit\u0026#34; Configuration: RepositoryName: \u0026#34;ssm-parameter-stack\u0026#34; BranchName: \u0026#34;main\u0026#34; Path: \u0026#34;vpc/v3\u0026#34;   The path must exist in the Source dictionary and needs to be relative to the root of the git repo.\nLimitations You must be using at least version 0.71.0 to use this and you can only use this for products, stacks, apps and workspaces. At the moment of writing this, a change to the source will trigger all pipelines that listen to it. A future version will resolve this.\nIf you are providing your own build, testing or package stages you will need to manage the path to the your source in the buildspec you write.\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/40-reinvent2019/100-task-1.html",
	"title": "Budget &amp; Cost governance",
	"tags": [],
	"description": "",
	"content": " The ask Cloud usage within Example Corp has picked up significantly and a number of teams are now using EC2 instances in innovative ways. The customer has noticed that teams are often not sure which EC2 instance types to use for their applications and this is leading to underutilized EC2 instances that are run on-demand. To bring down costs, the customer has purchased a set of EC2 reserved instances, based on common workload profiles, and we need to ensure the teams are using them for long running applications.\nTo help the customer, we will design and then deploy a control that gives them visibility into which EC2 instance types are being used within an AWS account.\nThe plan We are going to create and deploy a governance control using an AWS Config managed rule to ensure the right instance types are being used.\nYou can follow these steps to do this:\n Create the control   Provision the control   Future work will involve mandating that teams use only approved instance types. To start, we will gather data via AWS Config.\nIf you need help at any time please raise your hand.\n "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/40-reinvent2019/100-task-1/100-create-the-control.html",
	"title": "Create the control",
	"tags": [],
	"description": "",
	"content": " What are we going to do? We are going to perform the following steps:\n define a product with a version and a portfolio in a hub account add the source code for the product provision that product into a spoke account  The hub AWS Account is the source of truth for our AWS Service Catalog products. Spoke AWS accounts are consumers of these products, you can think of them as accounts that need governance controls applied. For this workshop, we are using the same account as both the hub and spoke for simplicity; in a multi-account setup, these could be separate AWS Accounts and Regions.\nStep by step guide Here are the steps you need to follow to \u0026ldquo;Create the control\u0026rdquo;\nDefine a product with a version and a portfolio   Navigate to the ServiceCatalogFactory CodeCommit repository\n  Scroll down to the bottom of the page and hit the Create file button\n      Copy the following snippet into the main input field:\n Schema: factory-2019-04-01 Products: - Name: \u0026#34;aws-config-desired-instance-types\u0026#34; Owner: \u0026#34;budget-and-cost-governance@example.com\u0026#34; Description: \u0026#34;Enables AWS Config rule - desired-instance-type with our RIs\u0026#34; Distributor: \u0026#34;cloud-engineering\u0026#34; SupportDescription: \u0026#34;Speak to budget-and-cost-governance@example.com about exceptions and speak to cloud-engineering@example.com about implementation issues\u0026#34; SupportEmail: \u0026#34;cloud-engineering@example.com\u0026#34; SupportUrl: \u0026#34;https://wiki.example.com/cloud-engineering/budget-and-cost-governance/aws-config-desired-instance-types\u0026#34; Tags: - Key: \u0026#34;type\u0026#34; Value: \u0026#34;governance\u0026#34; - Key: \u0026#34;creator\u0026#34; Value: \u0026#34;cloud-engineering\u0026#34; - Key: \u0026#34;cost-center\u0026#34; Value: \u0026#34;governance\u0026#34; Versions: - Name: \u0026#34;v1\u0026#34; Description: \u0026#34;v1 of aws-config-desired-instance-types\u0026#34; Active: True Source: Provider: \u0026#34;CodeCommit\u0026#34; Configuration: RepositoryName: \u0026#34;aws-config-desired-instance-types\u0026#34; BranchName: \u0026#34;main\u0026#34; Portfolios: - \u0026#34;cloud-engineering-governance\u0026#34; Portfolios: - DisplayName: \u0026#34;cloud-engineering-governance\u0026#34; Description: \u0026#34;Portfolio containing the products needed to govern AWS accounts\u0026#34; ProviderName: \u0026#34;cloud-engineering\u0026#34; Associations: - \u0026#34;arn:aws:iam::${AWS::AccountId}:role/TeamRole\u0026#34; Tags: - Key: \u0026#34;type\u0026#34; Value: \u0026#34;governance\u0026#34; - Key: \u0026#34;creator\u0026#34; Value: \u0026#34;cloud-engineering\u0026#34; - Key: \u0026#34;cost-center\u0026#34; Value: \u0026#34;governance\u0026#34;     Set the File name to portfolios/reinvent.yaml\n  Set your Author name\n  Set your Email address\n  Set your Commit message\n  Using a good / unique commit message will help you understand what is going on later.\n  Click the Commit changes button:    What did we just do? The YAML file we created in the CodeCommit repository told the framework to perform several actions:\n create a product named aws-config-desired-instance-types add a v1 of our product create a portfolio named cloud-engineering-governance add the product: aws-config-desired-instance-types to the portfolio: cloud-engineering-governance  Verify the change worked Once you have made your changes the ServiceCatalogFactory Pipeline should have run. If you were very quick in making the change, the pipeline may still be running. If it has not yet started feel free to the hit the Release change button.\nOnce it has completed it should show the Source and Build stages in green to indicate they have completed successfully:\n  If this is failing please raise your hand for some assistance\n Add the source code for our product When you configured your product version, you specified the following version:\n Versions: - Name: \u0026#34;v1\u0026#34; Description: \u0026#34;v1 of aws-config-desired-instance-types\u0026#34; Active: True Source: Provider: \u0026#34;CodeCommit\u0026#34; Configuration: RepositoryName: \u0026#34;aws-config-desired-instance-types\u0026#34; BranchName: \u0026#34;main\u0026#34;   This tells the framework the source code for the product comes from the main branch of a CodeCommit repository of the name aws-config-desired-instance-types.\nWe now need to create the CodeCommit repository and add the AWS CloudFormation template we are going to use for our product.\n  Navigate to AWS CodeCommit\n  Click Create repository\n     Input the name aws-config-desired-instance-types     Click Create     Scroll down to the bottom of the page and hit the Create file button     Copy the following snippet into the main input field:   AWSTemplateFormatVersion: \u0026#34;2010-09-09\u0026#34; Description: \u0026#34;Create an AWS Config rule ensuring the given instance types are the only instance types used\u0026#34; Parameters: InstanceType: Type: String Description: \u0026#34;Comma separated list of EC2 instance types (for example, \u0026#39;t2.small, m4.large\u0026#39;).\u0026#34; Default: \u0026#34;t2.micro, t2.small\u0026#34; Resources: AWSConfigRule: Type: AWS::Config::ConfigRule Properties: ConfigRuleName: \u0026#34;desired-instance-type\u0026#34; Description: \u0026#34;Checks whether your EC2 instances are of the specified instance types.\u0026#34; InputParameters: instanceType: !Ref InstanceType Scope: ComplianceResourceTypes: - \u0026#34;AWS::EC2::Instance\u0026#34; Source: Owner: AWS SourceIdentifier: DESIRED_INSTANCE_TYPE     Set the File name to product.template.yaml\n  Set your Author name\n  Set your Email address\n  Set your Commit message\n  Using a good / unique commit message will help you understand what is going on later.\n Creating that file should trigger your aws-config-desired-instance-types-v1-pipeline.\nOnce the pipeline has completed it should show the Source, Tests, Package and Deploy stages in green to indicate they have completed successfully:\n  You should see your commit message on this screen, it will help you know which version of ServiceCatalogFactory repository the pipeline is processing.\n If this is failing please raise your hand for some assistance\n Once you have verified the pipeline has run you can go to Service Catalog products to view your newly created version.\nYou should see the product you created listed:\n  Click on the product and verify v1 is there\n  If you cannot see your version please raise your hand for some assistance\n You have now successfully created a version for your product!\nVerify the product was added to the portfolio Now that you have verified the pipeline has run you can go to Service Catalog portfolios to view your portfolio.\n Click on reinvent-cloud-engineering-governance      Click on the product aws-config-desired-instance-types\n  Click on the version v1\n    "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/40-reinvent2019/150-task-2/100-create-the-control.html",
	"title": "Create the control",
	"tags": [],
	"description": "",
	"content": " What are we going to do? We are going to perform the following steps:\n define another product with a version and add it to the existing cloud-engineering-governance portfolio add the source code for our product provision that product into a spoke account  Step by step guide Here are the steps you need to follow to \u0026ldquo;Create the control\u0026rdquo;\nDefine a product with a version and a portfolio   Navigate to the ServiceCatalogFactory CodeCommit repository again\n  Click on portfolios\n     Click on reinvent.yaml     Click Edit     We will need to insert the following to the products section:   - Name: \u0026#34;aws-config-rds-storage-encrypted\u0026#34; Owner: \u0026#34;data-governance@example.com\u0026#34; Description: \u0026#34;Enables AWS Config rule - aws-config-rds-storage-encrypted\u0026#34; Distributor: \u0026#34;cloud-engineering\u0026#34; SupportDescription: \u0026#34;Speak to data-governance@example.com about exceptions and speak to cloud-engineering@example.com about implementation issues\u0026#34; SupportEmail: \u0026#34;cloud-engineering@example.com\u0026#34; SupportUrl: \u0026#34;https://wiki.example.com/cloud-engineering/data-governance/aws-config-rds-storage-encrypted\u0026#34; Tags: - Key: \u0026#34;type\u0026#34; Value: \u0026#34;governance\u0026#34; - Key: \u0026#34;creator\u0026#34; Value: \u0026#34;cloud-engineering\u0026#34; - Key: \u0026#34;cost-center\u0026#34; Value: \u0026#34;governance\u0026#34; Versions: - Name: \u0026#34;v1\u0026#34; Description: \u0026#34;v1 of aws-config-rds-storage-encrypted\u0026#34; Active: True Source: Provider: \u0026#34;CodeCommit\u0026#34; Configuration: RepositoryName: \u0026#34;aws-config-rds-storage-encrypted\u0026#34; BranchName: \u0026#34;main\u0026#34; Portfolios: - \u0026#34;cloud-engineering-governance\u0026#34;    Once completed it should like look this:   Schema: factory-2019-04-01 Products: - Name: \u0026#34;aws-config-desired-instance-types\u0026#34; Owner: \u0026#34;budget-and-cost-governance@example.com\u0026#34; Description: \u0026#34;Enables AWS Config rule - desired-instance-type with our RIs\u0026#34; Distributor: \u0026#34;cloud-engineering\u0026#34; SupportDescription: \u0026#34;Speak to budget-and-cost-governance@example.com about exceptions and speak to cloud-engineering@example.com about implementation issues\u0026#34; SupportEmail: \u0026#34;cloud-engineering@example.com\u0026#34; SupportUrl: \u0026#34;https://wiki.example.com/cloud-engineering/budget-and-cost-governance/aws-config-desired-instance-types\u0026#34; Tags: - Key: \u0026#34;type\u0026#34; Value: \u0026#34;governance\u0026#34; - Key: \u0026#34;creator\u0026#34; Value: \u0026#34;cloud-engineering\u0026#34; - Key: \u0026#34;cost-center\u0026#34; Value: \u0026#34;governance\u0026#34; Versions: - Name: \u0026#34;v1\u0026#34; Description: \u0026#34;v1 of aws-config-desired-instance-types\u0026#34; Active: True Source: Provider: \u0026#34;CodeCommit\u0026#34; Configuration: RepositoryName: \u0026#34;aws-config-desired-instance-types\u0026#34; BranchName: \u0026#34;main\u0026#34; Portfolios: - \u0026#34;cloud-engineering-governance\u0026#34; - Name: \u0026#34;aws-config-rds-storage-encrypted\u0026#34; Owner: \u0026#34;data-governance@example.com\u0026#34; Description: \u0026#34;Enables AWS Config rule - aws-config-rds-storage-encrypted\u0026#34; Distributor: \u0026#34;cloud-engineering\u0026#34; SupportDescription: \u0026#34;Speak to data-governance@example.com about exceptions and speak to cloud-engineering@example.com about implementation issues\u0026#34; SupportEmail: \u0026#34;cloud-engineering@example.com\u0026#34; SupportUrl: \u0026#34;https://wiki.example.com/cloud-engineering/data-governance/aws-config-rds-storage-encrypted\u0026#34; Tags: - Key: \u0026#34;type\u0026#34; Value: \u0026#34;governance\u0026#34; - Key: \u0026#34;creator\u0026#34; Value: \u0026#34;cloud-engineering\u0026#34; - Key: \u0026#34;cost-center\u0026#34; Value: \u0026#34;governance\u0026#34; Versions: - Name: \u0026#34;v1\u0026#34; Description: \u0026#34;v1 of aws-config-rds-storage-encrypted\u0026#34; Active: True Source: Provider: \u0026#34;CodeCommit\u0026#34; Configuration: RepositoryName: \u0026#34;aws-config-rds-storage-encrypted\u0026#34; BranchName: \u0026#34;main\u0026#34; Portfolios: - \u0026#34;cloud-engineering-governance\u0026#34; Portfolios: - DisplayName: \u0026#34;cloud-engineering-governance\u0026#34; Description: \u0026#34;Portfolio containing the products needed to govern AWS accounts\u0026#34; ProviderName: \u0026#34;cloud-engineering\u0026#34; Associations: - \u0026#34;arn:aws:iam::${AWS::AccountId}:role/TeamRole\u0026#34; Tags: - Key: \u0026#34;type\u0026#34; Value: \u0026#34;governance\u0026#34; - Key: \u0026#34;creator\u0026#34; Value: \u0026#34;cloud-engineering\u0026#34; - Key: \u0026#34;cost-center\u0026#34; Value: \u0026#34;governance\u0026#34;    Set your Author name Set your Email address Set your Commit message  Using a good / unique commit message will help you understand what is going on later.\n  Click the Commit changes button:    What did we just do? The YAML we pasted in the previous step told the framework to perform several actions:\n create a product named aws-config-rds-storage-encrypted add a v1 of our product add the product: aws-config-rds-storage-encrypted to the portfolio: cloud-engineering-governance  Verify that the change worked Once you have made your changes the ServiceCatalogFactory Pipeline should have run. If you were very quick, the pipeline may still be running. If it has not yet started feel free to the hit the Release change button.\nOnce it has completed it should show the Source and Build stages in green to indicate they have completed successfully:\n  If this is failing please raise your hand for some assistance\n Add the source code for our product When you configured your product version, you specified the following version:\n Versions: - Name: \u0026#34;v1\u0026#34; Description: \u0026#34;v1 of aws-config-rds-storage-encrypted\u0026#34; Active: True Source: Provider: \u0026#34;CodeCommit\u0026#34; Configuration: RepositoryName: \u0026#34;aws-config-rds-storage-encrypted\u0026#34; BranchName: \u0026#34;main\u0026#34;   This tells the framework the source code for the product comes from the main branch of a CodeCommit repository of the name aws-config-rds-storage-encrypted.\nWe now need to create the CodeCommit repository and add the CloudFormation template we are going to use for our product.\n  Navigate to AWS CodeCommit\n  Click Create repository\n     Input the name aws-config-rds-storage-encrypted     Click Create     Scroll down to the bottom of the page and hit the Create file button     Copy the following snippet into the main input field:   AWSTemplateFormatVersion: \u0026#39;2010-09-09\u0026#39; Description: \u0026#34;Create an AWS Config rule ensuring RDS instances use encrypted storage\u0026#34; Resources: AWSConfigRule: Type: AWS::Config::ConfigRule Properties: ConfigRuleName: \u0026#34;rds-storage-encrypted\u0026#34; Description: \u0026#34;Checks whether storage encryption is enabled for your RDS DB instances.\u0026#34; Scope: ComplianceResourceTypes: - \u0026#34;AWS::RDS::DBInstance\u0026#34; Source: Owner: AWS SourceIdentifier: RDS_STORAGE_ENCRYPTED     Set the File name to product.template.yaml\n  Set your Author name\n  Set your Email address\n  Set your Commit message\n  Using a good / unique commit message will help you understand what is going on later.\n Creating that file should trigger your aws-config-rds-storage-encrypted-v1-pipeline.\nOnce the pipeline has completed it should show the Source, Package, Package and Deploy stages in green to indicate they have completed successfully:\n  You should see your commit message on this screen, it will help you know which version of ServiceCatalogFactory repository the pipeline is processing.\n If this is failing please raise your hand for some assistance\n Once you have verified the pipeline has run you can go to Service Catalog products to view your newly created version.\nYou should see the product you created listed:\n  Click on the product and verify v1 is there\n  If you cannot see your version please raise your hand for some assistance\n You have now successfully created a version for your product!\nVerify the product was added to the portfolio Now that you have verified the pipeline has run you can go to Service Catalog portfolios to view your portfolio.\n Click on reinvent-cloud-engineering-governance      Click on the product aws-config-rds-storage-encrypted\n  Click on the version v1\n    "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/every-day-use/100-creating-a-product.html",
	"title": "Creating a product",
	"tags": [],
	"description": "",
	"content": " This tutorial will walk you through \u0026ldquo;Creating a product\u0026rdquo;\nWe will assume you have:\n installed Service Catalog Factory correctly  In the tutorial you will:\n Define the product   Add the source code   Creating CodeStar pipelines   Creating S3 pipelines   During this process you will check your progress by verifying what the framework is doing.\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/every-day-use/100-creating-a-product/100-define-the-product.html",
	"title": "Define the product",
	"tags": [],
	"description": "",
	"content": " What are we going to do? We are going to perform the following steps:\n create a portfolio file define a product define a version for our product commit our portfolio file verify the framework has create an AWS CodePipeline for our product version  Step by step guide Here are the steps you need to follow to \u0026ldquo;Define the product\u0026rdquo;\nCreate the portfolio file We need to tell the framework that a product exists. We do that by creating a portfolio file and by describing the products details there.\nHere is how we do this:\n Navigate to the ServiceCatalogFactory CodeCommit repository Scroll down to the bottom of the page and hit the Create file button      Copy the following snippet into the main input field:\n Schema: factory-2019-04-01 Products: - Name: \u0026#34;aws-config-enable-config\u0026#34; Owner: \u0026#34;data-governance@example.com\u0026#34; Description: \u0026#34;Enables AWS Config\u0026#34; Distributor: \u0026#34;cloud-engineering\u0026#34; SupportDescription: \u0026#34;Speak to data-governance@example.com about exceptions and speak to cloud-engineering@example.com about implementation issues\u0026#34; SupportEmail: \u0026#34;cloud-engineering@example.com\u0026#34; SupportUrl: \u0026#34;https://wiki.example.com/cloud-engineering/governance/aws-config-enable-config\u0026#34; Tags: - Key: \u0026#34;type\u0026#34; Value: \u0026#34;governance\u0026#34; - Key: \u0026#34;creator\u0026#34; Value: \u0026#34;cloud-engineering\u0026#34; - Key: \u0026#34;cost-center\u0026#34; Value: \u0026#34;cloud-engineering\u0026#34;     Set the File name to portfolios/reinvent.yaml\n  Set your Author name\n  Set your Email address\n  Set your Commit message\n  Using a good / unique commit message will help you understand what is going on later.\n  Click the Commit changes button:    We have just told the framework there is a product named aws-config-enable-config. This product has no versions and so it will not appear in AWS Service Catalog yet.\nCreate the version We now need to tell the framework we want to create a new version of our product.\n  Navigate to the ServiceCatalogFactory CodeCommit repository again\n  Click on portfolios\n     Click on reinvent.yaml     Click Edit      Add the following to the end of the file (be careful with your indentation):\n Versions: - Name: \u0026#34;v1\u0026#34; Description: \u0026#34;v1 of aws-config-enable-config\u0026#34; Active: True Source: Provider: \u0026#34;CodeCommit\u0026#34; Configuration: RepositoryName: \u0026#34;aws-config-enable-config\u0026#34; BranchName: \u0026#34;main\u0026#34;     Verify the contents of your file matches this:\n Schema: factory-2019-04-01 Products: - Name: \u0026#34;aws-config-enable-config\u0026#34; Owner: \u0026#34;data-governance@example.com\u0026#34; Description: \u0026#34;Enables AWS Config\u0026#34; Distributor: \u0026#34;cloud-engineering\u0026#34; SupportDescription: \u0026#34;Speak to data-governance@example.com about exceptions and speak to cloud-engineering@example.com about implementation issues\u0026#34; SupportEmail: \u0026#34;cloud-engineering@example.com\u0026#34; SupportUrl: \u0026#34;https://wiki.example.com/cloud-engineering/governance/aws-config-enable-config\u0026#34; Tags: - Key: \u0026#34;type\u0026#34; Value: \u0026#34;governance\u0026#34; - Key: \u0026#34;creator\u0026#34; Value: \u0026#34;cloud-engineering\u0026#34; - Key: \u0026#34;cost-center\u0026#34; Value: \u0026#34;cloud-engineering\u0026#34; Versions: - Name: \u0026#34;v1\u0026#34; Description: \u0026#34;v1 of aws-config-enable-config\u0026#34; Active: True Source: Provider: \u0026#34;CodeCommit\u0026#34; Configuration: RepositoryName: \u0026#34;aws-config-enable-config\u0026#34; BranchName: \u0026#34;main\u0026#34;     Once you have updated the file fill in the fields for Author name, Email address, Commit message and hit Commit changes\n  Using a good / unique commit message will help you understand what is going on later.\n Verify the version was created Once you have made your changes the ServiceCatalogFactory Pipeline should have run or if you were quick may still be running. If it has not yet started feel free to the hit the Release change button.\nOnce it has completed it should show the Source and Build stages in green to indicate they have completed successfully:\n  You should see your commit message on this screen, it will help you know which version of ServiceCatalogFactory repo the pipeline is processing.\n Now that your ServiceCatalogFactory pipeline has completed you can view the newly created pipeline: aws-config-enable-config-v1-pipeline\nYou can safely ignore the aws-config-enable-config-v1-pipeline has failed warning. For the pipeline to succeed, we need to add the source code for it to work which we will do in the next step.\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/every-day-use/110-deleting-a-product/100-disabling-the-product-versions.html",
	"title": "Disabling the product versions",
	"tags": [],
	"description": "",
	"content": " What are we going to do? We are going to perform the following steps:\n disable a product version  Step by step guide Here are the steps you need to follow to \u0026ldquo;Disabling the product versions\u0026rdquo;\nDisable the product version When working with other teams it is recommended that you disable a product version before you delete it. This gives teams time to react before deletion of the product. If they are dependent on the product version still they can reach out to you to inform you.\nTo disable a version you need to set its Active attribute to False. You do this by editing its definition in the portfolio yaml.\n  Navigate to the ServiceCatalogFactory CodeCommit repository\n  Click on portfolios\n      Click on the portfolio yaml containing your product\n  Click Edit\n  Add or set the attribute Active for the version you want to disable to False:\n Schema: factory-2019-04-01 Products: - Name: account-vending-machine Owner: central-it@customer.com Description: The iam roles needed for you to do your jobs Distributor: central-it-team SupportDescription: Contact us on Chime for help #central-it-team SupportEmail: central-it-team@customer.com SupportUrl: https://wiki.customer.com/central-it-team/self-service/account-iam  Tags: - Key: product-type Value: iam Versions: - Name: v1 Description: The iam roles needed for you to do your jobs Active: False  Source: Provider: CodeCommit Configuration: RepositoryName: account-vending-machine BranchName: v1     Set your Author name\n  Set your Email address\n  Set your Commit message\n  Click the Commit changes button:\n    When the framework runs the product will be disabled. This change will only affect the version of the product in your factory account. If you are using the imported product in your spoke accounts it will have affect there otherwise you will need to run service-catalog-puppet to cascade the change.\nYou can verify this by navigating to Service Catalog and checking your disabled product. It should look like:\n  "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/2021-workshop/10-prerequisites/100-installation.html",
	"title": "Installation",
	"tags": [],
	"description": "",
	"content": " What are we going to do? We need to install the Service Catalog tools to get started. The Service Catalog tools comprises of two open source solutions:\n aws-service-catalog-factory - this helps us build pipelines for a single account aws-service-catalog-puppet - this helps us to share, provision and configure resources across a multi account  For the workshop we recommend you install the tools in the same region as each other and we recommend you use eu-west-1.\nHow are we going to do it?  Service Catalog Factory   Service Catalog Puppet   "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/2021-workshop/100-removing-the-default-networking.html",
	"title": "Removing the default networking",
	"tags": [],
	"description": "",
	"content": " What are we going to do? When you create a new AWS account there are some networking resources already provisioned in the account. If you are planning on using an AWS Transit Gateway or connecting the AWS account to your local network then these resources may not be required.\nWe are going to create an AWS Lambda function that assumes role into an AWS account to remove the networking resources. We will use a Service Catalog tools stack for this. To do this we will create a pipeline that will create the stack and then we will provision the stack. Once the stack is provisioned we will invoke the lambda function so that the resources are removed. Finally, we will verify the resources are no longer present using an assertion.\nHow are we going to do it? This task is broken down into the following steps:\n Creating the Lambda   Deploying the Lambda   Invoking the Lambda   Asserting the resources are removed   "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/40-reinvent2019/30-prerequisites/100-running-yourself.html",
	"title": "Running yourself",
	"tags": [],
	"description": "",
	"content": " What are we going to do? We are going to perform the following steps:\n Enable AWS Config Install the tools Create an IAM Role  In order to run the workshop in your own account you will need to enable AWS Config and create an IAM Role named TeamRole which you must then assume in order to complete the activities.\nStep by step guide Here are the steps you need to follow to \u0026ldquo;Running yourself\u0026rdquo;\nEnable AWS Config   You should save the following into a file named enable-aws-config.template.yaml:\n AWSTemplateFormatVersion: 2010-09-09 Description: | This template creates a Config Recorder and an Amazon S3 bucket where logs are published. Resources: ConfigRole: Type: \u0026#39;AWS::IAM::Role\u0026#39; Description: The IAM role used to configure AWS Config Properties: AssumeRolePolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Principal: Service: - config.amazonaws.com Action: - \u0026#39;sts:AssumeRole\u0026#39; ManagedPolicyArns: - arn:aws:iam::aws:policy/service-role/AWSConfigRole Policies: - PolicyName: root PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: \u0026#39;s3:GetBucketAcl\u0026#39; Resource: !Sub arn:aws:s3:::${S3ConfigBucket} - Effect: Allow Action: \u0026#39;s3:PutObject\u0026#39; Resource: !Sub arn:aws:s3:::${S3ConfigBucket}/AWSLogs/${AWS::AccountId}/${AWS::Region} Condition: StringEquals: \u0026#39;s3:x-amz-acl\u0026#39;: bucket-owner-full-control - Effect: Allow Action: \u0026#39;config:Put*\u0026#39; Resource: \u0026#39;*\u0026#39; ConfigRecorder: Type: \u0026#39;AWS::Config::ConfigurationRecorder\u0026#39; DependsOn: ConfigRole Properties: Name: default RoleARN: !GetAtt ConfigRole.Arn DeliveryChannel: Type: \u0026#39;AWS::Config::DeliveryChannel\u0026#39; Properties: ConfigSnapshotDeliveryProperties: DeliveryFrequency: Six_Hours S3BucketName: !Ref S3ConfigBucket S3ConfigBucket: DeletionPolicy: Retain Description: S3 bucket with AES256 Encryption set Type: AWS::S3::Bucket Properties: BucketName: !Sub config-bucket-${AWS::AccountId} PublicAccessBlockConfiguration: BlockPublicAcls: True BlockPublicPolicy: True IgnorePublicAcls: True RestrictPublicBuckets: True BucketEncryption: ServerSideEncryptionConfiguration: - ServerSideEncryptionByDefault: SSEAlgorithm: AES256 S3ConfigBucketPolicy: Type: AWS::S3::BucketPolicy Description: S3 bucket policy Properties: Bucket: !Ref S3ConfigBucket PolicyDocument: Version: 2012-10-17 Statement: - Sid: AWSBucketPermissionsCheck Effect: Allow Principal: Service: - config.amazonaws.com Action: s3:GetBucketAcl Resource: - !Sub \u0026#34;arn:aws:s3:::${S3ConfigBucket}\u0026#34; - Sid: AWSBucketDelivery Effect: Allow Principal: Service: - config.amazonaws.com Action: s3:PutObject Resource: !Sub \u0026#34;arn:aws:s3:::${S3ConfigBucket}/AWSLogs/*/*\u0026#34; Outputs: ConfigRoleArn: Value: !GetAtt ConfigRole.Arn S3ConfigBucketArn: Value: !GetAtt S3ConfigBucket.Arn     You should then use AWS CloudFormation to create a stack named enable-aws-config using the template you just created\n  What did we just do?  You created an AWS CloudFormation template You used the template to create a stack The stack you created enabled AWS Config  Install the tools  You should save the following into a file named install-the-tools.template.yaml:   AWSTemplateFormatVersion: 2010-09-09 Description: \u0026#34;This template uses Stack Resource to install Service Catalog Factory and Puppet\u0026#34; Resources: FactoryInstall: Type: AWS::CloudFormation::Stack Properties: Parameters: EnabledRegions: eu-west-1 TemplateURL: \u0026#34;https://service-catalog-tools.s3.eu-west-2.amazonaws.com/factory/latest/servicecatalog-factory-initialiser.template.yaml\u0026#34; TimeoutInMinutes: 30 PuppetInstall: Type: AWS::CloudFormation::Stack Properties: Parameters: EnabledRegions: eu-west-1 ShouldCollectCloudformationEvents: False ShouldForwardEventsToEventbridge: False ShouldForwardFailuresToOpscenter: False TemplateURL: \u0026#34;https://service-catalog-tools.s3.eu-west-2.amazonaws.com/puppet/latest/servicecatalog-puppet-initialiser.template.yaml\u0026#34; TimeoutInMinutes: 30    You should then use AWS CloudFormation to create a stack named install-the-tools.template using the template you just created  What did we just do?  You created an AWS CloudFormation template You used the template to create a stack named install-the-tools The stack you created installed the service catalog tools with the correct configuration for the workshop to run  Create an IAM Role  You should save the following into a file named create-iam-role.template.yaml:   AWSTemplateFormatVersion: \u0026#34;2010-09-09\u0026#34; Description: | Template used to create an IAM role to be used for the service catalog tools workshop Resources: TeamRole: Type: AWS::IAM::Role Properties: RoleName: TeamRole AssumeRolePolicyDocument: Version: \u0026#34;2012-10-17\u0026#34; Statement: - Effect: \u0026#34;Allow\u0026#34; Principal: AWS: !Sub \u0026#34;arn:aws:iam::${AWS::AccountId}:root\u0026#34; Action: - \u0026#34;sts:AssumeRole\u0026#34; ManagedPolicyArns: - arn:aws:iam::aws:policy/AdministratorAccess     You should then use AWS CloudFormation to create a stack named create-iam-role.template using the template you just created\n  You should then assume that role in order to start the workshop\n  What did we just do?  You created an AWS CloudFormation template You used the template to create a stack named create-iam-role You then assumed the role so you have the correct permissions needed and have the correct role name  "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/installation/100-securing-the-installation.html",
	"title": "Securing the installation",
	"tags": [],
	"description": "",
	"content": "Service Catalog Puppet introduces two high-privileged IAM roles (PuppetDeployInSpokeRole and PuppetRole). To ensure that your Service Catalog Puppet installation is secure, it is needed to take additional precautions securing these IAM roles.\nThe related IAM Roles are deployed in both, hub and spoke accounts and have the following trust relationships:\nPuppetDeployInSpokeRole { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;Service\u0026quot;: \u0026quot;codebuild.amazonaws.com\u0026quot; }, \u0026quot;Action\u0026quot;: \u0026quot;sts:AssumeRole\u0026quot; } ] } PuppetRole { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;AWS\u0026quot;: \u0026quot;arn:aws:iam::\u0026lt;hub-account-id\u0026gt;:root\u0026quot; }, \u0026quot;Action\u0026quot;: \u0026quot;sts:AssumeRole\u0026quot; }, { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;AWS\u0026quot;: \u0026quot;arn:aws:iam::\u0026lt;spoke-account-id\u0026gt;:root\u0026quot; }, \u0026quot;Action\u0026quot;: \u0026quot;sts:AssumeRole\u0026quot; } ] } Risk The risk with these IAM role is a potential privilege escalation by either directly assuming the PuppetRole or by launching a CodeBuild instance and assigning the PuppetDeployInSpokeRole to this instance. This would allow any IAM user/role with appropriate permissions to execute commands with the privileges of the Puppet roles.\nMitigating risk via SCP The following Service Control Policy denies the sts:AssumeRole and iam:* actions on all IAM roles which are created with the path servicecatalog-puppet. This path is the default setting. If you defined another path for these IAM roles, you will need to adapt the SCP accordingly.\nApply this SCP to all spoke accounts and the hub account of Service Catalog Puppet after deploying the Puppet resources in these accounts. See the AWS documentation  on how to create and apply SCPs.\n{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Sid\u0026quot;: \u0026quot;PreventPrivilegeEscalationInServiceCatalogPuppet\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Deny\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;sts:AssumeRole\u0026quot;, \u0026quot;iam:*\u0026quot; ], \u0026quot;Resource\u0026quot;: [ \u0026quot;arn:aws:iam::*:role/servicecatalog-puppet/*\u0026quot; ], \u0026quot;Condition\u0026quot;: { \u0026quot;ArnNotLike\u0026quot;: { \u0026quot;aws:PrincipalARN\u0026quot;: [ \u0026quot;arn:aws:iam::*:role/servicecatalog-puppet/*\u0026quot; ] } } } ] } "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/design-considerations/selecting-a-factory-account.html",
	"title": "Selecting a factory account",
	"tags": [],
	"description": "",
	"content": " What are we going to do? This article will help you choose which AWS Account is most suitable to use for the Service Catalog Tools\nRecommended background reading? It is recommended that you have read the following:\n Multi Account Strategy  Selecting how many factory accounts to have Currently, you can only have one factory per account but you can create multiple accounts each with their own factory.\nIf multiple teams want to make use of Service Catalog Factory the recommendation is that each team have their own instance. The teams are then independent and can operate without impacting each other. There is also a separation of concerns if there is a security factory account and a networking factory account. This reduces the blast radius should there be an incident and it enables easier billing calculations.\nIf your organization has a central cloud engineering team who work to deliver the requirements of these other teams it may be easier to manage all of the provisioning from a single factory account.\nSelecting a factory account In order to select a factory account we need to consider how you are going to be using the framework.\nA common use case is where teams use the framework to build and provision security controls into accounts that are operating their customer facing applications.\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/managing-your-environments/gitops.html",
	"title": "GitOps",
	"tags": [],
	"description": "",
	"content": " GitOps We recommend you start your management journey with the Service Catalog Tools following a GitOps methodology. As your usage increases you may find you want to move to continuous delivery or roll out your own custom process.\nWhilst following GitOps and managing multiple environments you may encounter some duplication between the configurations defining each of your environments. The following article explains some of the capabilities that have been introduced to help resolve these problems.\nWhen following GitOps to manage multiple organizations or installations, if the contents of your manifest files are significantly different to each other we recommend you have a single git repo or branch for each install of this solution.\nIf the contents is similar you could use intrinsic-functions,\nconditions or manifest properties within your manifest file to manage variance between the different sets of accounts details or parameter details.\nThis article will provide advice on how to manage change in a multi AWS Organizations environment.\nThere are two examples of when you may want to have multiple organizations we will cover.\nMulti Org SDLC Those instances where you would like to develop or test changes to items you deploy, share or execute in a different AWS Organization to production.\nIf you are developing or testing features that use AWS Organizations we recommend where possible to use an additional AWS Organization. These sorts of changes include when you are developing solutions around AWS Organizations integration with other AWS Services like AWS Security Hub or Amazon Guard Duty. For other scenarios we recommend you continue using your production Organization to reduce any overheads associated with the management of an Organizations. If you do wish to maintain a stronger level of isolation or you have additional reasons for having additional Organizations this article can help you.\nMultiple Prod Orgs Those instances where you would like to make changes to multiple Organizations with a single configuration change.\nIf you are managing multiple production AWS Organizations and would like to reduce the overhead of gitOps this article can help you.\nBackground The Service Catalog Tools default approach for environment management is to use a gitOps approach where you have a set of configuration files per environment that you make changes to. These files stored are stored in a git repository.\nWhen change is detected in the git repository it is applied by the tools. Multiple environments are generally managed via multiple branches and or multiple repositories which can lead to a sprawl of git branches or repositories, it also means rolling out a change out may mean merging changes between many branches or repositories which can increase the time and effort required to roll out changes.\nThe number of people you have in your teams, the number of environments you have, the number of things you deploy, share or execute and the number of changes to push through your environments affect how well a gitOps approach works for you. GitOps starts off simple and becomes more complex with more overheads as you make more changes to your configurations. The goal of this article to identify specific pain points and solutions for them. It is not possible to write guidance for every team as you are all different.\nMulti Org SDLC When working with a multi Org environment we recommend using the same git branch and repository for all environments.\nWhen using Service Catalog Factory this means changes to your git repository affect all environments in parallel. We recommend not deploying changes to a published pipeline - if you want to make a code change to something create a new version.\nWhen using Service Catalog Puppet, where it is possible to specify a version externally to the manifest file we recommend doing so and setting the default version to the production version and overriding it for different environments. This means production (which should be the safest version to deploy) is deployed by default and you need to override it for other environments.\nThe following sections explain how to you can maintain a single manifest file for multiple Organizations when using Service Catalog Puppet:\nDifferent Hub/Core Accounts in a Multi Org SDLC If you have differences in the account list we recommend using intrinsic-functions to declare your AWS account details - for example your security tooling account or log storage account ID will most likely be different in each Organization. Using intrinsic-functions to declare and use the account IDs for each hub account allows you to have different intrinsic-functions files per Organization but keep the same manifest file.\nDifferent Parameters in a Multi Org SDLC If you have Organization wide parameters that are different between Organizations you can use SMM parameters to store the values. These parameters may be a log storage account id, the Arn of a networking resource or an email address. We recommend defining these parameters in the global parameters section of the main manifest file and use a naming convention for the SSM parameters to ensure they are consistent and unique. You can use intrinsic-functions in the parameter name for this usecase but we do not recommend it.\nRolling out changes to already defined items in a Multi Org SDLC We recommend you use the manifest.ini file to specify the versions for the production Organization. If you want to update the version of a stack (or any other item) we recommend defining the version number in the manifest-\u0026laquo;dev-org-puppet-account-id\u0026raquo;.ini file. Should something go wrong with your overriding, the production version will be provisioned to the dev Org which is generally a much lower impact than deploying the test version to production. When you have developing we recommend you change the manifest-\u0026laquo;test-org-puppet-account-id\u0026raquo;.ini file to reflect the change. When testing is complete we recommend deleting the entries from the dev and test ini files and updating the production version (all in the same commit). Following this pattern will show the change you are rolling out in the git history and it should be easy to see what changed, when and if your git commit comments are good then you will see why.\nRolling out the first version of a new item in a Multi Org SDLC If you are creating a new launch (or something of any other action type) we recommend adding it to the main manifest file with a status of \u0026ldquo;ignored\u0026rdquo;. In the manifest-\u0026laquo;dev-org-puppet-account-id\u0026raquo;.properties or manifest-\u0026laquo;test-org-puppet-account-id\u0026raquo;.properties file we recommend you set the status to \u0026ldquo;provisioned\u0026rdquo;, \u0026ldquo;terminated\u0026rdquo;, \u0026ldquo;shared\u0026rdquo; or \u0026ldquo;enabled\u0026rdquo; depending on which type of action you are configuring and where you would like to test it.\nMultiple Prod Orgs When working with a multiple Prod Orgs environment we recommend using the same git branch and repository for all environments.\nWhen using Service Catalog Factory this means changes to your git repository affect all environments in parallel.\nDifferent Hub/Core Accounts in a Multiple Prod Orgs See \u0026ldquo;Different Hub/Core Accounts in a Multi Org SDLC\u0026rdquo; above\nDifferent Parameters in a Multiple Prod Orgs See \u0026ldquo;Different Parameters in a Multi Org SDLC\u0026rdquo; above\nRolling out changes to already defined items in a Multiple Prod Orgs We recommend you roll out your changes in waves. You should decide which Organizations should be in each wave based on risk. You should aim to roll out changes to lower risk waves before higher risk waves. You should test the effects of your changes after each wave completes and before beginning the next. You should then follow the above guidance \u0026ldquo;Rolling out changes to already defined items\u0026rdquo; to see how you can roll out changes to each Organization at a time.\nRolling out the first version of a new item in a Multiple Prod Orgs We recommend you follow the patterns defined in \u0026ldquo;Rolling out changes to already defined items\u0026rdquo; and in \u0026ldquo;Rolling out the first version of a new item\u0026rdquo;\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/every-day-use/110-deleting-a-product.html",
	"title": "Deleting a product",
	"tags": [],
	"description": "",
	"content": " This tutorial will walk you through \u0026ldquo;Deleting a product\u0026rdquo;\nWe will assume you have:\n installed Service Catalog Factory correctly added a product correctly  In the tutorial you will:\n Disabling the product versions   Deleting the product   During this process you will check your progress by verifying what the framework is doing.\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/managing-your-environments/dont-repeat-yourself.html",
	"title": "Don&#39;t Repeat Yourself",
	"tags": [],
	"description": "",
	"content": " Don't Repeat Yourself When you are managing your environments it is best to avoid duplication in your configurations. Duplication can increase the cost of change which may reduce your appetite to make lower priority changes leading to higher technical debt. Duplication can also lead to more human error as you may make a change in only one place instead of many and in the instance where you need to roll out a change quickly duplication can slow you down.\nThere are different ways of removing duplication as different scenarios call for different solutions. They are grouped into categories below:\n Accounts section   "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/managing-your-environments/gitops/intrinsic-functions.html",
	"title": "Intrinsic functions",
	"tags": [],
	"description": "",
	"content": " Intrinsic Functions There is a built in intrinsic function ${AWS::PuppetAccountId} that you can use anywhere within your manifest file.\nWhen your manifest file being expanded the string ${AWS::PuppetAccountId} will be replace with the value of the account id. This allows you avoid hard coding the account id of the hub account.\n accounts: - account_id: \u0026#34;${AWS::PuppetAccountId}\u0026#34; name: \u0026#34;puppet-account\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; - \u0026#34;eu-west-2\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34;   You can specify your own functions within a file named intrinsic-functions.properties within the root of your ServiceCatalogPuppet repository. Within this you can specify name value pairs:\n SecurityToolingAccountId=012345678910   You can then use these within your manifest file anywhere using ${Custom::\u0026lt;THE_NAME_YOU_SPECIFIED\u0026gt;}:\n accounts: - account_id: \u0026#34;${Custom::SecurityToolingAccountId}\u0026#34; name: \u0026#34;puppet-account\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; - \u0026#34;eu-west-2\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34;   To improve readability we recommend you use the functions to set global parameters and use YAML alias and anchors to reduce the size and complexity of your expanded manifest file by removing duplication.\nIf you want to specify different values for different installs you can override values in an intrinsic-functions-0123456789010.properties file. In this example when the manifest file is expanded in account 0123456789010 any values in intrinsic-functions-0123456789010.properties will override values set in the intrinsic-functions.properties file.\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/managing-your-environments/dealing-with-variance.html",
	"title": "Dealing With Variance",
	"tags": [],
	"description": "",
	"content": " What are we going to do? This article will help you to understand how you can deal with variance in your AWS Organization\nWhy is this important When defining your AWS accounts using this solution you can specify accounts one-by-one or you can specify an OU. There are many instances where customers would like to generally refer to an OU but have a single account (or two) within the OU that they would like to treat differently.\nUsing Overwrite or append When you describe an OU you can additionally describe an account that is within the OU so long as you added an overwrite or append attribute:\n accounts: - ou: \u0026#39;/sharedservices\u0026#39; name: \u0026#39;sharedservices\u0026#39; default_region: eu-west-1 regions_enabled: - eu-west-1 - eu-west-2 - eu-west-3 - us-east-1 tags: - outype:foundational - ou:sharedservices - type:prod - team:ccoe - role:securitytooling - role:account - account_id: 0123456789010 name: \u0026#39;special-account\u0026#39; overwrite: tags: - role:special - account_id: 0123456789011 name: \u0026#39;also-special-account\u0026#39; overwrite: tags: - role:also-special   You can use the role:special and role:also-special tags in your actions to target these accounts.\nUsing AWS Organizations account tags If you have many exceptions using append or overwrite can become cumbersome or you may want to use AWS Organizations account tags for other reasons. To use AWS Organizations account tags you add the attribute organizations_account_tags to the account section - you can add this to accounts and to OUs. When you add it, you must specify a value:\n  ignore - this is the default. Setting ignore means you will ignore AWS Organizations tags\n  honour - this means any tags you specify in AWS Organizations will REPLACE the tags you provide in the manifest\n  append - this means any tags you specify in AWS Organizations will APPEND the tags you provide in the manifest\n accounts: - ou: \u0026#39;/sharedservices\u0026#39; name: \u0026#39;sharedservices\u0026#39; default_region: eu-west-1 regions_enabled: - eu-west-1 - eu-west-2 - eu-west-3 - us-east-1 organizations_account_tags: honour tags: - outype:foundational - ou:sharedservices - type:prod - team:ccoe - role:securitytooling - role:account     You can use the append and overwrite described in the section above in conjunction with organizations_account_tags. If you do the organizations_account_tags will take affect first and then append or overwrite will affect the tags from AWS Organizations.\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/managing-your-environments/fault-tolerance.html",
	"title": "Fault Tolerance",
	"tags": [],
	"description": "",
	"content": " What are we going to do? This article will help you to understand how to make the workflow you generate more fault tolerant.\nWhy is this important Customers are using this solution to deploy, operate and govern AWS multi account estates. These estates are made up of several regions and hundreds of accounts.\nWhen building a workflow using this solution you can use dependencies to say only perform an action after another action has completed successfully. By default, every sub action of the dependency must complete successfully before the dependent action is instructed to start.\nA practical example When inflating your accounts you may want to enable AWS Config before enabling AWS Config rules. By default, if the enabling of Config fails in one region of one account no Config rules will be deployed. This is great for rapid feedback - fail fast - but in production this means you have regions of accounts with missing guardrails.\nThe rest of this section explains your options for improving fault tolerance along with the costs of each option.\nSpoke execution mode Using spoke execution mode is the quickest way to improve fault tolerance. When you configure an action to be performed using spoke execution mode you are delegating execution to the spoke. This solution will fan out the spoke actions to each spoke in parallel in a single batch. Each spoke does not affect another - meaning a failure in one spoke will have no impact to others. However, within the spoke a failure enabling Config in one region will mean no rules enabled in other regions. To further improve fault tolerance you will need to use fine grain depends on.\nEnabling spoke execution mode means errors will not be visible in your pipeline output. You will need to log into the spoke account to view the execution there or you will need to use AWS Ops Center in the hub account to see what went wrong. This can increase operational complexity.\nEnabling spoke execution mode typically reduces execution time considerably. We have seen it reduce execution time by more than a factor of 8.\nFine grain depends on When you say AWS Config rules depends on AWS Config enablement the framework will enable AWS Config across all regions of all accounts before attempting to enable any AWS Config rules. Using fine grain depends on you can change that behaviour. You can say:\n Once AWS Config enable completes in the region of the account, start AWS Config rules in the same region of the same account Once AWS Config enable completes in the region x of all accounts, start AWS Config rules in region x of all accounts Once AWS Config enable completes in all regions of account n, start AWS Config rules in all regions of account n  This can be useful when deploying / configuring critical services within you multi account estate but it can increase the execution time of your workflow. We have seen it add 30 - 60 mins of execution time to estates with 6 regions, 200 accounts and 8 actions.\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/managing-your-environments/workflow-tracing.html",
	"title": "Workflow Tracing",
	"tags": [],
	"description": "",
	"content": " What are we going to do? This article will help you to understand how to visualise a completed workflow (pipeline).\nWhy is this important As your adoption of this solution develops your workflow will grow and your pipeline will take longer to run.\nVisualising the workflow allows you to see how long tasks take to execute, what order they execute in and what blocks other tasks from executing. The visualisation also allows you to see the saturation of the workers to help you decide if you need to increase the number of workers.\nGenerating the traces As the workflow runs traces are generated and stored in Amazon S3. It is possible to export these traces in the Google Trace Event format using the following command:\n servicecatalog-puppet export-traces \u0026lt;aws-codebuild-execution-id\u0026gt;   This will produce the input needed when using solutions like Perfetto\nRunning Perfetto locally You may prefer to run Perfetto locally. To do so you can check out VizTracer which provides some helper commands to get it up and running quickly.\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/managing-your-environments/gitops/conditions.html",
	"title": "Conditions",
	"tags": [],
	"description": "",
	"content": " Conditions You can create conditions within your manifest file. These conditions work just like the AWS CloudFormation conditions and can be used to decide whether actions occur or not:\n conditions: IsDev: !Equals - 156551640785 - ${AWS::PuppetAccountId} IsProd: !Not - !Equals - 156551640785 - ${AWS::PuppetAccountId}   At the moment you can use Equals and Not functions to create conditions. Then you can specify conditions for your stacks and other actions:\n stacks: amazon-guardduty-multi-account-prereqs-orgs-account: condition: IsDev name: amazon-guardduty-multi-account-prereqs-orgs-account version: v1 execution: hub capabilities: - CAPABILITY_NAMED_IAM deploy_to: tags: - tag: \u0026#39;role:org_management\u0026#39; regions: default_region outputs: ssm: - param_name: \u0026#34;/foundational/GuardDutyMultiAccount/GuardDutyMultiAccountDelegateAdminRoleArn\u0026#34; stack_output: GuardDutyMultiAccountDelegateAdminRoleArn   When you use conditions you are responsible for ensuring that all actions referenced within the depends_on have the same conditions.\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/design-considerations/using-parameters-in-the-real-world.html",
	"title": "Using parameters in the real world",
	"tags": [],
	"description": "",
	"content": " What are we going to do? This article will show you how to manage parameters across your Service Catalog Tools environment. It is a collection of real world examples of how to use parameters for organization unit, account and region level wide configurations.\nUsing parameters (basic usage) When you specify a launch you can specify parameters. Here is an example where a vpc is provisioned into the default region of each account tagged as type:spoke:\n launches: networking: portfolio: \u0026#34;networking\u0026#34; product: \u0026#34;vpc\u0026#34; version: \u0026#34;v1\u0026#34; parameters: cidr: default: \u0026#34;10.0.0.1/24\u0026#34; deploy_to: tags: - tag: \u0026#34;type:spoke\u0026#34; regions: \u0026#34;default_region\u0026#34;   You could have also retrieved the value from SSM:\n launches: networking: portfolio: \u0026#34;networking\u0026#34; product: \u0026#34;vpc\u0026#34; version: \u0026#34;v1\u0026#34; parameters: cidr: ssm: name: \u0026#34;/multi-account-config/networking/vpc/default/cidr\u0026#34; deploy_to: tags: - tag: \u0026#34;type:spoke\u0026#34; regions: \u0026#34;default_region\u0026#34;   This makes it dynamic but what happens if you want to have a different value for each region?\nUsing mappings for parameters You can use a mapping to make this more configurable:\n mappings: VPCs: us-east-1: \u0026#34;cidr\u0026#34;: \u0026#34;10.0.0.1/24\u0026#34; us-west-1: \u0026#34;cidr\u0026#34;: \u0026#34;192.168.0.1/26\u0026#34; launches: networking: portfolio: \u0026#34;networking\u0026#34; product: \u0026#34;vpc\u0026#34; version: \u0026#34;v1\u0026#34; parameters: cidr: mapping: [VPCs, AWS::Region, cidr] deploy_to: tags: - tag: \u0026#34;type:spoke\u0026#34; regions: \u0026#34;default_region\u0026#34;   With the above configuration you are saying when provisioning vpc into us-east-1 use 10.0.0.1/24 and when provisioning into us-west-1 use 192.168.0.1/26. This allows you to have a different parameter value for each region but that value will be the same for every launch. To make it different per account you can use the following:\n mappings: VPCs: 0123456789010: \u0026#34;cidr\u0026#34;: \u0026#34;10.0.0.1/24\u0026#34; 0098765432110: \u0026#34;cidr\u0026#34;: \u0026#34;192.168.0.1/26\u0026#34; launches: networking: portfolio: \u0026#34;networking\u0026#34; product: \u0026#34;vpc\u0026#34; version: \u0026#34;v1\u0026#34; parameters: cidr: mapping: [VPCs, AWS::AccountId, cidr] deploy_to: tags: - tag: \u0026#34;type:spoke\u0026#34; regions: \u0026#34;default_region\u0026#34;   With the above configuration you are saying when provisioning vpc into account 0123456789010 use 10.0.0.1/24 and when provisioning into account 0098765432110 use 192.168.0.1/26. This allows you to have a different parameter value for each account but you will have to update your manifest file each time you want to add an account.\nUsing intrinsic functions in ssm parameter names You can use the account id and region name within the SSM parameter name value to use account and region specific ssm parameters:\n launches: networking: portfolio: \u0026#34;networking\u0026#34; product: \u0026#34;vpc\u0026#34; version: \u0026#34;v1\u0026#34; parameters: cidr: ssm: name: \u0026#34;/vpcs/${AWS::AccountId}/${AWS::Region}/cidr\u0026#34; deploy_to: tags: - tag: \u0026#34;type:spoke\u0026#34; regions: \u0026#34;default_region\u0026#34;   Each time vpc is provisioned into a region of an account the region name and account id will be used to substitute values in the ssm name attribute. For example, when you provision into us-east-1 of account 012345678910 the ssm parameter used to get the value for the cidr parameter will be the one with the name \u0026quot;/vpcs/012345678910/us-east-1/cidr\u0026rdquo;.\nStoring values in ssm using intrinsic functions You can store the stack outputs for your products in SSM and use intrinsic functions to derive the name:\n launches: networking: portfolio: \u0026#34;networking\u0026#34; product: \u0026#34;vpc\u0026#34; version: \u0026#34;v1\u0026#34; parameters: cidr: ssm: name: \u0026#34;/vpcs/${AWS::AccountId}/${AWS::Region}/cidr\u0026#34; deploy_to: tags: - tag: \u0026#34;type:spoke\u0026#34; regions: \u0026#34;default_region\u0026#34; outputs: ssm: - param_name: \u0026#34;/vpcs/${AWS::AccountId}/${AWS::Region}/id\u0026#34; stack_output: VPCId  When you provision into us-east-1 of account 012345678910 the ssm parameter used to store the stack output will have the name of \u0026quot;/vpcs/012345678910/us-east-1/id\u0026rdquo;\nCustomer provided parameters If you have built a self-service / account vending mechanism you may want to allow the customers of your solution to set some parameters to be used later on - for example whether they require a connected account or not, if they want private subnets or public or even if they want to have networking at all or not.\nIf you are vending accounts by provisioning a product into your Service Catalog Tools account you have a very easy option. Include an SSM parameter into your account creation product. The name of the parameter should be derived from the account id of the newly created account:\n AWSTemplateFormatVersion: \u0026#39;2010-09-09\u0026#39; Parameters: NetworkType: Type: String AllowedValues\u0026#34; : [\u0026#34;connected\u0026#34;, \u0026#34;private\u0026#34;, \u0026#34;public\u0026#34;, \u0026#34;none\u0026#34;] Description: Type of networking setup required Resources: Account: Type: Custom::Resource Description: A custom resource representing an AWS Account Properties: ServiceToken: !Ref AccountCreatorLambdaArn Email: !Ref Email AccountName: !Ref AccountName IamUserAccessToBilling: !Ref IamUserAccessToBilling NetworkingRequired: Type: AWS::SSM::Parameter Properties: Name: !Sub \u0026#34;/networking/${Account.AccountId}/NetworkType\u0026#34; Value: !Ref NetworkType  Please note some of the parameters and resources have been omitted from the example above.\nThis will create an SSM parameter in your Service Catalog Tools account that can be used in your launches:\n launches: networking: portfolio: \u0026#34;networking\u0026#34; product: \u0026#34;networks\u0026#34; version: \u0026#34;v1\u0026#34; parameters: NetworkType: ssm: name: \u0026#34;/networking/${AWS::AccountId}/NetworkType\u0026#34; deploy_to: tags: - tag: \u0026#34;type:spoke\u0026#34; regions: \u0026#34;default_region\u0026#34;   Within your product you can use conditions to provision the correct set of resources or you can use three launches (one for each network type) along with a condition on whether they should do anything or not:\n launches: networking-connected: portfolio: \u0026#34;networking\u0026#34; product: \u0026#34;networks\u0026#34; version: \u0026#34;v1\u0026#34; parameters: NetworkType: ssm: name: \u0026#34;/networking/${AWS::AccountId}/NetworkType\u0026#34; deploy_to: tags: - tag: \u0026#34;type:spoke\u0026#34; regions: \u0026#34;default_region\u0026#34; networking-private: portfolio: \u0026#34;networking\u0026#34; product: \u0026#34;networks\u0026#34; version: \u0026#34;v1\u0026#34; parameters: NetworkType: ssm: name: \u0026#34;/networking/${AWS::AccountId}/NetworkType\u0026#34; deploy_to: tags: - tag: \u0026#34;type:spoke\u0026#34; regions: \u0026#34;default_region\u0026#34; networking-private: portfolio: \u0026#34;networking\u0026#34; product: \u0026#34;networks\u0026#34; version: \u0026#34;v1\u0026#34; parameters: NetworkType: ssm: name: \u0026#34;/networking/${AWS::AccountId}/NetworkType\u0026#34; deploy_to: tags: - tag: \u0026#34;type:spoke\u0026#34; regions: \u0026#34;default_region\u0026#34;   Example product template for private product:\n AWSTemplateFormatVersion: 2010-09-09 Parameters: NetworkType: Description: Type of network to create Type: String AllowedValues\u0026#34; : [\u0026#34;connected\u0026#34;, \u0026#34;private\u0026#34;, \u0026#34;public\u0026#34;, \u0026#34;none\u0026#34;] Conditions: CreateNetwork: !Equals - !Ref NetworkType - private Resources: Network: Type: \u0026#39;AWS::EC2::VPC\u0026#39; Properties: CidrBlock: 10.0.0.0/16 EnableDnsSupport: \u0026#39;false\u0026#39; EnableDnsHostnames: \u0026#39;false\u0026#39;  Using large numbers of SSM parameters If you are using SSM parameters to store region or account specific configurations you can easily end up with a large number of SSM parameters.\nFor example, having the following parameters Network Type, VPC Cidr, VPC Id, Number of subnets for 7 regions of 300 accounts is over 8,000 parameters.\nWhen using a large number of SSM parameters we recommend you prefix your SSM parameter with a common value which is distinct from other use cases and use the prefix option when specifying the parameter in the manifest file.\nFor example in the networking example above we recommend the following SSM parameter names:\n /platform-protected/configurations/networking/${AWS::AccountId}/${AWS::Region}/NetworkType /platform-protected/configurations/networking/${AWS::AccountId}/${AWS::Region}/VPC/Cidr /platform-protected/configurations/networking/${AWS::AccountId}/${AWS::Region}/VPC/Id /platform-protected/configurations/networking/${AWS::AccountId}/${AWS::Region}/Subnets/count  Having /platform-protected in the prefix makes it easier to write IAM policies for the resources, protecting it from write changes from non service roles.\nHaving /platform-protected/configurations/networking in the prefix makes it easier to write IAM policies by functional area - networking prefixes can be made read/write for members of the networking teams.\nHaving /platform-protected/configurations/networking in the prefix makes it easier to use paths:\n launches: networking-vpc: portfolio: \u0026#34;networking\u0026#34; product: \u0026#34;networks\u0026#34; version: \u0026#34;v1\u0026#34; parameters: NetworkType: ssm: name: \u0026#34;/platform-protected/configurations/networking/${AWS::AccountId}/${AWS::Region}/NetworkType\u0026#34; path: \u0026#34;/platform-protected/configurations/networking\u0026#34; deploy_to: tags: - tag: \u0026#34;type:spoke\u0026#34; regions: \u0026#34;default_region\u0026#34;   If you specify a path the solution will use AWS SSM get_parameters_by_path instead of get_parameter. This provides a significant performance improvement reducing the overall execution time of your workflow.\nUsing boto3 parameters You can use the result of a boto3 call as a parameter.\nHere we are saying use the ssm client in the spoke account for the region you are provisioning the stack into to call get_parameter and filter the result down to Parameter.Value:\n stacks: golden-ami-id-replicator: name: \u0026#34;ssm-parameter\u0026#34; version: \u0026#34;v2\u0026#34; execution: \u0026#34;hub\u0026#34; parameters: Name: default: \u0026#34;GoldenAMIId\u0026#34; Value: boto3: account_id: \u0026#34;${AWS::AccountId}\u0026#34; region: \u0026#34;${AWS::Region}\u0026#34; client: \u0026#34;ssm\u0026#34; call: \u0026#34;get_parameter\u0026#34; use_paginator: false arguments: Name: \u0026#34;GoldenAMIId\u0026#34; use_paginator: false filter: \u0026#34;Parameter.Value\u0026#34; deploy_to: tags: - tag: role:spoke regions: regions_enabled  If you omit the region the framework will use the home region where you installed the framework and if you omit the account_id the framework will use the hub account where you installed the framework.\nUsing ${AWS::AccountId} and ${AWS::Region} evaluate to the account and region where the action is occuring.\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/managing-your-environments/gitops/manifest-properties.html",
	"title": "Manifest properties",
	"tags": [],
	"description": "",
	"content": " Manifest properties If you are using puppet to manage multiple environments you may find it easier to keep the versions of your launches in properties files instead of the manifest.yaml files. To do this you create a file named manifest.properties in the same directory as your manifest.yaml file. Within this file you can specify the following:\n [launches] IAM-1.version = v50  This will set the version for the launch with the name IAM-1 to v50.\nPlease note this will overwrite the values specified in the manifest.yaml files with no warning.\nIf you are using multiple instances of puppet you can also create a file named manifest-.properties. Values in this file will overwrite all other values making the order of reading:\n manifest.yaml other manifest files (eg in manifests/) manifest.properties manifest-\u0026laquo;puppet-account-id\u0026raquo;.properties  "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/every-day-use/150-creating-a-portfolio.html",
	"title": "Creating a portfolio",
	"tags": [],
	"description": "",
	"content": " What are we going to do? This tutorial will walk you through \u0026ldquo;Creating a portfolio\u0026rdquo; with a spoke account.\nWe will assume you have installed Service Catalog Puppet correctly.\nWe are going to perform the following steps:\n create a portfolio file commit our portfolio file verify the framework has created an AWS Service Catalog portfolio  During this process you will check your progress by verifying what the framework is doing at each step.\nStep by step guide Here are the steps you need to follow to \u0026ldquo;Creating a portfolio\u0026rdquo;\nAdding the portfolio to the framework   Navigate to the ServiceCatalogFactory CodeCommit repository\n  Click on portfolios and then on reinvent.yaml and finally on edit if that file exists otherwise create a file named portfolios/reinvent.yaml\n  Add the following to the end of the file (be careful with your indentation):\n   Portfolios: - DisplayName: \u0026#34;cloud-engineering-governance\u0026#34; Description: \u0026#34;Portfolio containing the products needed to govern AWS accounts\u0026#34; ProviderName: \u0026#34;cloud-engineering\u0026#34; Associations: - \u0026#34;arn:aws:iam::${AWS::AccountId}:role/TeamRole\u0026#34; Tags: - Key: \u0026#34;type\u0026#34; Value: \u0026#34;governance\u0026#34; - Key: \u0026#34;creator\u0026#34; Value: \u0026#34;cloud-engineering\u0026#34; - Key: \u0026#34;cost-center\u0026#34; Value: \u0026#34;cloud-engineering\u0026#34;   Once you have updated the file fill in the fields for Author name, Email address, Commit message and hit Commit changes\n  Using a good / unique commit message will help you understand what is going on later.\n Verify the portfolio was created Once you have made your changes the ServiceCatalogFactory Pipeline should have run or if you were quick may still be running. If it has not yet started feel free to the hit the Release change button.\nOnce it has completed it should show the Source and Build stages in green to indicate they have completed successfully:\n  Once you have verified the pipeline has run you can go to Service Catalog portfolios to view your portfolio.\nYou should see the portfolio you just created listed:\n  You have now successfully created a portfolio!\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/40-reinvent2019/150-task-2.html",
	"title": "Data governance",
	"tags": [],
	"description": "",
	"content": " The ask Example Corp now has a number of development and test workloads in AWS. Many of these workloads make use of data storage services such as Amazon RDS and Amazon S3. The customer has recently established a dedicated data governance team, who have been tasked with identifying controls for workloads that process data classified as confidential or internal-use only.\nThe data governance team has recently issued guidelines around the use of encryption for data at rest and in transit in the cloud. We have below an excerpt from the data governance standard:\n 3.1.1 All cloud data storage systems must be configured to support encryption at rest and in transit using industry supported encryption algorithms for data classified as Confidential, or Internal-Use only. For a list of approved encryption algorithms and key-lengths please see Appendix A.\n From these guidelines there are two new requirements:\n The data governance team at Example Corp wants to get visibility into resources where encryption at rest is not being used The team wants to use AWS Service Catalog to make it easy for development teams to comply with the encryption-at-rest requirement, without having to set it up themselves.  These requirements will shape the work that goes into building additional data governance controls as development teams look to use additional AWS services to store production or material data.\nThe plan We are going to create and deploy a data governance control using an AWS Config managed rule to ensure the teams are using encryption when creating an RDS instance. We will then create a self service Service Catalog product so the teams can create compliant resources.\n Create the control   Provision the control   Create the product   If you need help at any time please raise your hand.\n "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/managing-your-environments/fine-grained-depends-on.html",
	"title": "Fine grained depends_on",
	"tags": [],
	"description": "",
	"content": " What are we going to do? This article will explain how the depends_on clause works within service catalog puppet and how fine tune its use to achieve better saturation of workers as well reducing the impact of failures when managing your multi account.\nWhat is depends_on? Within service catalog puppet you can provision AWS Service Catalog products, share Service Catalog portfolios, execute AWS Lambda functions, run AWS CodeBuild projects and run assertions to verify the effects of your configurations.\nWhen configuring service catalog puppet you may want to specify the order in which these things happen. For example, when building out your networking capability, you may want to provision a Service Catalog product containing a lambda function that can remove any default networking resources in your AWS account (default VPC, subnet, security group etc).\nOnce you have provisioned that, you may want to execute it and then assert that the networking resources are removed before provisioning a new VPC and then Subnets. Once all that provisioning is completed, you may then want to share a portfolio that allows users to provision an EKS cluster into the newly created networking stacks.\nThe basic building blocks for ordering these things is a depends_on clause which you can use in all sections of the manifest file\nHere is an example:\n launches: networking: portfolio: \u0026#34;networking\u0026#34; product: \u0026#34;vpc\u0026#34; version: \u0026#34;v1\u0026#34; depends_on: - name: terminate-default-networking type: lambda-execution parameters: cidr: default: \u0026#34;10.0.0.1/24\u0026#34; deploy_to: tags: - tag: \u0026#34;type:spoke\u0026#34; regions: \u0026#34;default_region\u0026#34;   In this example before any provisioning occurs for the networking launch every lambda-executions for terminate-default-networking must have completed successfully across all regions of all accounts. If any fail the launch will not be scheduled.\nFine tuning depends_on When you configure service catalog puppet you are building a workflow that is executed by a set of workers. You can configure the number of workers in the workflow and you can configure the shape and size of the workflow by modifying the manifest file.\nWhen you use the default configuration of depends_on a natural choke point is created. By saying B depends on A, you are saying everything within A must complete before B can begin. You can reduce the impact of this by configuring the depends_on affinity value.\nRegion You can set the affinity to region:\n launches: networking: portfolio: \u0026#34;networking\u0026#34; product: \u0026#34;vpc\u0026#34; version: \u0026#34;v1\u0026#34; depends_on: - name: terminate-default-networking type: lambda-execution affinity: region parameters: cidr: default: \u0026#34;10.0.0.1/24\u0026#34; deploy_to: tags: - tag: \u0026#34;type:spoke\u0026#34; regions: \u0026#34;default_region\u0026#34;   In this example before any provisioning occurs for the networking launch in us-east-1 every lambda-executions for terminate-default-networking in us-east-1 across all accounts must have completed successfully. If any lambda-execution for terminate-default-networking in any account in us-east-1 fails then no launches for networking in us-east-1 will be scheduled.\nThis is useful when you are building product sets that should be provisioned in hub and spoke accounts and rely on regional resources like networking.\nAccount You can set the affinity to account:\n launches: networking: portfolio: \u0026#34;networking\u0026#34; product: \u0026#34;vpc\u0026#34; version: \u0026#34;v1\u0026#34; depends_on: - name: terminate-default-networking type: lambda-execution affinity: account parameters: cidr: default: \u0026#34;10.0.0.1/24\u0026#34; deploy_to: tags: - tag: \u0026#34;type:spoke\u0026#34; regions: \u0026#34;default_region\u0026#34;   In this example before any provisioning occurs for the networking launch in account 2 every lambda-executions for terminate-default-networking in account 1 across all regions must have completed successfully. If any lambda-execution for terminate-default-networking in any region in account 1 fails then no launches for networking in account 2 will be scheduled.\nThis is useful when you are building product sets that should be provisioned in hub and spoke accounts and rely on global resources like IAM.\nAccount and region You can set the affinity to account:\n launches: networking: portfolio: \u0026#34;networking\u0026#34; product: \u0026#34;vpc\u0026#34; version: \u0026#34;v1\u0026#34; depends_on: - name: terminate-default-networking type: lambda-execution affinity: account-and-region parameters: cidr: default: \u0026#34;10.0.0.1/24\u0026#34; deploy_to: tags: - tag: \u0026#34;type:spoke\u0026#34; regions: \u0026#34;default_region\u0026#34;   In this example before any provisioning occurs for the networking launch in us-east-1 of account 2 every lambda-executions for terminate-default-networking in account 1 in us-east-1 must have completed successfully. If any lambda-execution for terminate-default-networking in us-east-1 in account 1 fails then no launches for networking in account 2 will be scheduled.\nThis is useful when you are building products sets that should be provisioned into accounts in order into multiple spoke accounts, for example VPCs, Subnets and NACLs.\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/every-day-use/180-adding-a-product-to-a-portfolio.html",
	"title": "Adding a product to a portfolio",
	"tags": [],
	"description": "",
	"content": " What are we going to do? This tutorial will walk you through \u0026ldquo;Adding a product to a portfolio\u0026rdquo; into a spoke account.\nWe will assume you have:\n installed Service Catalog Puppet correctly you have created a product you have created a portfolio  We are going to perform the following steps:\n add a product to a portfolio  During this process you will check your progress by verifying what the framework is doing at each step.\nStep by step guide Here are the steps you need to follow to \u0026ldquo;Adding a product to a portfolio\u0026rdquo;\nAdd the product to the portfolio   Navigate to the ServiceCatalogFactory CodeCommit repository again\n  Click on portfolios\n     Click on reinvent.yaml     Click Edit     Replace the contents of your file with this:   Schema: factory-2019-04-01 Products: - Name: \u0026#34;aws-config-enable-config\u0026#34; Owner: \u0026#34;data-governance@example.com\u0026#34; Description: \u0026#34;Enables AWS Config\u0026#34; Distributor: \u0026#34;cloud-engineering\u0026#34; SupportDescription: \u0026#34;Speak to data-governance@example.com about exceptions and speak to cloud-engineering@example.com about implementation issues\u0026#34; SupportEmail: \u0026#34;cloud-engineering@example.com\u0026#34; SupportUrl: \u0026#34;https://wiki.example.com/cloud-engineering/governance/aws-config-enable-config\u0026#34; Tags: - Key: \u0026#34;type\u0026#34; Value: \u0026#34;governance\u0026#34; - Key: \u0026#34;creator\u0026#34; Value: \u0026#34;cloud-engineering\u0026#34; - Key: \u0026#34;cost-center\u0026#34; Value: \u0026#34;cloud-engineering\u0026#34; Versions: - Name: \u0026#34;v1\u0026#34; Description: \u0026#34;v1 of aws-config-enable-config\u0026#34; Active: True Source: Provider: \u0026#34;CodeCommit\u0026#34; Configuration: RepositoryName: \u0026#34;aws-config-enable-config\u0026#34; BranchName: \u0026#34;main\u0026#34; Portfolios: - \u0026#34;cloud-engineering-governance\u0026#34; Portfolios: - DisplayName: \u0026#34;cloud-engineering-governance\u0026#34; Description: \u0026#34;Portfolio containing the products needed to govern AWS accounts\u0026#34; ProviderName: \u0026#34;cloud-engineering\u0026#34; Associations: - \u0026#34;arn:aws:iam::${AWS::AccountId}:role/TeamRole\u0026#34; Tags: - Key: \u0026#34;type\u0026#34; Value: \u0026#34;governance\u0026#34; - Key: \u0026#34;creator\u0026#34; Value: \u0026#34;cloud-engineering\u0026#34; - Key: \u0026#34;cost-center\u0026#34; Value: \u0026#34;cloud-engineering\u0026#34;    Take note of the highlighted lines 26 and 27. We have added a portfolio to the product. Please be aware that the role arn:aws:iam::${AWS::AccountId}:role/TeamRole probably does not exist in your account and will need updating.  Once you have updated the file fill in the fields for Author name, Email address, Commit message and hit Commit changes\n  Using a good / unique commit message will help you understand what is going on later.\n Verify the product was added to the portfolio Once you have made your changes the ServiceCatalogFactory Pipeline should have run or if you were quick may still be running. If it has not yet started feel free to the hit the Release change button.\nOnce it has completed it should show the Source and Build stages in green to indicate they have completed successfully:\n  Once you have verified the pipeline has run you can go to Service Catalog portfolios to view your portfolio.\n Click on reinvent-cloud-engineering-governance     Click on the product aws-config-enable-config     Click on the version v1    "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/every-day-use/190-creating-a-manifest.html",
	"title": "Creating a manifest",
	"tags": [],
	"description": "",
	"content": " What are we going to do? This tutorial will walk you through \u0026ldquo;Creating a manifest\u0026rdquo;\nWe will assume you have installed Service Catalog Puppet correctly.\nWe are going to perform the following steps:\n create a manifest file  During this process you will check your progress by verifying what the framework is doing at each step.\nStep by step guide Here are the steps you need to follow to \u0026ldquo;Creating a manifest\u0026rdquo;\nCreating the manifest file   Navigate to the ServiceCatalogPuppet CodeCommit repository\n  Scroll down to the bottom of the page and hit the Create file button\n    Editing the manifest file  Write out the content of your manifest file. Here is an example snippet:   accounts: - account_id: \u0026#34;\u0026lt;YOUR_ACCOUNT_ID_WITHOUT_HYPHENS\u0026gt;\u0026#34;  name: \u0026#34;puppet-account\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34;    Please read through the docs to help you write out the full manifest file.  Committing the manifest file Now that we have written the manifest file we are ready to commit it.\n  Set the File name to manifest.yaml\n  Set your Author name\n  Set your Email address\n  Set your Commit message\n  Using a good / unique commit message will help you understand what is going on later.\n  Click the Commit changes button:    "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/every-day-use/192-adding-an-account.html",
	"title": "Adding an account",
	"tags": [],
	"description": "",
	"content": " What are we going to do? This tutorial will walk you through \u0026ldquo;Adding an account\u0026rdquo;\nWe will assume you have:\n installed Service Catalog Puppet correctly created a manifest bootstrapped a spoke  We are going to perform the following steps:\n adding an account to the manifest file  During this process you will check your progress by verifying what the framework is doing at each step.\nStep by step guide Here are the steps you need to follow to \u0026ldquo;Adding an account\u0026rdquo;\nAdding an account to the manifest file   Navigate to the ServiceCatalogPuppet CodeCommit repository\n  Click on manifest.yaml\n  Click Edit\n      Append the following snippet to the YAML document in the main input field (be careful with your indentation):\n  Copy the following snippet into the main input field:\n accounts: - account_id: \u0026#34;\u0026lt;YOUR_ACCOUNT_ID_WITHOUT_HYPHENS\u0026gt;\u0026#34; name: \u0026#34;puppet-account\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; - \u0026#34;eu-west-2\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34;     Update account_id on line to show the account id of the account you have bootstrapped\n  Committing the manifest file   Set the File name to manifest.yaml\n  Set your Author name\n  Set your Email address\n  Set your Commit message\n  Using a good / unique commit message will help you understand what is going on later.\n  Click the Commit changes button:    "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/conclusion/",
	"title": "Conclusion",
	"tags": [],
	"description": "",
	"content": "What have we accomplished? We started with a set of requirements for budget, cost, and data governance and turned them into codified controls in your AWS Accounts using native AWS Services and modern software development practices.\n An AWS Config rule that helps us discover Amazon EC2 instance types in use An AWS Config rule that identifies Amazon RDS instances that do not use encryption at rest An AWS Service Catalog product that creates a curated version of Amazon Aurora that enforces encryption by default  In the workshop we used a single AWS Account and a single AWS Region to show you what's possible using open source service catalog tools and AWS Service Catalog. You can use the service catalog tools and AWS Service Catalog to provision products across AWS estates that have hundreds of AWS Accounts, in different regions based on your requirements.\nWhat's next? We recommend that you follow this workshop with further reading about service catalog factory and service catalog puppet to get a deeper understanding of how the service catalog tools work behind the scenes to easily create and provision curated AWS Service Catalog products across your organisation.\nLevel up your skills by trying out the tasks you've completed across an estate of AWS Accounts. Think about how you could fit the tools into your day-to-day workflow with AWS Service Catalog.\nFeedback Remember to provide feedback for the workshop in the re:Invent mobile app before you leave. Your feedback helps us shape the content of workshops and is important in driving future work on service catalog tools.\nIf you'd like to provide feedback about the service catalog tools or report bugs in the workshop, please use our GitHub issue tracker.\nEnjoy re:Invent!\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/installation/installing-specific-versions.html",
	"title": "Installing specific versions",
	"tags": [],
	"description": "",
	"content": "You may want to install a specific version of either factory or puppet. For example, you may want to ensure you are using the same version in two different installations. In order to do so you can alter the value of the version parameter in the initialisation stacks you created when installing the solution.\nBefore updating the parameter value you must select which version you would like to install. You can see which versions you can use in the following links:\n Factory - https://pypi.org/project/aws-service-catalog-factory/#history Puppet - https://pypi.org/project/aws-service-catalog-puppet/#history  Once you have selected the version you wish to install you must update your initialiser stack. If you followed the install guide you should have a stack named factory-initialization-stack or puppet-initialization-stack.\nYou should navigate to AWS CloudFormation in the region of the AWS account where you installed the solution and search for the stack.\nOnce found, you should click Update button, on the next screen ensure Use current template is selected before clicking Next.\nThen on the Parameters screen scroll to the bottom of the page where you will see the Advanced section where you may see the warning Do not change unless told to do so.\nWithin the Advanced section there is a parameter named Version.\nYou will need to set its value following the pattern:\naws-service-catalog-factory==\u0026lt;version\u0026gt;  or aws-service-catalog-puppet==\u0026lt;version\u0026gt;  where version is the name of the version you would like to install. For example to install version 0.60.0 of factory you should use:\naws-service-catalog-factory==0.60.0  "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/design-considerations/portfolio-management.html",
	"title": "Portfolio Management",
	"tags": [],
	"description": "",
	"content": " What are we going to do? This article will help you understand AWS Service Catalog portfolios, what they are, how they are used by the service and how you can best make use of them whilst using the Service Catalog Tools.\nWhat is a portfolio? Within AWS Service Catalog a portfolio is a logical grouping of products.\nIt makes sense to group products that provision similar or complimentary resources together, but this may not give you the flexibility you need:\n Within AWS Service Catalog you set associations at the portfolio level so by default when you grant access to a portfolio all products can be seen. Within AWS Service Catalog you can share portfolios with other accounts. You cannot share just a single product from a portfolio.  How you can make best use of them When using the Service Catalog Tools we recommend you think about how your products will be consumed. If you are going to offer a \u0026lsquo;service catalog\u0026rsquo; or build a vending machine we recommend grouping those products together into a portfolio.\nWhen using the Service Catalog Tools to provision resources into an account we recommend grouping those products into a portfolio.\nWhen you have multiple teams building products we recommend each team having their own portfolios.\nThis normally results in at least two portfolios per team:\n team a  self service offering other products   team b  self service offering other products    The teams we have worked with normally group the products into mandatory products and self service products. For example, if the team using Service Catalog Tools is a cloud engineering / CCOE team they would provision products like AWS Security Hub Enabler into an account - this would be mandatory. If the same team had some optional products like Encrypted S3 Bucket then this would go into an optional portfolio. This results in the following structure:\n team a  mandatory optional    The names of the portfolios are for you to chose as you know your audience better than we do but we have found the following names work well:\n mandatory optional vending-machine networking-self-service well-governed-vending-machine well-architected-vending-machine compulsory  If you would like to share your portfolio names raise a github issue to share\n "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/40-reinvent2019/100-task-1/200-provision-the-control.html",
	"title": "Provision the control",
	"tags": [],
	"description": "",
	"content": " What are we going to do? We are going to perform the following steps:\n create a manifest file with our account in it provision the product aws-config-desired-instance-types into our account  Step by step guide Here are the steps you need to follow to provision the control. In the previous task, we created an AWS Service Catalog product but it has not yet been provisioned.\nCreate a manifest file with our account in it   Navigate to the ServiceCatalogPuppet CodeCommit repository\n  Scroll down to the bottom of the page and hit the Create file button\n     For the next step you will need to know your account id. To find your account id you can check the console, in the top right drop down. It is a 12 digit number. When using your account id please do not include the hyphens ('-') and do not use the angle brackets (\u0026lsquo;\u0026lt;\u0026rsquo;,\u0026lsquo;\u0026gt;\u0026rsquo;)     Copy the following snippet into the main input field and replace account_id to show your account id on the highlighted line:   accounts: - account_id: \u0026#34;\u0026lt;YOUR_ACCOUNT_ID_WITHOUT_HYPHENS\u0026gt;\u0026#34;  name: \u0026#34;puppet-account\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34;   it should look like the following - but with your account id on the highlighted line:\n accounts: - account_id: \u0026#34;012345678910\u0026#34;  name: \u0026#34;puppet-account\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34;   Provision the product aws-config-desired-instance-types into a spoke account   Append the following snippet to the end of the main input field:\n launches: aws-config-desired-instance-types: portfolio: \u0026#34;reinvent-cloud-engineering-governance\u0026#34; product: \u0026#34;aws-config-desired-instance-types\u0026#34; version: \u0026#34;v1\u0026#34; parameters: InstanceType: default: \u0026#34;t2.medium, t2.large, t2.xlarge\u0026#34;  deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;     The CloudFormation template we used to create this product had a parameter named InstanceType. The highlighted lines show how we can use the framework to set a value for that parameter when provisioning it.\n  The main input field should look like this (remember to set your account_id):   accounts: - account_id: \u0026#34;\u0026lt;YOUR_ACCOUNT_ID_WITHOUT_HYPHENS\u0026gt;\u0026#34; name: \u0026#34;puppet-account\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34; launches: aws-config-desired-instance-types: portfolio: \u0026#34;reinvent-cloud-engineering-governance\u0026#34; product: \u0026#34;aws-config-desired-instance-types\u0026#34; version: \u0026#34;v1\u0026#34; parameters: InstanceType: default: \u0026#34;t2.medium, t2.large, t2.xlarge\u0026#34; deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;   Committing the manifest file Now that we have written the manifest file we are ready to commit it.\n  Set the File name to manifest.yaml\n  Set your Author name\n  Set your Email address\n  Set your Commit message\n  Using a good / unique commit message will help you understand what is going on later.\n  Click the Commit changes button:    What did we just do? The YAML file we created in the previous step told the framework to perform the following actions:\n provision a product named aws-config-desired-instance-types into each of the enabled regions of the account  When you added the following:\n launches: aws-config-desired-instance-types: portfolio: \u0026#34;reinvent-cloud-engineering-governance\u0026#34; product: \u0026#34;aws-config-desired-instance-types\u0026#34; version: \u0026#34;v1\u0026#34; parameters: InstanceType: default: \u0026#34;t2.medium, t2.large, t2.xlarge\u0026#34; deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;    You told the framework to provision v1 of aws-config-desired-instance-types from the portfolio cloud-engineering-governance into every account that has the tag type:prod\n accounts: - account_id: \u0026#34;\u0026lt;YOUR_ACCOUNT_ID_WITHOUT_HYPHENS\u0026gt;\u0026#34; name: \u0026#34;puppet-account\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34;   Within each account there will be a copy of the product provisioned into each of the regions listed in the regions_enabled section:\n accounts: - account_id: \u0026#34;\u0026lt;YOUR_ACCOUNT_ID_WITHOUT_HYPHENS\u0026gt;\u0026#34; name: \u0026#34;puppet-account\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; tags:  - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34;   For this workshop, we are creating and provisioning the product into the same AWS Account, but in a multi-account setup, you might choose to create a product in a \u0026ldquo;hub\u0026rdquo; account and provision it only to \u0026ldquo;spoke\u0026rdquo; accounts.\nIn the workshop, you will only have permission to view the products in eu-west-1.\n Verifying the provisioned product Once you have made your changes the ServiceCatalogPuppet Pipeline should have run. If you were quick in making the change, the pipeline may still be running. If it has not yet started feel free to the hit the Release change button.\nOnce it has completed it should show the Source, Generate and Deploy stages in green to indicate they have completed successfully:\n  If this is failing please raise your hand for some assistance\n Once you have verified the pipeline has run you can go to Service Catalog provisioned products to view your provisioned product. Please note when you arrive at the provisioned product page you will need to select account from the filter by drop down in the top right:\n  If you cannot see your product please raise your hand for some assistance\n You have now successfully provisioned a product\nVerify that the AWS Config rule is enabled To see the AWS Config rule enabled, navigate to AWS Config rules. Once there you should see the following:\n  "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/40-reinvent2019/150-task-2/200-provision-the-control.html",
	"title": "Provision the control",
	"tags": [],
	"description": "",
	"content": " What are we going to do? We are going to perform the following steps:\n provision the product aws-config-rds-storage-encrypted  For this workshop, we are using the same account as both the hub and spoke for simplicity; in a multi-account setup, products that are created in a hub account could be provisioned in multiple spoke accounts.\nStep by step guide Here are the steps you need to follow to \u0026ldquo;Provision the control\u0026rdquo;\nProvision the product aws-config-rds-storage-encrypted into a spoke account   Navigate to the ServiceCatalogPuppet CodeCommit repository again\n  Click on manifest.yaml\n  Click Edit\n  Append the following snippet to the end of the main input field:\n   aws-config-rds-storage-encrypted: portfolio: \u0026#34;reinvent-cloud-engineering-governance\u0026#34; product: \u0026#34;aws-config-rds-storage-encrypted\u0026#34; version: \u0026#34;v1\u0026#34; deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;    The main input field should look like this (remember to set your account_id):   accounts: - account_id: \u0026#34;\u0026lt;YOUR_ACCOUNT_ID_WITHOUT_HYPHENS\u0026gt;\u0026#34; name: \u0026#34;puppet-account\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34; launches: aws-config-desired-instance-types: portfolio: \u0026#34;reinvent-cloud-engineering-governance\u0026#34; product: \u0026#34;aws-config-desired-instance-types\u0026#34; version: \u0026#34;v1\u0026#34; parameters: InstanceType: default: \u0026#34;t2.medium, t2.large, t2.xlarge\u0026#34; deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34; aws-config-rds-storage-encrypted: portfolio: \u0026#34;reinvent-cloud-engineering-governance\u0026#34; product: \u0026#34;aws-config-rds-storage-encrypted\u0026#34; version: \u0026#34;v1\u0026#34; deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;   AWS Committing the manifest file Now that we have written the manifest file we are ready to commit it.\n Set your Author name Set your Email address Set your Commit message  Using a good / unique commit message will help you understand what is going on later.\n  Click the Commit changes button:    What did we just do? The YAML we pasted in the previous step told the framework to perform the following actions:\n provision a product named aws-config-rds-storage-encrypted into each of the enabled regions of the account  Verifying the provisioning Once you have made your changes the ServiceCatalogPuppet Pipeline should have run. If you were quick may still be running. If it has not yet started feel free to the hit the Release change button.\nOnce it has completed it should show the Source, Generate and Deploy stages in green to indicate they have completed successfully:\n  If this is failing please raise your hand for some assistance\n Once you have verified the pipeline has run you can go to Service Catalog provisioned products to view your provisioned product. Please note when you arrive at the provisioned product page you will need to select account from the filter by drop down in the top right:\n  If you cannot see your product please raise your hand for some assistance\n You have now successfully provisioned a product\nVerify the AWS Config rule is enabled To see the AWS Config rule enabled, navigate to AWS Config rules. Once there you should see the following:\n  "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/every-day-use/200-provisioning-a-product.html",
	"title": "Provisioning a product",
	"tags": [],
	"description": "",
	"content": " What are we going to do? This tutorial will walk you through \u0026ldquo;Provisioning a product\u0026rdquo; into a spoke account.\nWe will assume you have:\n installed Service Catalog Puppet correctly bootstrapped a spoke you have created a product you have created a portfolio  We are going to perform the following steps:\n create a manifest file add an account to the manifest file add a launch to the manifest file  During this process you will check your progress by verifying what the framework is doing at each step.\nStep by step guide Here are the steps you need to follow to \u0026ldquo;Provisioning a product\u0026rdquo;\nCreating the manifest file   Navigate to the ServiceCatalogPuppet CodeCommit repository\n  Scroll down to the bottom of the page and hit the Create file button\n    Adding an account to the manifest file   Copy the following snippet into the main input field:\n accounts: - account_id: \u0026#34;\u0026lt;YOUR_ACCOUNT_ID_WITHOUT_HYPHENS\u0026gt;\u0026#34; name: \u0026#34;puppet-account\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; - \u0026#34;eu-west-2\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34;     Update account_id on line to show your account id\n  Adding a launch to the manifest Now we are ready to add a product to the manifest file.\n Add the following snippet to the end of the main input field:   launches: aws-config-enable-config: portfolio: \u0026#34;reinvent-cloud-engineering-governance\u0026#34; product: \u0026#34;aws-config-enable-config\u0026#34; version: \u0026#34;v1\u0026#34; deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;    The main input field should look like this:   accounts: - account_id: \u0026#34;\u0026lt;YOUR_ACCOUNT_ID_WITHOUT_HYPHENS\u0026gt;\u0026#34; name: \u0026#34;puppet-account\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; - \u0026#34;eu-west-2\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34; launches: aws-config-enable-config: portfolio: \u0026#34;reinvent-cloud-engineering-governance\u0026#34; product: \u0026#34;aws-config-enable-config\u0026#34; version: \u0026#34;v1\u0026#34; deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;   Committing the manifest file Now that we have written the manifest file we are ready to commit it.\n  Set the File name to manifest.yaml\n  Set your Author name\n  Set your Email address\n  Set your Commit message\n  Using a good / unique commit message will help you understand what is going on later.\n  Click the Commit changes button:    Verifying the provisioning Once you have made your changes the ServiceCatalogPuppet Pipeline should have run or if you were quick may still be running. If it has not yet started feel free to the hit the Release change button.\nOnce it has completed it should show the Source and Deploy stages in green to indicate they have completed successfully:\n  Once you have verified the pipeline has run you can go to Service Catalog provisioned products to view your provisioned product. Please note when you arrive at the provisioned product page you will need to select account from the filter by drop down in the top right:\n  You have now successfully provisioned a product! When provisioned, this product will automatically enable AWS Config.\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/2021-workshop/200-provisioning-a-vpc.html",
	"title": "Provisioning a VPC",
	"tags": [],
	"description": "",
	"content": " What are we going to do? By now, you should have created a stack containing a lambda, provisioned it, invoked the lambda and then asserted the effect of the lambda invocation.\nNow the default networking resources are removed, you are ready to provision a new VPC. The following section will show you what it is like to create a VPC and storing a stack output in AWS Systems Manager Parameter Store for later use.\nHere are the steps you will need to follow:\n Creating the Stack   Provisioning the Stack   "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/design-considerations/account-tagging.html",
	"title": "Account Tagging",
	"tags": [],
	"description": "",
	"content": " What are we going to do? This article will explain how tags are used by the Service Catalog Tools and will make some recommendations on which tags you should be thinking about using.\nHow does Service Catalog Tools use account tags? When writing your manifest you can specify tags for AWS Accounts:\n accounts: - account_id: \u0026#34;012345678910\u0026#34; name: \u0026#34;prod-member-service-9\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; - \u0026#34;eu-west-2\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34;    The tags assigned to this account are type:prod and partition:eu.\nThese tags are later used in the launches and spoke-portfolio-shares sections of the manifest file to choose which AWS Accounts should have products provisioned into them and which AWS Accounts should have portfolios shared with them:\n launches: aws-config-rds-storage-encrypted: portfolio: \u0026#34;reinvent-cloud-engineering-governance\u0026#34; product: \u0026#34;aws-config-rds-storage-encrypted\u0026#34; version: \u0026#34;v1\u0026#34; deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34;  regions: \u0026#34;default_region\u0026#34; spoke-local-portfolios: cloud-engineering-self-service: portfolio: \u0026#34;reinvent-cloud-engineering-self-service\u0026#34; deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34;  regions: \u0026#34;default_region\u0026#34;   The Service Catalog Tools looks through the launches and the spoke-local-portfolios. For each launch and spoke-local-portfolio found the framework will look through the tags specified in the tags section. For each tag found the Service Catalog Tools will look through the list of accounts for an account with the same tag. When the tag is found the product is provisioned if the tag was found in the launch section otherwise the portfolio specified will be shared if the tag was found in a spoke-local-portfolio.\nHow you can make best use of them Having the right number of tags is essential. Too few tags will cause you to have less flexibility but having too many may lead to a larger than needed manifest file or feeling overwhelmed.\nTo begin with, we recommend using foundation and additional tags to align to the multi-account strategy best practice:\n outype:foundational outype:additional  We then recommend describing which OU the AWS Accounts are in:\n ou:sharedservices ou:networking ou:securityreadonly  We then recommend the following tags based on the type of the workloads that exist in the AWS account:\n type:prod type:test type:dev type:sandbox type:suspended  We then recommend using a set of scope tags to help explain the governance needs of the accounts:\n scope:pci scope:pii scope:hipaa  We may also want to classify the account by the confidentiality of the data within it\n confidentiality:highly confidentiality:medium confidentiality:public  It may also be convenient to tag the accounts with the team or business unit:\n team:ccoe team:member-services team:mobile-banking businessunit:security  The tags you specify within the manifest are not applied to the accounts using AWS Organizations - they only exist within the manifest file. You can change them at any time and renaming them will not result in changes.\nIf you would like to share your tagging patterns raise a github issue to share\n "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/every-day-use/100-creating-a-product/300-add-the-source-code.html",
	"title": "Add the source code",
	"tags": [],
	"description": "",
	"content": " What are we going to do? We are going to perform the following steps:\n Add the source code for the version of the AWS Service Catalog product we have just created  Step by step guide Here are the steps you need to follow to \u0026ldquo;Add the source code\u0026rdquo;\nAdd the source code for your product When you configured your product version, you specified the following:\n Versions: - Name: \u0026#34;v1\u0026#34; Description: \u0026#34;v1 of aws-config-enable-config\u0026#34; Active: True Source: Provider: \u0026#34;CodeCommit\u0026#34; Configuration: RepositoryName: \u0026#34;aws-config-enable-config\u0026#34; BranchName: \u0026#34;main\u0026#34;   We now need to create the AWS CodeCommit repository and add the AWS CloudFormation template we are going to use for our product into that repository.\n  Navigate to AWS CodeCommit\n  Click Create repository\n     Input the name aws-config-enable-config     Click Create     Scroll down to the bottom of the page and hit the Create file button      Copy the following snippet into the main input field:\n AWSTemplateFormatVersion: 2010-09-09 Description: | This template creates a Config Recorder and an Amazon S3 bucket where logs are published. Resources: ConfigRole: Type: \u0026#39;AWS::IAM::Role\u0026#39; Description: The IAM role used to configure AWS Config Properties: AssumeRolePolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Principal: Service: - config.amazonaws.com Action: - \u0026#39;sts:AssumeRole\u0026#39; ManagedPolicyArns: - arn:aws:iam::aws:policy/service-role/AWSConfigRole Policies: - PolicyName: root PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: \u0026#39;s3:GetBucketAcl\u0026#39; Resource: !Sub arn:aws:s3:::${S3ConfigBucket} - Effect: Allow Action: \u0026#39;s3:PutObject\u0026#39; Resource: !Sub arn:aws:s3:::${S3ConfigBucket}/AWSLogs/${AWS::AccountId}/${AWS::Region} Condition: StringEquals: \u0026#39;s3:x-amz-acl\u0026#39;: bucket-owner-full-control - Effect: Allow Action: \u0026#39;config:Put*\u0026#39; Resource: \u0026#39;*\u0026#39; ConfigRecorder: Type: \u0026#39;AWS::Config::ConfigurationRecorder\u0026#39; DependsOn: ConfigRole Properties: Name: default RoleARN: !GetAtt ConfigRole.Arn DeliveryChannel: Type: \u0026#39;AWS::Config::DeliveryChannel\u0026#39; Properties: ConfigSnapshotDeliveryProperties: DeliveryFrequency: Six_Hours S3BucketName: !Ref S3ConfigBucket S3ConfigBucket: DeletionPolicy: Retain Description: S3 bucket with AES256 Encryption set Type: AWS::S3::Bucket Properties: BucketName: !Sub config-bucket-${AWS::AccountId} PublicAccessBlockConfiguration: BlockPublicAcls: True BlockPublicPolicy: True IgnorePublicAcls: True RestrictPublicBuckets: True BucketEncryption: ServerSideEncryptionConfiguration: - ServerSideEncryptionByDefault: SSEAlgorithm: AES256 S3ConfigBucketPolicy: Type: AWS::S3::BucketPolicy Description: S3 bucket policy Properties: Bucket: !Ref S3ConfigBucket PolicyDocument: Version: 2012-10-17 Statement: - Sid: AWSBucketPermissionsCheck Effect: Allow Principal: Service: - config.amazonaws.com Action: s3:GetBucketAcl Resource: - !Sub \u0026#34;arn:aws:s3:::${S3ConfigBucket}\u0026#34; - Sid: AWSBucketDelivery Effect: Allow Principal: Service: - config.amazonaws.com Action: s3:PutObject Resource: !Sub \u0026#34;arn:aws:s3:::${S3ConfigBucket}/AWSLogs/*/*\u0026#34; Outputs: ConfigRoleArn: Value: !GetAtt ConfigRole.Arn S3ConfigBucketArn: Value: !GetAtt S3ConfigBucket.Arn     Set the File name to product.template.yaml\n  Set your Author name\n  Set your Email address\n  Set your Commit message\n  Using a good / unique commit message will help you understand what is going on later.\n Creating that file should trigger your aws-config-enable-config-v1-pipeline.\nOnce the pipeline has completed it should show the Source, Package and Deploy stages in green to indicate they have completed successfully:\n  You should see your commit message on this screen, it will help you know which version of ServiceCatalogFactory repository the pipeline is processing.\n Once you have verified the pipeline has run you can go to Service Catalog products to view your newly created version.\nClick on the product and verify v1 is there\n  You have now successfully created a version for your product!\nSpecifying source code to import If you are using AWS CodeCommit as your SCM you are able to request the framework to create the git repository for you and you can specify an AWS S3 source for where the initial commit should come from:\n - Name: \u0026#34;vpc\u0026#34; Owner: \u0026#34;networking@example.com\u0026#34; Description: \u0026#34;vpc\u0026#34; Distributor: \u0026#34;cloud-engineering\u0026#34; SupportDescription: \u0026#34;Speak to networking@example.com for help\u0026#34; SupportEmail: \u0026#34;networking@example.com\u0026#34; SupportUrl: \u0026#34;https://wiki.example.com/cloud-engineering/networking/vpc\u0026#34; Source: Provider: \u0026#34;CodeCommit\u0026#34; Configuration: RepositoryName: \u0026#34;networking-vpc\u0026#34; Code: S3: Bucket: \u0026#34;service-catalog-tools-product-sets-eu-west-2\u0026#34; Versions: - Name: \u0026#34;v1\u0026#34; Description: \u0026#34;v1 of vpc\u0026#34; Active: True Source: Configuration: BranchName: \u0026#34;v1\u0026#34; Code: S3: Key: \u0026#34;product-sets/networking/vpc/v1/networking--vpc--v1.zip\u0026#34; Portfolios: - \u0026#34;demo-portfolio\u0026#34;   In the example above we are saying the initial source code should come from the S3 bucket named service-catalog-tools-product-sets-eu-west-2 using the key product-sets/networking/vpc/v1/networking\u0026ndash;vpc\u0026ndash;v1.zip\nYou can split the declaration between the product and version as we have in the example above or you could have specified all of the configuration under the version.\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/40-reinvent2019/150-task-2/300-create-the-product.html",
	"title": "Create the product",
	"tags": [],
	"description": "",
	"content": " What are we going to do? We have provisioned a detective control to look for AWS RDS Instances that have don't have encryption enabled. We can do better, and create an AWS Service Catalog product that meets the encryption requirement by default using service catalog tools. When users create a new RDS instance using this product, encryption at rest is enabled by default and no further configuration is required.\nWe are going to perform the following steps:\n define a product with a version and a portfolio add the source code for our product share that portfolio with a spoke account  Step by step guide Here are the steps you need to follow to \u0026ldquo;Create the product\u0026rdquo;\nDefine a product with a version and a portfolio   Navigate to the ServiceCatalogFactory CodeCommit repository again\n  Click on portfolios\n     Click on reinvent.yaml     Click Edit     Add the following to the products section:   - Name: \u0026#34;rds-instance\u0026#34; Owner: \u0026#34;data-governance@example.com\u0026#34; Description: \u0026#34;A compliant RDS Instance you can use that meets data governance standards\u0026#34; Distributor: \u0026#34;cloud-engineering\u0026#34; SupportDescription: \u0026#34;Speak to data-governance@example.com about exceptions and speak to cloud-engineering@example.com about implementation issues\u0026#34; SupportEmail: \u0026#34;cloud-engineering@example.com\u0026#34; SupportUrl: \u0026#34;https://wiki.example.com/cloud-engineering/data-governance/rds-instance\u0026#34; Options: ShouldCFNNag: True Tags: - Key: \u0026#34;type\u0026#34; Value: \u0026#34;governance\u0026#34; - Key: \u0026#34;creator\u0026#34; Value: \u0026#34;cloud-engineering\u0026#34; Versions: - Name: \u0026#34;v1\u0026#34; Description: \u0026#34;v1 of rds-instance\u0026#34; Active: True Source: Provider: \u0026#34;CodeCommit\u0026#34; Configuration: RepositoryName: \u0026#34;rds-instance\u0026#34; BranchName: \u0026#34;main\u0026#34; Portfolios: - \u0026#34;cloud-engineering-self-service\u0026#34;    Add the following to the portfolios section:   - DisplayName: \u0026#34;cloud-engineering-self-service\u0026#34; Description: \u0026#34;Portfolio containing products that you can use to ensure you meet the governance guidelines\u0026#34; ProviderName: \u0026#34;cloud-engineering\u0026#34; Associations: - \u0026#34;arn:aws:iam::${AWS::AccountId}:role/TeamRole\u0026#34; Tags: - Key: \u0026#34;type\u0026#34; Value: \u0026#34;governance\u0026#34; - Key: \u0026#34;creator\u0026#34; Value: \u0026#34;cloud-engineering\u0026#34;    Once completed it should like look this:   Schema: factory-2019-04-01 Products: - Name: \u0026#34;aws-config-desired-instance-types\u0026#34; Owner: \u0026#34;budget-and-cost-governance@example.com\u0026#34; Description: \u0026#34;Enables AWS Config rule - desired-instance-type with our RIs\u0026#34; Distributor: \u0026#34;cloud-engineering\u0026#34; SupportDescription: \u0026#34;Speak to budget-and-cost-governance@example.com about exceptions and speak to cloud-engineering@example.com about implementation issues\u0026#34; SupportEmail: \u0026#34;cloud-engineering@example.com\u0026#34; SupportUrl: \u0026#34;https://wiki.example.com/cloud-engineering/budget-and-cost-governance/aws-config-desired-instance-types\u0026#34; Tags: - Key: \u0026#34;type\u0026#34; Value: \u0026#34;governance\u0026#34; - Key: \u0026#34;creator\u0026#34; Value: \u0026#34;cloud-engineering\u0026#34; - Key: \u0026#34;cost-center\u0026#34; Value: \u0026#34;governance\u0026#34; Versions: - Name: \u0026#34;v1\u0026#34; Description: \u0026#34;v1 of aws-config-desired-instance-types\u0026#34; Active: True Source: Provider: \u0026#34;CodeCommit\u0026#34; Configuration: RepositoryName: \u0026#34;aws-config-desired-instance-types\u0026#34; BranchName: \u0026#34;main\u0026#34; Portfolios: - \u0026#34;cloud-engineering-governance\u0026#34; - Name: \u0026#34;aws-config-rds-storage-encrypted\u0026#34; Owner: \u0026#34;data-governance@example.com\u0026#34; Description: \u0026#34;Enables AWS Config rule - aws-config-rds-storage-encrypted\u0026#34; Distributor: \u0026#34;cloud-engineering\u0026#34; SupportDescription: \u0026#34;Speak to data-governance@example.com about exceptions and speak to cloud-engineering@example.com about implementation issues\u0026#34; SupportEmail: \u0026#34;cloud-engineering@example.com\u0026#34; SupportUrl: \u0026#34;https://wiki.example.com/cloud-engineering/data-governance/aws-config-rds-storage-encrypted\u0026#34; Tags: - Key: \u0026#34;type\u0026#34; Value: \u0026#34;governance\u0026#34; - Key: \u0026#34;creator\u0026#34; Value: \u0026#34;cloud-engineering\u0026#34; - Key: \u0026#34;cost-center\u0026#34; Value: \u0026#34;governance\u0026#34; Versions: - Name: \u0026#34;v1\u0026#34; Description: \u0026#34;v1 of aws-config-rds-storage-encrypted\u0026#34; Active: True Source: Provider: \u0026#34;CodeCommit\u0026#34; Configuration: RepositoryName: \u0026#34;aws-config-rds-storage-encrypted\u0026#34; BranchName: \u0026#34;main\u0026#34; Portfolios: - \u0026#34;cloud-engineering-governance\u0026#34; - Name: \u0026#34;rds-instance\u0026#34; Owner: \u0026#34;data-governance@example.com\u0026#34; Description: \u0026#34;A compliant RDS Instance you can use that meets data governance standards\u0026#34; Distributor: \u0026#34;cloud-engineering\u0026#34; SupportDescription: \u0026#34;Speak to data-governance@example.com about exceptions and speak to cloud-engineering@example.com about implementation issues\u0026#34; SupportEmail: \u0026#34;cloud-engineering@example.com\u0026#34; SupportUrl: \u0026#34;https://wiki.example.com/cloud-engineering/data-governance/rds-instance\u0026#34; Options: ShouldCFNNag: True  Tags: - Key: \u0026#34;type\u0026#34; Value: \u0026#34;governance\u0026#34; - Key: \u0026#34;creator\u0026#34; Value: \u0026#34;cloud-engineering\u0026#34; Versions: - Name: \u0026#34;v1\u0026#34; Description: \u0026#34;v1 of rds-instance\u0026#34; Active: True Source: Provider: \u0026#34;CodeCommit\u0026#34; Configuration: RepositoryName: \u0026#34;rds-instance\u0026#34; BranchName: \u0026#34;main\u0026#34; Portfolios: - \u0026#34;cloud-engineering-self-service\u0026#34; Portfolios: - DisplayName: \u0026#34;cloud-engineering-governance\u0026#34; Description: \u0026#34;Portfolio containing the products needed to govern AWS accounts\u0026#34; ProviderName: \u0026#34;cloud-engineering\u0026#34; Associations: - \u0026#34;arn:aws:iam::${AWS::AccountId}:role/TeamRole\u0026#34; Tags: - Key: \u0026#34;type\u0026#34; Value: \u0026#34;governance\u0026#34; - Key: \u0026#34;creator\u0026#34; Value: \u0026#34;cloud-engineering\u0026#34; - Key: \u0026#34;cost-center\u0026#34; Value: \u0026#34;governance\u0026#34; - DisplayName: \u0026#34;cloud-engineering-self-service\u0026#34; Description: \u0026#34;Portfolio containing products that you can use to ensure you meet the governance guidelines\u0026#34; ProviderName: \u0026#34;cloud-engineering\u0026#34; Associations: - \u0026#34;arn:aws:iam::${AWS::AccountId}:role/TeamRole\u0026#34; Tags: - Key: \u0026#34;type\u0026#34; Value: \u0026#34;governance\u0026#34; - Key: \u0026#34;creator\u0026#34; Value: \u0026#34;cloud-engineering\u0026#34;   Have a look at the highlighted lines. We are using this to turn on cfn-nag, an open source tool by Stelligent that looks for insecure configuration of resources. This will add an extra layer of governance ensuring the AWS CloudFormation templates we are using meets the quality bar set by cfn-nag.\n   Set your Author name\n  Set your Email address\n  Set your Commit message\n  Using a good / unique commit message will help you understand what is going on later.\n  Click the Commit changes button:    What did we just do? The YAML we pasted in the previous step told the framework to perform several actions:\n create a product named rds-instance add a v1 of our product create a portfolio named cloud-engineering-self-service add the product: rds-instance to the portfolio: cloud-engineering-self-service  Verify the change worked Once you have made your changes the ServiceCatalogFactory Pipeline should have run. If you were very quick, the pipeline may still be running. If it has not yet started feel free to the hit the Release change button.\nOnce it has completed it should show the Source and Build stages in green to indicate they have completed successfully:\n  If this is failing please raise your hand for some assistance\n Add the source code for our product When you configured your product version, you specified the following version:\n Versions: - Name: \u0026#34;v1\u0026#34; Description: \u0026#34;v1 of rds-instance\u0026#34; Active: True Source: Provider: \u0026#34;CodeCommit\u0026#34; Configuration: RepositoryName: \u0026#34;rds-instance\u0026#34; BranchName: \u0026#34;main\u0026#34;   This tells the framework the source code for the product comes from the main branch of a CodeCommit repository of the name rds-instance.\nWe now need to create the CodeCommit repository and add the AWS CloudFormation template we are going to use for our product.\n  Navigate to AWS CodeCommit\n  Click Create repository\n     Input the name rds-instance     Click Create     Scroll down to the bottom of the page and hit the Create file button     Copy the following snippet into the main input field:   AWSTemplateFormatVersion: 2010-09-09 Description: \u0026#34;RDS Storage Encrypted\u0026#34; Parameters: RdsDbMasterUsername: Description: RdsDbMasterUsername Type: String Default: someuser RdsDbMasterUserPassword: Description: RdsDbMasterUserPassword Type: String NoEcho: true RdsDbDatabaseName: Description: DbDatabaseName Type: String Default: mysql57_database Resources: VPC: Type: AWS::EC2::VPC Properties: CidrBlock: 10.0.0.0/16 EnableDnsSupport: \u0026#39;false\u0026#39; EnableDnsHostnames: \u0026#39;false\u0026#39; Subnet1: Type: AWS::EC2::Subnet Properties: VpcId: Ref: VPC CidrBlock: 10.0.0.0/24 AvailabilityZone: !Select [0, !GetAZs {Ref: \u0026#39;AWS::Region\u0026#39;}] Subnet2: Type: AWS::EC2::Subnet Properties: VpcId: Ref: VPC CidrBlock: 10.0.1.0/24 AvailabilityZone: !Select [1, !GetAZs {Ref: \u0026#39;AWS::Region\u0026#39;}] RdsDbSubnetGroup: Type: AWS::RDS::DBSubnetGroup Properties: DBSubnetGroupDescription: Database subnets for RDS SubnetIds: - !Ref Subnet1 - !Ref Subnet2 RdsSecurityGroup: Type: AWS::EC2::SecurityGroup Description: Used to grant access to and from the VPC Properties: VpcId: !Ref VPC GroupDescription: Allow MySQL (TCP3306) access to and from the VPC SecurityGroupIngress: - IpProtocol: tcp FromPort: 3306 ToPort: 3306 CidrIp: 10.0.0.0/32 SecurityGroupEgress: - IpProtocol: tcp FromPort: 3306 ToPort: 3306 CidrIp: 10.0.0.0/32 RdsDbClusterParameterGroup: Type: AWS::RDS::DBClusterParameterGroup Properties: Description: CloudFormation Aurora Cluster Parameter Group Family: aurora-mysql5.7 Parameters: server_audit_logging: 0 server_audit_events: \u0026#39;CONNECT,QUERY,QUERY_DCL,QUERY_DDL,QUERY_DML,TABLE\u0026#39; RdsDbCluster: Type: AWS::RDS::DBCluster Properties: DBSubnetGroupName: !Ref RdsDbSubnetGroup MasterUsername: !Ref RdsDbMasterUsername MasterUserPassword: !Ref RdsDbMasterUserPassword DatabaseName: !Ref RdsDbDatabaseName Engine: aurora-mysql VpcSecurityGroupIds: - !Ref RdsSecurityGroup DBClusterIdentifier : !Sub \u0026#39;${AWS::StackName}-dbcluster\u0026#39; DBClusterParameterGroupName: !Ref RdsDbClusterParameterGroup PreferredBackupWindow: 18:05-18:35 RdsDbParameterGroup: Type: AWS::RDS::DBParameterGroup Properties: Description: CloudFormation Aurora Parameter Group Family: aurora-mysql5.7 Parameters: aurora_lab_mode: 0 general_log: 1 slow_query_log: 1 long_query_time: 10 RdsDbInstance: Type: AWS::RDS::DBInstance Properties: DBSubnetGroupName: !Ref RdsDbSubnetGroup DBParameterGroupName: !Ref RdsDbParameterGroup Engine: aurora-mysql DBClusterIdentifier: !Ref RdsDbCluster AutoMinorVersionUpgrade: \u0026#39;true\u0026#39; PubliclyAccessible: \u0026#39;false\u0026#39; PreferredMaintenanceWindow: Thu:19:05-Thu:19:35 AvailabilityZone: !Select [0, !GetAZs {Ref: \u0026#39;AWS::Region\u0026#39;}] DBInstanceClass: \u0026#39;db.t2.small\u0026#39;     Set the File name to product.template.yaml\n  Set your Author name\n  Set your Email address\n  Set your Commit message\n  Click Commit changes\n  Using a good / unique commit message will help you understand what is going on later.\n Creating that file should trigger your rds-instance-v1-pipeline.\nOnce the pipeline has completed it should show the Source stage in green to indicate it has completed successfully but it should show the CFNNag action within the Tests stage as failing:\n  Clicking the Details link within the CFNNag box will bring you to the AWS CodeBuild project. When you scroll near to the bottom of that page you should see an error:\n·[0;31;49m| FAIL F26·[0m ·[0;31;49m|·[0m ·[0;31;49m| Resources: [\u0026#34;RdsDbCluster\u0026#34;]·[0m ·[0;31;49m| Line Numbers: [84]·[0m ·[0;31;49m|·[0m ·[0;31;49m| RDS DBCluster should have StorageEncrypted enabled·[0m CFNNag has determined you are not applying encryption to your DBCluster. This is a violation of the data governance guidelines and so we need to fix it.\n  Go to AWS CodeCommit\n  Click on the rds-instance repository\n  Click on product.template.yaml\n  Click on edit\n  Replace the contents with this:\n   AWSTemplateFormatVersion: 2010-09-09 Description: \u0026#34;RDS Storage Encrypted\u0026#34; Parameters: RdsDbMasterUsername: Description: RdsDbMasterUsername Type: String Default: someuser RdsDbMasterUserPassword: Description: RdsDbMasterUserPassword Type: String NoEcho: true RdsDbDatabaseName: Description: DbDatabaseName Type: String Default: mysql57_database Resources: VPC: Type: AWS::EC2::VPC Properties: CidrBlock: 10.0.0.0/16 EnableDnsSupport: \u0026#39;false\u0026#39; EnableDnsHostnames: \u0026#39;false\u0026#39; Subnet1: Type: AWS::EC2::Subnet Properties: VpcId: Ref: VPC CidrBlock: 10.0.0.0/24 AvailabilityZone: !Select [0, !GetAZs {Ref: \u0026#39;AWS::Region\u0026#39;}] Subnet2: Type: AWS::EC2::Subnet Properties: VpcId: Ref: VPC CidrBlock: 10.0.1.0/24 AvailabilityZone: !Select [1, !GetAZs {Ref: \u0026#39;AWS::Region\u0026#39;}] RdsDbSubnetGroup: Type: AWS::RDS::DBSubnetGroup Properties: DBSubnetGroupDescription: Database subnets for RDS SubnetIds: - !Ref Subnet1 - !Ref Subnet2 RdsSecurityGroup: Type: AWS::EC2::SecurityGroup Description: Used to grant access to and from the VPC Properties: VpcId: !Ref VPC GroupDescription: Allow MySQL (TCP3306) access to and from the VPC SecurityGroupIngress: - IpProtocol: tcp FromPort: 3306 ToPort: 3306 CidrIp: 10.0.0.0/32 SecurityGroupEgress: - IpProtocol: tcp FromPort: 3306 ToPort: 3306 CidrIp: 10.0.0.0/32 RdsDbClusterParameterGroup: Type: AWS::RDS::DBClusterParameterGroup Properties: Description: CloudFormation Aurora Cluster Parameter Group Family: aurora-mysql5.7 Parameters: server_audit_logging: 0 server_audit_events: \u0026#39;CONNECT,QUERY,QUERY_DCL,QUERY_DDL,QUERY_DML,TABLE\u0026#39; RdsDbCluster: Type: AWS::RDS::DBCluster Properties: DBSubnetGroupName: !Ref RdsDbSubnetGroup MasterUsername: !Ref RdsDbMasterUsername MasterUserPassword: !Ref RdsDbMasterUserPassword DatabaseName: !Ref RdsDbDatabaseName Engine: aurora-mysql VpcSecurityGroupIds: - !Ref RdsSecurityGroup DBClusterIdentifier : !Sub \u0026#39;${AWS::StackName}-dbcluster\u0026#39; DBClusterParameterGroupName: !Ref RdsDbClusterParameterGroup PreferredBackupWindow: 18:05-18:35 StorageEncrypted: True  RdsDbParameterGroup: Type: AWS::RDS::DBParameterGroup Properties: Description: CloudFormation Aurora Parameter Group Family: aurora-mysql5.7 Parameters: aurora_lab_mode: 0 general_log: 1 slow_query_log: 1 long_query_time: 10 RdsDbInstance: Type: AWS::RDS::DBInstance Properties: DBSubnetGroupName: !Ref RdsDbSubnetGroup DBParameterGroupName: !Ref RdsDbParameterGroup Engine: aurora-mysql DBClusterIdentifier: !Ref RdsDbCluster AutoMinorVersionUpgrade: \u0026#39;true\u0026#39; PubliclyAccessible: \u0026#39;false\u0026#39; PreferredMaintenanceWindow: Thu:19:05-Thu:19:35 AvailabilityZone: !Select [0, !GetAZs {Ref: \u0026#39;AWS::Region\u0026#39;}] DBInstanceClass: \u0026#39;db.t2.small\u0026#39; StorageEncrypted: True    Please observe the highlighted lines showing where we have made a change. We have added:\nStorageEncrypted: True  Set your Author name Set your Email address Set your Commit message Click Commit changes  Using a good / unique commit message will help you understand what is going on later.\n Creating that file should trigger your rds-instance-v1-pipeline.\nOnce the pipeline has completed it should show the Source and Tests stages in green to indicate they have completed successfully:\n  You should see your commit message on this screen, it will help you know which version of ServiceCatalogFactory repository the pipeline is processing.\n If this is failing please raise your hand for some assistance\n Once you have verified the pipeline has run you can go to Service Catalog products to view your newly created version.\nYou should see the product you created listed:\n  Click on the product and verify v1 is there\n  If you cannot see your version please raise your hand for some assistance\n You have now successfully created a version for your product!\nVerify that the product was added to the portfolio Now that you have verified the pipeline has run you can go to Service Catalog portfolios to view your portfolio.\n Click on cloud-engineering-self-service      Click on the product rds-instance\n  Click on the version v1\n    Share portfolio with a spoke account   Navigate to the ServiceCatalogPuppet CodeCommit repository again\n  Click on manifest.yaml\n  Click Edit\n     Append the following snippet to the YAML document in the main input field (be careful with your indentation):   spoke-local-portfolios: cloud-engineering-self-service: portfolio: \u0026#34;reinvent-cloud-engineering-self-service\u0026#34; deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;    The main input field should look like this:   accounts: - account_id: \u0026#34;\u0026lt;YOUR_ACCOUNT_ID_WITHOUT_HYPHENS\u0026gt;\u0026#34; name: \u0026#34;puppet-account\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34; launches: aws-config-desired-instance-types: portfolio: \u0026#34;reinvent-cloud-engineering-governance\u0026#34; product: \u0026#34;aws-config-desired-instance-types\u0026#34; version: \u0026#34;v1\u0026#34; parameters: InstanceType: default: \u0026#34;t2.medium, t2.large, t2.xlarge\u0026#34; deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34; aws-config-rds-storage-encrypted: portfolio: \u0026#34;reinvent-cloud-engineering-governance\u0026#34; product: \u0026#34;aws-config-rds-storage-encrypted\u0026#34; version: \u0026#34;v1\u0026#34; deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34; spoke-local-portfolios: cloud-engineering-self-service: portfolio: \u0026#34;reinvent-cloud-engineering-self-service\u0026#34; deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;   Committing the manifest file   Set your Author name\n  Set your Email address\n  Set your Commit message\n  Using a good / unique commit message will help you understand what is going on later.\n  Click the Commit changes button:    Verifying the sharing Once you have made your changes the ServiceCatalogPuppet Pipeline should have run. If you were quick may still be running. If it has not yet started feel free to the hit the Release change button.\nOnce it has completed it should show the Source and Build stages in green to indicate they have completed successfully:\n  If this is failing please raise your hand for some assistance\n Once you have verified the pipeline has run you can go to Service Catalog portfolios to view your shared product.\nWhen you share a portfolio the framework will decide if it should share the portfolio. If the target account is the same as the factory account it will not share the portfolio as it is not needed.\nIf you cannot see your product please raise your hand for some assistance\n "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/every-day-use/110-deleting-a-product/300-deleting-the-product.html",
	"title": "Deleting the product",
	"tags": [],
	"description": "",
	"content": " What are we going to do? We are going to perform the following steps:\n delete a product version delete a product  Step by step guide Here are the steps you need to follow to \u0026ldquo;Deleting the product\u0026rdquo;\nDelete a product version When you are ready to delete a product version you will need to edit its definition in the portfolio yaml.\n  Navigate to the ServiceCatalogFactory CodeCommit repository\n  Click on portfolios\n      Click on the portfolio yaml containing your product\n  Click Edit\n  Add or set the attribute Status for the version you want to delete to terminated:\n Schema: factory-2019-04-01 Products: - Name: account-vending-machine Owner: central-it@customer.com Description: The iam roles needed for you to do your jobs Distributor: central-it-team SupportDescription: Contact us on Chime for help #central-it-team SupportEmail: central-it-team@customer.com SupportUrl: https://wiki.customer.com/central-it-team/self-service/account-iam  Tags: - Key: product-type Value: iam Versions: - Name: v1 Description: The iam roles needed for you to do your jobs Status: terminated  Source: Provider: CodeCommit Configuration: RepositoryName: account-vending-machine BranchName: v1     Set your Author name\n  Set your Email address\n  Set your Commit message\n  Click the Commit changes button:\n    When the framework runs the product version will be deleted. This change will only affect the version of the product in your factory account. If you are using the imported product in your spoke accounts it will have affect there otherwise you will need to run service-catalog-puppet to cascade the change.\nYou can verify this by navigating to Service Catalog and checking your disabled version is removed.\nDelete a product When you are ready to delete a product you will need to edit its definition in the portfolio yaml.\n  Navigate to the ServiceCatalogFactory CodeCommit repository\n  Click on portfolios\n      Click on the portfolio yaml containing your product\n  Click Edit\n  Add or set the attribute Status for the product you want to delete to terminated:\n Schema: factory-2019-04-01 Products: - Name: account-vending-machine Status: terminated  Owner: central-it@customer.com Description: The iam roles needed for you to do your jobs Distributor: central-it-team SupportDescription: Contact us on Chime for help #central-it-team SupportEmail: central-it-team@customer.com SupportUrl: https://wiki.customer.com/central-it-team/self-service/account-iam  Tags: - Key: product-type Value: iam Versions: - Name: v1 Description: The iam roles needed for you to do your jobs Source: Provider: CodeCommit Configuration: RepositoryName: account-vending-machine BranchName: v1     Set your Author name\n  Set your Email address\n  Set your Commit message\n  Click the Commit changes button:\n    When the framework runs the product will be deleted. This change will only affect the version of the product in your factory account. If you are using the imported product in your spoke accounts it will have affect there otherwise you will need to run service-catalog-puppet to cascade the change. If you are using imported products in your spokes then the product will be deleted there.\nYou can verify this by navigating to Service Catalog and checking your disabled version is removed.\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/2021-workshop/300-provisioning-subnets.html",
	"title": "Provisioning Subnets",
	"tags": [],
	"description": "",
	"content": " What are we going to do? By now, you will have removed the default networking resources and provisioned a new VPC. You are now ready to start creating some subnets.\nWe are going to create a subnet by provisioning an AWS Service Catalog product using a launch and then we are going to provision another subnet using Hashicorp Terraform using a workspace.\nHere are the steps to do this:\n Using AWS Service Catalog   Using Terraform   "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/every-day-use/300-sharing-a-portfolio.html",
	"title": "Sharing a portfolio",
	"tags": [],
	"description": "",
	"content": " What are we going to do? This will talk you through the different options you have for sharing a portfolio from a hub account to a spoke account. The different options have different limitations so it is good to read this before you set up sharing. Changing between the different options can affect the user experience of the people using the shared portfolio.\nSpoke Local Portfolios Spoke-local-portfolios was the first way you could share a portfolio using this solution. With spoke-local-portfolios a copy of the portfolio you created in the hub account is created in the spoke and the products are either copied or imported into the spoke portfolio. If you use imported products a change to the product in the hub cascades to the spoke whereas if you copied the product you will need to run this solution to sync the change. With spoke-local-portfolios you can create launch role constraints and resource update constraints in the spoke duplicated portfolio. You can use AWS Organizations for sharing or you can share between directly between accounts. If want to use different launch role constraints per spoke without having to create multiple portfolios this is the route for you.\nIn order to create a spoke-local-portfolio 11 tasks are created and then executed:\n  Imported Portfolios Imported-portfolios is the easiest way to share a portfolio using this solution. With imported-portfolios the portfolio you created in the hub account is shared with the spoke account. Changes made to the portfolio hub are reflected within the spoke account. With imported-portfolios you can create launch role constraints and resource update constraints in hub account and their affects are applied within the spoke accounts also. You can use AWS Organizations for sharing or you can share between directly between accounts. If want to use different launch role constraints per spoke without having to create multiple portfolios you should use spoke-local-portfolios instead.\nIn order to create an imported-portfolio 6 tasks are created and then executed:\n  Summary It takes more time for this solution to create spoke-local-portfolios than it does to create imported-portfolios. If you need to customise your portfolio constraints per spoke then you will either need to create duplicate portfolios when using imported-portfolios of you can use spoke-local-portfolios. Spoke-local-portfolios require more effort to keep in sync.\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/every-day-use/302-imported-portfolios.html",
	"title": "Imported Portfolios",
	"tags": [],
	"description": "",
	"content": " What are we going to do? This tutorial will walk you through how to use \u0026ldquo;Imported Portfolios\u0026rdquo;\nWe will assume you have:\n installed Service Catalog Puppet correctly bootstrapped a spoke you have created a product you have created a portfolio  We are going to perform the following steps:\n create a manifest file add an account to the manifest file add a imported-portfolios to the manifest file  During this process you will check your progress by verifying what the framework is doing at each step.\nStep by step guide Here are the steps you need to follow to \u0026ldquo;Imported Portfolios\u0026rdquo;\nCreating the manifest file   Navigate to the ServiceCatalogPuppet CodeCommit repository\n  Scroll down to the bottom of the page and hit the Create file button\n    Adding an account to the manifest file We will start out by adding your account to the manifest file.\n  Navigate to the ServiceCatalogPuppet CodeCommit repository\n  Scroll down to the bottom of the page and hit the Create file button\n      Copy the following snippet into the main input field:\n accounts: - account_id: \u0026#34;\u0026lt;YOUR_ACCOUNT_ID_WITHOUT_HYPHENS\u0026gt;\u0026#34; name: \u0026#34;puppet-account\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; - \u0026#34;eu-west-2\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34;     Update account_id on line to show your account id\n  Adding imported-portfolios to the manifest Now we are ready to add an imported portfolio to the manifest file.\n Add the following snippet to the end of the main input field:   imported-portfolios: account-vending-for-spokes: portfolio: reinvent-cloud-engineering-governance shared_with: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;    The main input field should look like this:   accounts: - account_id: \u0026#34;\u0026lt;YOUR_ACCOUNT_ID_WITHOUT_HYPHENS\u0026gt;\u0026#34; name: \u0026#34;puppet-account\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; - \u0026#34;eu-west-2\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34; imported-portfolios: account-vending-for-spokes: portfolio: reinvent-cloud-engineering-governance share_with: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;   Committing the manifest file Now that we have written the manifest file we are ready to commit it.\n  Set the File name to manifest.yaml\n  Set your Author name\n  Set your Email address\n  Set your Commit message\n  Using a good / unique commit message will help you understand what is going on later.\n  Click the Commit changes button:    Verifying the sharing Once you have made your changes the ServiceCatalogPuppet Pipeline should have run or if you were quick may still be running. If it has not yet started feel free to the hit the Release change button.\nOnce it has completed it should show the Source and Deploy stages in green to indicate they have completed successfully:\n  Once you have verified the pipeline has run you can go to Service Catalog portfolios to view your shared product.\nWhen you share a portfolio the framework will decide if it should share the portfolio. If the target account is the same as the factory account it will not share the portfolio as it is not needed.\nAdding Associations In order to use an AWS Service Catalog portfolio as an end user (to provision products) you will need to have an association added to the portfolio. Associations can be for groups, users or roles. To add an association you need to follow the example below:\n accounts: - account_id: \u0026#34;\u0026lt;YOUR_ACCOUNT_ID_WITHOUT_HYPHENS\u0026gt;\u0026#34; name: \u0026#34;puppet-account\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; - \u0026#34;eu-west-2\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34; imported-portfolios: account-vending-for-spokes: portfolio: reinvent-cloud-engineering-governance associations: - arn:aws:iam::${AWS::AccountId}:role/MyServiceCatalogAdminRole share_with: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;   Please note the solution will replace ${AWS::AccountId} with the account id of the target account where the portfolio will be share to. You can add more than one association but you cannot add the same association twice. If you are wanting to add an association for you as the maintainer we recommend you add this in the portfolio/*.yaml files but if you are adding associations for people that will be using the portfolio we recommend adding them here in manifest.\nWhen using associations it may be useful for your imported portfolio to depend_on a stack or launch that provisions the specified role into the spoke account.\nIf you are using service AWS Control Tower and/or AWS SSO the IAM role that gets created in the spoke accounts is not deterministic. You can use a wildcard in the association to work around this:\n accounts: - account_id: \u0026#34;\u0026lt;YOUR_ACCOUNT_ID_WITHOUT_HYPHENS\u0026gt;\u0026#34; name: \u0026#34;puppet-account\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; - \u0026#34;eu-west-2\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34; imported-portfolios: account-vending-for-spokes: portfolio: reinvent-cloud-engineering-governance associations: - arn:aws:iam::${AWS::AccountId}:role/IDoNotKnowTheSuffix-* share_with: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;   Migrating from spoke-local-portfolios If you are migrating from spoke-local-portfolios to imported-portfolios it is important to understand the differences between the two options which is documented in the sharing a portfolio section.\nIf you are migrating from a spoke-local-portfolio you should create a new imported-portfolio copying the contents of the spoke-local-portfolios you are migrating from:\nFor example, if you had a spoke-local-portfolio of:\n accounts: - account_id: \u0026#34;\u0026lt;YOUR_ACCOUNT_ID_WITHOUT_HYPHENS\u0026gt;\u0026#34; name: \u0026#34;puppet-account\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; - \u0026#34;eu-west-2\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34; spoke-local-portfolios: account-vending-for-spokes: portfolio: reinvent-cloud-engineering-governance associations: - arn:aws:iam::${AWS::AccountId}:role/IDoNotKnowTheSuffix-* share_with: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;   you should have an imported-portfolio of:\n accounts: - account_id: \u0026#34;\u0026lt;YOUR_ACCOUNT_ID_WITHOUT_HYPHENS\u0026gt;\u0026#34; name: \u0026#34;puppet-account\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; - \u0026#34;eu-west-2\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34; imported-portfolios: account-vending-for-spokes: portfolio: reinvent-cloud-engineering-governance associations: - arn:aws:iam::${AWS::AccountId}:role/IDoNotKnowTheSuffix-* share_with: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;   If you want to remove the spoke-local-portfolio you should set its status to terminated and have it depend on your imported-portfolio. Once you have migrated to an imported portfolio and deleted the spoke local portfolio you can remove the spoke-local-portfolio from the manifest file.\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/every-day-use/303-spoke-local-portfolios.html",
	"title": "Spoke Local Portfolios",
	"tags": [],
	"description": "",
	"content": " What are we going to do? This tutorial will walk you through how to use \u0026ldquo;Spoke Local Portfolios\u0026rdquo;\nWe will assume you have:\n installed Service Catalog Puppet correctly bootstrapped a spoke you have created a product you have created a portfolio  We are going to perform the following steps:\n create a manifest file add an account to the manifest file add a spoke-local-portfolios to the manifest file  During this process you will check your progress by verifying what the framework is doing at each step.\nStep by step guide Here are the steps you need to follow to \u0026ldquo;Spoke Local Portfolios\u0026rdquo;\nCreating the manifest file   Navigate to the ServiceCatalogPuppet CodeCommit repository\n  Scroll down to the bottom of the page and hit the Create file button\n    Adding an account to the manifest file We will start out by adding your account to the manifest file.\n  Navigate to the ServiceCatalogPuppet CodeCommit repository\n  Scroll down to the bottom of the page and hit the Create file button\n      Copy the following snippet into the main input field:\n accounts: - account_id: \u0026#34;\u0026lt;YOUR_ACCOUNT_ID_WITHOUT_HYPHENS\u0026gt;\u0026#34; name: \u0026#34;puppet-account\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; - \u0026#34;eu-west-2\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34;     Update account_id on line to show your account id\n  Adding spoke-local-portfolio to the manifest Now we are ready to add a spoke-local-portfolio to the manifest file.\n Add the following snippet to the end of the main input field:   spoke-local-portfolios: account-vending-for-spokes: portfolio: reinvent-cloud-engineering-governance share_with: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;    The main input field should look like this:   accounts: - account_id: \u0026#34;\u0026lt;YOUR_ACCOUNT_ID_WITHOUT_HYPHENS\u0026gt;\u0026#34; name: \u0026#34;puppet-account\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; - \u0026#34;eu-west-2\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34; spoke-local-portfolios: account-vending-for-spokes: portfolio: reinvent-cloud-engineering-governance share_with: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;   Committing the manifest file Now that we have written the manifest file we are ready to commit it.\n  Set the File name to manifest.yaml\n  Set your Author name\n  Set your Email address\n  Set your Commit message\n  Using a good / unique commit message will help you understand what is going on later.\n  Click the Commit changes button:    Verifying the sharing Once you have made your changes the ServiceCatalogPuppet Pipeline should have run or if you were quick may still be running. If it has not yet started feel free to the hit the Release change button.\nOnce it has completed it should show the Source and Deploy stages in green to indicate they have completed successfully:\n  Once you have verified the pipeline has run you can go to Service Catalog portfolios to view your shared product.\nWhen you share a portfolio the framework will decide if it should share the portfolio. If the target account is the same as the factory account it will not share the portfolio as it is not needed.\nAdding Associations In order to use an AWS Service Catalog portfolio as an end user (to provision products) you will need to have an association added to the portfolio. Associations can be for groups, users or roles. To add an association you need to follow the example below:\n accounts: - account_id: \u0026#34;\u0026lt;YOUR_ACCOUNT_ID_WITHOUT_HYPHENS\u0026gt;\u0026#34; name: \u0026#34;puppet-account\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; - \u0026#34;eu-west-2\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34; spoke-local-portfolios: account-vending-for-spokes: portfolio: reinvent-cloud-engineering-governance associations: - arn:aws:iam::${AWS::AccountId}:role/MyServiceCatalogAdminRole share_with: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;   Please note the solution will replace ${AWS::AccountId} with the account id of the target account where the portfolio will be share to. You can add more than one association but you cannot add the same association twice. If you are wanting to add an association for you as the maintainer we recommend you add this in the portfolio/*.yaml files but if you are adding associations for people that will be using the portfolio we recommend adding them here in manifest.\nWhen using associations it may be useful for your spoke local portfolio to depend_on a stack or launch that provisions the specified role into the spoke account.\nIf you are using service AWS Control Tower and/or AWS SSO the IAM role that gets created in the spoke accounts is not deterministic. You can use a wildcard in the association to work around this:\n accounts: - account_id: \u0026#34;\u0026lt;YOUR_ACCOUNT_ID_WITHOUT_HYPHENS\u0026gt;\u0026#34; name: \u0026#34;puppet-account\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; - \u0026#34;eu-west-2\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34; spoke-local-portfolios: account-vending-for-spokes: portfolio: reinvent-cloud-engineering-governance associations: - arn:aws:iam::${AWS::AccountId}:role/IDoNotKnowTheSuffix-* share_with: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;   Adding Launch Role Constraints When AWS Service Catalog provisions a product from a portfolio the resources will be created by the principal who invoked the provision product command. This means that principal will need to have all of the permissions required to create each of the resources specified in the product. If you would like to delegate the creation of the resources to a service role you can do so using a launch role constraint.\nLaunch role constraints specify for a given portfolio, the specified products will be provisioned as service role - which needs to be assumable by the Service Catalog service.\nHere is an example of to configure this:\n accounts: - account_id: \u0026#34;\u0026lt;YOUR_ACCOUNT_ID_WITHOUT_HYPHENS\u0026gt;\u0026#34; name: \u0026#34;puppet-account\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; - \u0026#34;eu-west-2\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34; spoke-local-portfolios: account-vending-for-spokes: portfolio: reinvent-cloud-engineering-governance constraints: launch: - products: \u0026#34;VPC-*\u0026#34; roles: - arn:aws:iam::${AWS::AccountId}:role/MyServiceCatalogAdminRole share_with: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;   Please note the solution will replace ${AWS::AccountId} with the account id of the target account where the portfolio will be share to.\nWhen you specify VPC-* as the products value the solution will look for all products that match the wildcard and then set up a launch role constraint.\nWhen using launch role constraints it may be useful for your spoke local portfolio to depend_on a stack or launch that provisions the specified role into the spoke account.\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/every-day-use/100-creating-a-product/305-creating-codestar-pipelines.html",
	"title": "Creating CodeStar pipelines",
	"tags": [],
	"description": "",
	"content": " What are we going to do? We are going to perform the following steps:\n How to use AWS CodeStar Connections as a source for your pipelines (for Github.com/Github Enterprise and BitBucket Cloud)  Step by step guide Here are the steps you need to follow to \u0026ldquo;Creating CodeStar pipelines\u0026rdquo;\nAdd the source code for your product When you configured your product version, you may have specified the following:\n Versions: - Name: \u0026#34;v1\u0026#34; Description: \u0026#34;v1 of aws-config-enable-config\u0026#34; Source: Provider: \u0026#34;CodeCommit\u0026#34; Configuration: RepositoryName: \u0026#34;aws-config-enable-config\u0026#34; BranchName: \u0026#34;v1\u0026#34;   This would have taken your products source code from CodeCommit. If you are using BitBucket Cloud, Github.com or Github Enterprise (where you can use AWS CodeStar Connections) you may want to use CodeStar Connections to simplify the creation of product version pipelines.\nTo do so, you can specify the following:\n Versions: - Name: \u0026#34;v1\u0026#34; Description: \u0026#34;v1 of aws-config-enable-config\u0026#34; Source: Provider: \u0026#34;CodeStarSourceConnection\u0026#34; Configuration: BranchName: \u0026#34;v1\u0026#34; ConnectionArn: \u0026#34;arn:aws:codestar-connections:eu-west-1:0123456789010:connection/eb6703af-6407-0522dc6a6\u0026#34; FullRepositoryId: \u0026#34;exampleorg/aws-config-enable-config\u0026#34;   "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/every-day-use/305-using-tag-options.html",
	"title": "Using tag options",
	"tags": [],
	"description": "",
	"content": " What are tag options? AWS Service Catalog allows you to associate tag options to portfolios and products (tag options apply to each version / provisioning artefact of a product).\nWhen you associate a tag option you are forcing the user who launches (creates a provisioned product) to apply the specified tag name and value in your tag option.\nWhat are we going to do? This tutorial will walk you through \u0026ldquo;Using tag options\u0026rdquo;.\nWe will assume you have:\n installed Service Catalog Factory correctly installed Service Catalog Puppet correctly bootstrapped a spoke you have created a product you have created a portfolio  We are going to perform the following steps:\n Adding tag options to your portfolio Applying tag options when using Service Catalog Puppet  During this process you will check your progress by verifying what the framework is doing at each step.\nStep by step guide Here are the steps you need to follow to \u0026ldquo;Using tag options\u0026rdquo;\nAdding tag options to your portfolio You can add tag options to the portfolios or products that exist in the /portfolios directory of your ServiceCatalogFactory repository:\nApplying to a portfolio:\n --- Schema: factory-2019-04-01 Portfolios: - DisplayName: \u0026#34;shared\u0026#34; Description: \u0026#34;Shared portfolio\u0026#34; ProviderName: \u0026#34;shared\u0026#34; TagOptions: - Key: \u0026#34;creating-team\u0026#34; Value: \u0026#34;ccoe\u0026#34; Products: - Name: vpc Owner: central-it@customer.com Distributor: central-it-team   Applying to a product defined within a portfolio:\n --- Schema: factory-2019-04-01 Portfolios: - DisplayName: \u0026#34;shared\u0026#34; Description: \u0026#34;Shared portfolio\u0026#34; ProviderName: \u0026#34;shared\u0026#34; Products: - Name: vpc Owner: central-it@customer.com Distributor: central-it-team TagOptions: - Key: \u0026#34;product-type\u0026#34; Value: \u0026#34;networking\u0026#34;   Applying to a product defined outside of a portfolio:\n --- Schema: factory-2019-04-01 Portfolios: - DisplayName: \u0026#34;shared\u0026#34; Description: \u0026#34;Shared portfolio\u0026#34; ProviderName: \u0026#34;shared\u0026#34; Products: - Name: eks-cluster Owner: central-it@customer.com Distributor: central-it-team TagOptions: - Key: \u0026#34;creating-sub-team\u0026#34; Value: \u0026#34;networking-team\u0026#34; Portfolios: - shared   Applying tag options when using Service Catalog Puppet When you install or update Service Catalog Puppet you can modify the parameter named ShouldShareTagOptions value to enable or disable tag options when Service Catalog Puppet creates portfolio shares. This value will apply for each launch and each spoke-local-portfolio.\nYou can override the global should share tag options value within the manifest file for launches and spoke-local-portfolios:\n spoke-local-portfolios: ccoe-products: share_tag_options: True portfolio: ccoe-products associations: - arn:aws:iam::${AWS::AccountId}:role/EndUsers deploy_to: tags: - tag: \u0026#34;role:spoke\u0026#34; regions: \u0026#34;enabled_regions\u0026#34;    launches: sleeper: share_tag_options: True portfolio: \u0026#34;ccoe-products\u0026#34; product: \u0026#34;vpc\u0026#34; version: \u0026#34;v1\u0026#34; deploy_to: tags: - tag: \u0026#34;role:spoke\u0026#34; regions: \u0026#34;enabled_regions\u0026#34;  "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/every-day-use/100-creating-a-product/310-creating-s3-pipelines.html",
	"title": "Creating S3 pipelines",
	"tags": [],
	"description": "",
	"content": " What are we going to do? We are going to perform the following steps:\n How to use S3 as a source for your pipelines  Step by step guide Here are the steps you need to follow to \u0026ldquo;Creating S3 pipelines\u0026rdquo;\nAdd the source code for your product When you configured your product version, you may have specified the following:\n Versions: - Name: \u0026#34;v1\u0026#34; Description: \u0026#34;v1 of aws-config-enable-config\u0026#34; Source: Provider: \u0026#34;CodeCommit\u0026#34; Configuration: RepositoryName: \u0026#34;aws-config-enable-config\u0026#34; BranchName: \u0026#34;v1\u0026#34;   This would have taken your products source code from CodeCommit. If you are using BitBucket Server or an SCM solution where you cannot use AWS CodeStar Connections you may want to configure your pipelines source your products from AWS S3 and then use another solution to put the source code there. To do so, you can specify the following:\n Versions: - Name: \u0026#34;v1\u0026#34; Description: \u0026#34;v1 of aws-config-enable-config\u0026#34; Source: Provider: \u0026#34;S3\u0026#34; Configuration: BucketName: \u0026#34;incomingproductchanges\u0026#34; S3ObjectKey: \u0026#34;aws-config-enable-config/v1/product.zip\u0026#34;   Please note, if you create S3 sourced pipelines you will be responsible for the creation of the AWS S3 Bucket.\nSince version 0.64.0 you can use intrinsic functions in the BucketName and S3ObjectKey values. The resultant AWS CloudFormation template that is generated to provision the AWS CodePipeline uses the Sub intrinsic function so you can now do the following:\n Versions: - Name: \u0026#34;v1\u0026#34; Description: \u0026#34;v1 of aws-config-enable-config\u0026#34; Source: Provider: \u0026#34;S3\u0026#34; Configuration: BucketName: \u0026#34;incomingproductchanges-for${AWS::AccountId}\u0026#34; S3ObjectKey: \u0026#34;aws-config-enable-config/v1-${AWS::AccountId}/product.zip\u0026#34;   This is useful when creating configuration files that will be shared across different AWS Organizations.\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/every-day-use/310-using-resource-update-constraints.html",
	"title": "Using resource update constraints",
	"tags": [],
	"description": "",
	"content": " What are we going to do? This tutorial will walk you through \u0026ldquo;Using resource update constraints\u0026rdquo; to a spoke local portfolio\nWe will assume you have:\n installed Service Catalog Puppet correctly created a product created a portfolio added a product to a portfolio  We are going to perform the following steps:\n specify a resource update constraint to control tag updates  Step by step guide Here are the steps you need to follow to \u0026ldquo;Using resource update constraints\u0026rdquo;\nThings to note, before we start  This feature was added to version 0.178.0. You will need to be using this version (or later)  Specify a resource update constraint to control tag updates Now we are ready to add a stack to the manifest file.\n  Navigate to the ServiceCatalogPuppet CodeCommit repository\n  Click the ServiceCatalogPuppet repository\n  Click the link to the manifest.yaml file, and then click the Edit button\n  Find the spoke-local-portfolio you want to add the constraint to:\n   spoke-local-portfolios: networking-options-for-spokes: portfolio: networking deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;   Add the constraint:\n spoke-local-portfolios: networking-options-for-spokes: portfolio: networking constraints: resource_update: - products: \u0026#34;vpc\u0026#34; tag_update_on_provisioned_product: NOT_ALLOWED deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;   When adding the constraint you can specify ALLOWED or NOT_ALLOWED as the value for tag_update_on_provisioned_product.\nWhen specifying products you can use a wildcard to specify more than one product:\n spoke-local-portfolios: networking-options-for-spokes: portfolio: networking constraints: resource_update: - products: \u0026#34;vpc-with-*-subnet*\u0026#34; tag_update_on_provisioned_product: NOT_ALLOWED deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;   Or you can specify a list:\n spoke-local-portfolios: networking-options-for-spokes: portfolio: networking constraints: resource_update: - products: - \u0026#34;vpc-with-1-subnet\u0026#34; - \u0026#34;vpc-with-2-subnets\u0026#34; - \u0026#34;vpc-with-3-subnets\u0026#34; tag_update_on_provisioned_product: NOT_ALLOWED deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;   "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/every-day-use/350-adding-a-region.html",
	"title": "Adding a region",
	"tags": [],
	"description": "",
	"content": " What are we going to do? This tutorial will walk you through \u0026ldquo;Adding a region\u0026rdquo;.\nWe will assume you have:\n installed Service Catalog Factory correctly installed Service Catalog Puppet correctly  The steps you will take depend on how you installed the tooling. Please follow the appropriate section.\nIf you installed the tools by creating an AWS CloudFormation stack then please follow the AWS CloudFormation steps, otherwise please follow the Python steps. If you cannot remember how you installed the tools please follow the Python steps.\nIf you have switched to GitHub as the source of your ServiceCatalogFactory and ServiceCatalogPuppet repos you will need to follow the Python steps.\nWhichever steps you follow you will need to follow the steps in Populating the New Regions section\nStep by step guide Here are the steps you need to follow to \u0026ldquo;Adding a region\u0026rdquo;\nAWS CloudFormation steps Factory If you installed factory using the AWS CloudFormation way then you can update the stack you created, changing the parameters. This will update your config and bootstrap again for you.\n  In the AWS Console navigate to the AWS CloudFormation service where you created your initialisation stack - the recommended name for the stack was factory-initialization-stack.\n  Select the stack and click Update, then Use current template should be selected and you can click Next\n  In the EnabledRegions input specify the new list of every region you want to target.\n  Once you have done this click Next on the following two screens and then check the box I acknowledge that AWS CloudFormation might create IAM resources with custom names. and click Update Stack\n  Once this has completed you can run the servicecatalog-product-factory-initialiser AWS CodeBuild project and your install will be updated.\n  Puppet If you installed puppet using the AWS CloudFormation way then you can update the stack you created, changing the parameters. This will update your config and bootstrap again for you.\n  In the AWS Console navigate to the AWS CloudFormation service where you created your initialisation stack - the recommended name for the stack was puppet-initialization-stack.\n  Select the stack and click Update, then Use current template should be selected and you can click Next\n  In the EnabledRegions input specify the new list of every region you want to target.\n  Once you have done this click Next on the following two screens and then check the box I acknowledge that AWS CloudFormation might create IAM resources with custom names. and click Update Stack\n  Once this has completed you can run the servicecatalog-product-puppet-initialiser AWS CodeBuild project and your install will be updated.\n  Python steps Factory  You will need to install aws-service-catalog-factory via pip:   pip install aws-service-catalog-factory    You can then set your regions:   servicecatalog-factory set-regions eu-west-1,eu-west-2,eu-west-3    You will then need to bootstrap with the same settings you used when initially bootstrapping:   servicecatalog-factory bootstrap ...   If you are using GitHub as your ServiceCatalogFactory repo you will need to specify this whenever you bootstrap.\nTo get a list of the parameters for bootstrapping you can run:\n servicecatalog-factory bootstrap --help   Puppet  You will need to install aws-service-catalog-puppet via pip:   pip install aws-service-catalog-puppet    You can then set your regions:   servicecatalog-puppet set-regions eu-west-1,eu-west-2,eu-west-3    You will then need to bootstrap with the same settings you used when initially bootstrapping:   servicecatalog-puppet bootstrap ...   If you are using GitHub as your ServiceCatalogPuppet repo you will need to specify this whenever you bootstrap.\nTo get a list of the parameters for bootstrapping you can run:\n servicecatalog-puppet bootstrap --help   Populating the New Regions Once you have followed the instructions above your product pipelines have been reconfigured to add your products to the newly specified regions. For your products to appear in those regions you will need to run their pipelines again.\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/every-day-use/360-aws-organizations-integration.html",
	"title": "AWS Organizations Integration",
	"tags": [],
	"description": "",
	"content": " This tutorial will walk you through \u0026ldquo;AWS Organizations Integration\u0026rdquo; with Service Catalog Tools.\nWe will assume you have:\n installed Service Catalog Puppet correctly setup AWS Organizations including:  a Management Account Member Account(s) in an Organizational Unit (OU)   setup an IAM Role for AWS Organizations bootstrapped a spoke created a product created a portfolio  In the tutorial you will look at:\n Adding Accounts using Organizational Units (OU)   Sharing a portfolio using AWS Organizations   During this process you will check your progress by verifying what the framework is doing.\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/every-day-use/360-aws-organizations-integration/361-adding-accounts-in-ou.html",
	"title": "Adding Accounts using Organizational Units (OU)",
	"tags": [],
	"description": "",
	"content": " What are we going to do? This tutorial will walk you through \u0026ldquo;Adding Accounts using Organizational Units (OU)\u0026quot;.\nWe are going to perform the following steps:\n create a manifest file add accounts to the manifest file using AWS Organizations OU add a spoke-local-portfolios to the manifest file  During this process you will check your progress by verifying what the framework is doing at each step.\nStep by step guide Here are the steps you need to follow to \u0026ldquo;Adding Accounts using Organizational Units (OU)\u0026rdquo;\nCreating the manifest file   Navigate to the ServiceCatalogPuppet CodeCommit repository\n  Scroll down to the bottom of the page and hit the Create file button\n    Adding an OU to the manifest file This will allow you to provision AWS Service Catalog Products into multiple AWS Accounts and Regions across your AWS estate without listing individual AWS 12-digit Account IDs. Instead we will supply the AWS Organizations OU Path. Service Catalog Products will then be provisioned in AWS Accounts that are a member of this OU.\nWe will start out by adding your OU to the manifest file.\n  Navigate to the ServiceCatalogPuppet CodeCommit repository\n  Scroll down to the bottom of the page and hit the Create file button\n      Copy the following snippet into the main input field:\n accounts: - ou: \u0026#34;\u0026lt;YOUR_OU_OR_PATH\u0026gt;\u0026#34; name: \u0026#34;application-accounts\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; - \u0026#34;eu-west-2\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34;     Update \u0026lt;YOUR_OU_OR_PATH\u0026gt; to be your OU or OU Path which contains member accounts\n for example: /production/application-accounts    The framework will list the AWS Accounts in your OU and expand the manifest automatically.\nFor example, if your OU were to contain AWS Accounts: 0123456789010 and 0109876543210, then the expanded manifest file will look like this:\n accounts: - account_id: 0123456789010 name: \u0026#39;\u0026lt;YOUR_ACCOUNT_NAME\u0026gt;\u0026#39; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; - \u0026#34;eu-west-2\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34; - account_id: 0109876543210 name: \u0026#39;\u0026lt;YOUR_ACCOUNT_NAME\u0026gt;\u0026#39; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; - \u0026#34;eu-west-2\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34;   Adding spoke-local-portfolio to the manifest Now we are ready to add a product to the manifest file.\n Add the following snippet to the end of the main input field:   spoke-local-portfolios: account-vending-for-spokes: portfolio: \u0026#34;reinvent-cloud-engineering-governance\u0026#34; deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;    The main input field should look like this:   accounts: - ou: \u0026#34;\u0026lt;YOUR_OU_OR_PATH\u0026gt;\u0026#34; name: \u0026#34;application-accounts\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; - \u0026#34;eu-west-2\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34; spoke-local-portfolios: account-vending-for-spokes: portfolio: \u0026#34;reinvent-cloud-engineering-governance\u0026#34; deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;   Committing the manifest file Now that we have written the manifest file we are ready to commit it.\n  Set the File name to manifest.yaml\n  Set your Author name\n  Set your Email address\n  Set your Commit message\n  Using a good / unique commit message will help you understand what is going on later.\n  Click the Commit changes button:    Verifying the sharing Once you have made your changes the ServiceCatalogPuppet Pipeline should have run or if you were quick may still be running. If it has not yet started feel free to the hit the Release change button.\nOnce it has completed it should show the Source and Deploy stages in green to indicate they have completed successfully:\n  Once you have verified the pipeline has run you can go to Service Catalog portfolios in the member account to view your shared product.\nWhen you share a portfolio the framework will decide if it should share the portfolio. If the target account is the same as the factory account it will not share the portfolio as it is not needed.\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/every-day-use/360-aws-organizations-integration/362-sharing-a-portfolio-organizations.html",
	"title": "Sharing a portfolio using AWS Organizations",
	"tags": [],
	"description": "",
	"content": " What are we going to do? This tutorial will walk you through \u0026ldquo;Sharing a portfolio using AWS Organizations\u0026rdquo; to OU member accounts.\nDoing this will allow the framework to share Service Catalog Portfolios by using AWS Organizations OUs, rather than account-to-account sharing. This will reduce the time required to share Portfolios.\nWe are going to perform the following steps:\n enable organizational sharing create a manifest file add an account to the manifest file add a spoke-local-portfolios to the manifest file  During this process you will check your progress by verifying what the framework is doing at each step.\nStep by step guide Here are the steps you need to follow to \u0026ldquo;Sharing a portfolio using AWS Organizations\u0026rdquo;\nEnable Organizational Sharing This is action is only required once.\n Using the AWS Console:\n Navigate to Service Catalog in the AWS Console Select Portfolios from the sidebar. Select previously created Portfolio, i.e reinvent-cloud-engineering-governance Select Actions, then Share. Select the Organizations button.  You'll see a warning message: \u0026ldquo;Organizational sharing is not available\u0026rdquo;   Select the Enable button.    Creating the manifest file   Navigate to the ServiceCatalogPuppet CodeCommit repository\n  Scroll down to the bottom of the page and hit the Create file button\n    Adding an OU to the manifest file We will start out by adding your OU to the manifest file.\n  Navigate to the ServiceCatalogPuppet CodeCommit repository\n  Scroll down to the bottom of the page and hit the Create file button\n      Copy the following snippet into the main input field:\n accounts: - ou: \u0026#34;\u0026lt;YOUR_OU_OR_PATH\u0026gt;\u0026#34; name: \u0026#34;application-accounts\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; - \u0026#34;eu-west-2\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34;     Update \u0026lt;YOUR_OU_OR_PATH\u0026gt; to show your OU or OU Path which contains member accounts\n for example /production/application-accounts/    Adding spoke-local-portfolio to the manifest Now we are ready to add a product, which we will share via AWS Organizations, to the manifest file.\n Add the following snippet to the end of the main input field:   spoke-local-portfolios: account-vending-for-spokes: portfolio: \u0026#34;reinvent-cloud-engineering-governance\u0026#34; sharing_mode: AWS_ORGANIZATIONS deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;   Notice that we have included sharing_mode: AWS_ORGANIZATIONS.\nHere, the Portfolio Share will be accepted in the default_region of accounts that are type:prod.\n The main input field should look like this:   accounts: - ou: \u0026#34;\u0026lt;YOUR_OU_OR_PATH\u0026gt;\u0026#34; name: \u0026#34;application-accounts\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; - \u0026#34;eu-west-2\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34; spoke-local-portfolios: account-vending-for-spokes: portfolio: \u0026#34;reinvent-cloud-engineering-governance\u0026#34; sharing_mode: AWS_ORGANIZATIONS deploy_to: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;   Committing the manifest file Now that we have written the manifest file we are ready to commit it.\n  Set the File name to manifest.yaml\n  Set your Author name\n  Set your Email address\n  Set your Commit message\n  Using a good / unique commit message will help you understand what is going on later.\n  Click the Commit changes button:    Verifying the sharing Once you have made your changes the ServiceCatalogPuppet Pipeline should have run or if you were quick may still be running. If it has not yet started feel free to the hit the Release change button.\nOnce it has completed it should show the Source and Deploy stages in green to indicate they have completed successfully:\n  Once you have verified the pipeline has run you can go to Service Catalog portfolios in the member account to view your shared product.\nWhen you share a portfolio the framework will decide if it should share the portfolio. If the target account is the same as the factory account it will not share the portfolio as it is not needed.\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/every-day-use/380-provisioning-a-cloudformation-stack.html",
	"title": "Provisioning CloudFormation",
	"tags": [],
	"description": "",
	"content": " What are we going to do? This tutorial will walk you through how to use the \u0026ldquo;Provisioning CloudFormation\u0026rdquo; feature.\nWe will assume you have:\n installed Service Catalog Puppet correctly bootstrapped a spoke created a manifest file added an account to the manifest file  We will assume you are comfortable:\n making changes your manifest file  We are going to perform the following steps to \u0026ldquo;Provisioning CloudFormation\u0026rdquo;:\n upload a template to AWS CloudFormation specify an AWS CloudFormation template that should be provisioned  Step by step guide Here are the steps you need to follow to \u0026ldquo;Provisioning CloudFormation\u0026rdquo;\nThings to note, before we start  This feature was added to version 0.108.0. You will need to be using this version (or later) Stacks can use parameters, deploy_to and outputs Stacks can be used in spoke execution mode Stacks can be used in dry-runs Stacks do not appear in list-launches (they are not a launch)  Upload a template to AWS CloudFormation When you upgrade to version 0.108.0 or newer you will see a bucket named sc-puppet-stacks-repository-xxx where xxx is your AWS account id. You should upload a template into that bucket and get the version id of the template:\n  Specify an AWS CloudFormation template that should be provisioned Now we are ready to add a stack to the manifest file.\n  Navigate to the ServiceCatalogPuppet CodeCommit repository\n  Click the ServiceCatalogPuppet repository\n  Click the link to the manifest.yaml file, and then click the Edit button\n  Add the following snippet to the end of the main input field:\n   stacks: basic-vpc: key: product.template.yaml version_id: 1tPCvNHLEw8fsARqJ2RFouBfebRpURS7 depends_on: - name: basic-vpc type: launches affinity: account deploy_to: tags: - tag: group:spoke regions: regions_enabled outputs: ssm: - stack_output: VPCId param_name: \u0026#34;/vpcs/${AWS::AccountId}/${AWS::Region}/VPCId\u0026#34;   Committing the manifest file Now that we have updated the manifest file we are ready to commit our changes.\n Set your Author name Set your Email address Set your Commit message  Using a good / unique commit message will help you understand what is going on later.\n  Click the Commit changes button:    "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/every-day-use/382-migrating-from-launches-to-stacks.html",
	"title": "Migrating from launches to stacks",
	"tags": [],
	"description": "",
	"content": " What are we going to do? This tutorial will walk you through how to use the \u0026ldquo;Migrating from launches to stacks\u0026rdquo; feature.\nWe will assume you have:\n installed Service Catalog Puppet correctly bootstrapped a spoke created a manifest file added an account to the manifest file have provisioned a launch you want to migrate to a stack without recreating resources  We will assume you are comfortable:\n making changes your manifest file  Step by step guide Here are the steps you need to follow to \u0026ldquo;Migrating from launches to stacks\u0026rdquo;\nThings to note, before we start  This feature was added to version 0.114.0. You will need to be using this version (or later)  Updating your manifest file Your manifest should contain a launch:\n launches: basic-vpc: portfolio: ccoe product: basic-vpc version: v1 deploy_to: tags: - tag: group:spoke regions: regions_enabled   You will need to remove the launch named basic-vpc from your manifest file and add a stack to your manifest file. You can name the stack whatever you want. You will need to add an attribute named launch_name with the value of basic-vpc where basic-vpc is the launch name you used previously. In this example it looks like this:\n stacks: basic-vpc: name: basic-vpc version: v1 launch_name: basic-vpc deploy_to: tags: - tag: group:spoke regions: regions_enabled   When you make this change the framework will look for a provisioned product in the spoke account named basic-vpc. If it finds it, it will use your new (stack) template to update the CloudFormation stack your previous launch created. You should not terminate the provisioned product unless you want to terminate the resources. If you terminate the product or if the product does not exist in the spoke account a new stack will be created using the stack name from the manifest file as the CloudFormation stack name.\nWith this approach you can migrate a launch to a stack. In existing accounts the stack from the provisioned product will be used (with a name of SC-\u0026lt;account_id\u0026gt;-\u0026lt;pp_id\u0026gt;) and in accounts where the stack from the product was never provisioned a new stack will be created using the manifest file stack name.\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/every-day-use/384-migrating-from-stacksets-to-stacks.html",
	"title": "Migrating from stacksets to stacks",
	"tags": [],
	"description": "",
	"content": " What are we going to do? This tutorial will walk you through how to use the \u0026ldquo;Migrating from stacksets to stacks\u0026rdquo; feature.\nWe will assume you have:\n installed Service Catalog Puppet correctly bootstrapped a spoke created a manifest file added an account to the manifest file have provisioned a stackset you want to migrate to a stack without recreating resources  We will assume you are comfortable:\n making changes your manifest file  Step by step guide Here are the steps you need to follow to \u0026ldquo;Migrating from stacksets to stacks\u0026rdquo;\nThings to note, before we start  This feature was added to version 0.134.0. You will need to be using this version (or later)  Updating your manifest file Your account should contain a stack that was part of a stackset. You will need to know the stack set name used.\nYou will need to add a stack to your manifest file. You can name the stack whatever you want. You will need to add an attribute named stack_set_name and the value should be the stack set name - not the stack set instance. In this example it looks like this:\n stacks: basic-vpc: name: basic-vpc version: v1 stack_set_name: basic-vpc deploy_to: tags: - tag: group:spoke regions: regions_enabled   When you make this change the framework will look for a stacks in the spoke account with a stack name beginning with StackSet-basic-vpc-. If it finds it, it will use your new (stack) template to update the CloudFormation stack your stack set created. You should not modify the stack set unless you want to terminate the resources. If you terminate the stackset or if the stack instance does not exist in the spoke account a new stack will be created using the stack name from the manifest file as the CloudFormation stack name.\nWith this approach you can migrate a stack set to a stack. In existing accounts the stack from the stack set will be used (with a name of StackSet-\u0026lt;stack_name\u0026gt;-) and in accounts where the stack from the stack set was never provisioned a new stack will be created using the manifest file stack name.\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/every-day-use/385-provisioning-a-stack.html",
	"title": "Provisioning a Stack",
	"tags": [],
	"description": "",
	"content": " What are we going to do? This tutorial will walk you through how to use the \u0026ldquo;Provisioning a Stack\u0026rdquo; feature.\nWe will assume you have:\n installed Service Catalog Factory correctly installed Service Catalog Puppet correctly bootstrapped a spoke created a manifest file added an account to the manifest file  We will assume you are comfortable:\n making changes your portfolios files making changes your manifest file  We are going to perform the following steps to \u0026ldquo;Provisioning a Stack\u0026rdquo;:\n creating a stack using Service Catalog Factory provision a stack using Service Catalog Puppet  Step by step guide Here are the steps you need to follow for \u0026ldquo;Provisioning a Stack\u0026rdquo;\nThings to note, before we start  This feature was added in version 0.63.0 of Service Catalog Factory. You will need to be using this version (or later) This feature was added in version 0.109.0 of Service Catalog Puppet. You will need to be using this version (or later) Stacks can use parameters, deploy_to and outputs Stacks can be used in spoke execution mode Stacks can be used in dry-runs Stacks can be provisioned in spoke execution mode (since Service Catalog Puppet version 0.109.0) Stacks do not appear in list-launches (they are not a launch)  Creating a stack using Service Catalog Factory   Navigate to the ServiceCatalogFactory CodeCommit repository\n  click Add file and then Create file\n  Paste the following into the main input window:\n  In the File name field enter the following stacks/example.yaml\n   Schema: factory-2019-04-01 Stacks: - Name: \u0026#34;ssm-parameter\u0026#34; Versions: - Name: \u0026#34;v1\u0026#34; Source: Provider: \u0026#34;CodeCommit\u0026#34; Configuration: RepositoryName: \u0026#34;ssm-parameter-stack\u0026#34; BranchName: \u0026#34;main\u0026#34;     Please note the file name is not significant but it must have the extension .yaml and it must be in a directory named stacks\n  Fill in the other fields and save the file. Once you do the pipeline servicecatalog-factory-pipeline will run.\n  Once the pipeline is complete you will have a new pipeline named stack--ssm-parameter-v1-pipeline\n  You should navigate to AWS CodeCommit, create a repo named ssm-parameter-stack and add a file named stack.template.yaml with the following content on the main branch:\n   Parameters: Name: Type: String Value: Type: String Resources: Parameter: Type: AWS::SSM::Parameter Properties: Name: !Ref Name Type: String Value: !Ref Value Outputs: Value: Value: !GetAtt Parameter.Value    Once you have added the file the pipeline stack--ssm-parameter-v1-pipeline will run. It will take the source code and add will add it to Amazon S3 so you can use it in Service Catalog Puppet in the step below.  Provision a stack using Service Catalog Puppet Now we are ready to provision the stack using the manifest file.\n  Navigate to the ServiceCatalogPuppet CodeCommit repository\n  Click the ServiceCatalogPuppet repository\n  Click the link to the manifest.yaml file, and then click the Edit button\n  Add the following snippet to the end of the main input field:\n   stacks: ssm-parameter: name: ssm-parameter version: v1 parameters: Name: default: \u0026#34;hello\u0026#34; Value: default: \u0026#34;world\u0026#34; deploy_to: tags: - tag: type:prod regions: regions_enabled   Capabilities In some cases, you must explicitly acknowledge that your stack template contains certain capabilities in order for CloudFormation to create the stack.\nWhen you need to do this you can specify the capabilities as a list per stack:\n stacks: ssm-parameter: name: ssm-parameter version: v1 capabilities: - CAPABILITY_IAM parameters: Name: default: \u0026#34;hello\u0026#34; Value: default: \u0026#34;world\u0026#34; deploy_to: tags: - tag: type:prod regions: regions_enabled   Committing the manifest file Now that we have updated the manifest file we are ready to commit our changes.\n Set your Author name Set your Email address Set your Commit message  Using a good / unique commit message will help you understand what is going on later.\n  Click the Commit changes button:    "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/every-day-use/386-using-stack-and-launch-outputs.html",
	"title": "Using stack and launch outputs",
	"tags": [],
	"description": "",
	"content": " What are we going to do? This tutorial will walk you through how to use the \u0026ldquo;Using stack and launch outputs\u0026rdquo; feature.\nWe will assume you have:\n installed Service Catalog Puppet correctly bootstrapped a spoke created a manifest file added an account to the manifest file  We will assume you are comfortable:\n making changes your portfolios files making changes your manifest file provisioning stacks provisioning products / launches  Step by step guide Here are the steps you need to follow to \u0026ldquo;Using stack and launch outputs\u0026rdquo;\nThings to note, before we start  The cloudformation_stack_output feature was added in version 0.150.0 of Service Catalog Puppet. You will need to be using this version (or later) The servicecatalog_provisioned_product_output feature was added in version 0.160.0 of Service Catalog Puppet. You will need to be using this version (or later)  Provision a stack with some outputs In order to use a stack output you must first provision a stack that provides the output. Here is an example of such a template:\n Parameters: CidrBlock: Type: String Name: Type: String Resources: VPC: Type: AWS::EC2::VPC Properties: CidrBlock: !Ref CidrBlock EnableDnsHostnames: True EnableDnsSupport: True Tags: - Key: Name Value: !Ref Name Outputs: VpcId: Value: !Ref VPC Description: Output VPC ID for utilisation by dependent product VpcCidrBlock: Value: !GetAtt VPC.CidrBlock   Here we are provisioning a stack that outputs VpcId as an output.\nProvisioning a stack that depends on the initial stack, which consumes stack outputs Once you have provisioned the stack that provides the output you will need to define a stack that uses the output:\n Parameters: CidrBlock: Type: String VpcId: Type: String Resources: Subnet: Type: AWS::EC2::Subnet Properties: CidrBlock: !Select [ 0, !Cidr [ !Ref CidrBlock, 6, 5 ]] AvailabilityZone: !Select - 0 - Fn::GetAZs: !Ref \u0026#34;AWS::Region\u0026#34; Tags: - Key: Name Value: \u0026#34;Example-Subnet-AZ1\u0026#34; VpcId: !Ref VpcId   Here we are defining a stack that uses VpcId as a parameter.\nProvision a stack using Service Catalog Puppet Now we are ready to provision the stack using the manifest file.\nTo use the output from one stack as a parameter for another you can use the following syntax:\n stacks: example-subnet: name: example-subnet version: v1 depends_on: - example-vpc parameters: CidrBlock: cloudformation_stack_output: account_id: ${AWS::AccountId} region: ${AWS::Region} stack_name: example-vpc output_key: VpcCidrBlock VpcId: cloudformation_stack_output: account_id: ${AWS::AccountId} region: ${AWS::Region} stack_name: example-vpc output_key: VpcId deploy_to: tags: - tag: type:prod regions: regions_enabled   In this example we are reading VpcCidrBlock and VpcId from the stack named example-vpc when provisioning the stack named example-subnet.\nWorking with ServiceCatalog Provisioned product outputs If you would like to obtain the output of a provisioned product deployment, it is possible to call the servicecatalog_provisioned_product_output method to obtain stack outputs. In the example below we are calling from the output of a launch called \u0026lsquo;example-vpc\u0026rsquo;\n stacks: example-subnet: name: example-subnet version: v1 depends_on: - example-vpc parameters: CidrBlock: servicecatalog_provisioned_product_output: account_id: ${AWS::AccountId} region: ${AWS::Region} provisioned_product_name: example-vpc output_key: VpcCidrBlock VpcId: servicecatalog_provisioned_product_output: account_id: ${AWS::AccountId} region: ${AWS::Region} provisioned_product_name: example-vpc output_key: VpcId deploy_to: tags: - tag: type:prod regions: regions_enabled   "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/every-day-use/390-terminating-provisioned-resources.html",
	"title": "Terminating provisioned resources",
	"tags": [],
	"description": "",
	"content": " What are we going to do? This tutorial will walk you through how to use the \u0026ldquo;Terminating provisioned resources\u0026rdquo; feature.\nWe will assume you have:\n installed Service Catalog Factory correctly installed Service Catalog Puppet correctly bootstrapped a spoke created a manifest file added an account to the manifest file provisioned a resource - either through a stack, app, workspace, launch or service control policy  We will assume you are comfortable:\n making changes your manifest file  We are going to perform the following steps:\n terminate a provisioned resource  Step by step guide Here are the steps you need to follow to \u0026ldquo;Terminating provisioned resources\u0026rdquo;\nTerminate a provisioned resource When you are ready to terminate a provisioned resource you will need to edit its definition in the manifest yaml.\n  Navigate to the ServiceCatalogPuppet CodeCommit repository\n  Click the ServiceCatalogPuppet repository\n  Click the link to the manifest.yaml file, and then click the Edit button\n  Add or set the attribute status for the resource you want to terminate to terminated\n  Example \u0026ldquo;stack\u0026rdquo; resource deletion:\n   stacks: ssm-parameter: name: ssm-parameter status: terminated version: v1 parameters: Name: default: \u0026#34;hello\u0026#34; Value: default: \u0026#34;world\u0026#34; deploy_to: tags: - tag: type:prod regions: regions_enabled    Example \u0026ldquo;launch\u0026rdquo; resource deletion:   launches: ssm-parameter: portfolio: self-service-portfolio product: ssm-parameter status: terminated version: v1 parameters: Name: default: \u0026#34;hello\u0026#34; Value: default: \u0026#34;world\u0026#34; deploy_to: tags: - tag: type:prod regions: regions_enabled    Example \u0026ldquo;service-control-policy\u0026rdquo; resource deletion:   service-control-policies: deny-organizations-leave-organization: status: terminated description: \u0026#34;do not allow accounts to leave\u0026#34; tags: - Key: Category Value: Foundational content: default: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Deny\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;organizations:LeaveOrganization\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } apply_to: accounts: - account_id: \u0026#34;029953558454\u0026#34;   Please note terminating a service control policy will detach it from the target but it will not delete it.\nCommitting the manifest file Now that we have updated the manifest file we are ready to commit our changes.\n Set your Author name Set your Email address Set your Commit message Click the Commit changes button:    When the framework runs, the provisioned resource in the target account will be terminated.\nYou can verify this by navigating to the target account and checking the termination of resource.\nNotes You can set the status attribute to \u0026ldquo;terminated\u0026rdquo; for the following resources actions to terminate the resources previously provisioned:\n launches stacks workspaces apps  If a resource was previously terminated by the solution in future executions the solution will verify the resources are terminated.\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/every-day-use/400-invoking-a-lambda-function.html",
	"title": "Invoking a Lambda Function",
	"tags": [],
	"description": "",
	"content": " What are we going to do? This tutorial will walk you through how to use the \u0026ldquo;Invoking a Lambda Function\u0026rdquo; feature.\nWe will assume you have:\n installed Service Catalog Puppet correctly bootstrapped a spoke created a manifest file added an account to the manifest file  We are going to perform the following steps to \u0026ldquo;Invoking a Lambda Function\u0026rdquo;:\n set up an AWS IAM role in the spoke account create a sample lambda function add a lambda invoke to the manifest file  During this process you will check your progress by verifying what the framework is doing at each step.\nStep by step guide Here are the steps you need to follow to \u0026ldquo;Invoking a Lambda Function\u0026rdquo;\nSet Up an AWS IAM Role in the Spoke Account This guide assumes that a role exists within the spoke account that can be assumed by the Service Catalog Tools account. The name of this role would need to be the same across all spoke accounts, and the role would need permissions appropriate for your lambda function(s) to be able to complete its tasks. For the purpose of this example, a CloudFormation template has been provided that you can use to create the role in the spoke account that will allow our sample lambda function to run successfully.\n AWSTemplateFormatVersion: \u0026#39;2010-09-09\u0026#39; Description: \u0026#34;IAM Role in spoke accounts that will have trust relationship with Service Catalog Tools account.\u0026#34; Parameters: ToolsAccountId: Description: \u0026#34;The Service Catalog Tools AWS Account ID\u0026#34; Type: String ToolsAccountAccessRole: Description: \u0026#34;Name of the IAM role that the management account will be allowed to assume\u0026#34; Default: ToolsAccountAccessRole Type: String Resources: AssumedRole: Type: AWS::IAM::Role Description: | IAM Role needed by the Service Catalog Tools Account Properties: RoleName: !Ref ToolsAccountAccessRole Policies: - PolicyName: ToolsAccountTrustPolicy PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: \u0026#39;iam:*\u0026#39; Resource: \u0026#39;*\u0026#39; AssumeRolePolicyDocument: Version: \u0026#34;2012-10-17\u0026#34; Statement: - Effect: \u0026#34;Allow\u0026#34; Principal: AWS: !Sub \u0026#34;arn:aws:iam::${ToolsAccountId}:root\u0026#34; Action: - \u0026#34;sts:AssumeRole\u0026#34;   Creating the sample lambda function We will need to create the AWS Lambda function that will be executed by the framework. This function will exist in the account where you have installed the Service Catalog Tools. When you want to perform an action in a spoke account you should read the account_id and region properties from the event object. If you want to use parameters they are available using the parameters attribute in the event object.\n  You should save the following into a file named create-iam-group-lambda.yaml\n AWSTemplateFormatVersion: \u0026#39;2010-09-09\u0026#39; Description: Creates a Lambda function that assumes a role into spoke accounts and creates an IAM group Resources: rLambdaCustomRole: Type: AWS::IAM::Role Properties: AssumeRolePolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Principal: Service: [lambda.amazonaws.com] Action: - sts:AssumeRole ManagedPolicyArns: - !Ref rLambdaCustomPolicy rLambdaCustomPolicy: Type: AWS::IAM::ManagedPolicy Properties: PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: [\u0026#39;logs:CreateLogGroup\u0026#39;, \u0026#39;logs:CreateLogStream\u0026#39;, \u0026#39;logs:PutLogEvents\u0026#39;] Resource: !Sub arn:${AWS::Partition}:logs:${AWS::Region}:${AWS::AccountId}:log-group:* - Effect: Allow Action: - sts:AssumeRole Resource: \u0026#34;*\u0026#34; rLambda: Type: AWS::Lambda::Function Properties: FunctionName: create-iam-group Description: Creates an IAM Group Handler: index.lambda_handler Code: ZipFile: | import json import boto3 # Set up clients sts = boto3.client(\u0026#39;sts\u0026#39;) def get_session_info(event): acct_id = event[\u0026#39;account_id\u0026#39;] role_name = event[\u0026#39;parameters\u0026#39;][\u0026#39;RoleName\u0026#39;] role_arn = \u0026#39;arn:aws:iam::\u0026#39; + acct_id + \u0026#39;:role/\u0026#39; + role_name sts_response = sts.assume_role( RoleArn=role_arn, RoleSessionName=\u0026#39;LambdaInvokeSession\u0026#39; ) return sts_response def lambda_handler(event, context): print(event) sts_response = get_session_info(event) access_key = sts_response[\u0026#34;Credentials\u0026#34;][\u0026#34;AccessKeyId\u0026#34;] secret_key = sts_response[\u0026#34;Credentials\u0026#34;][\u0026#34;SecretAccessKey\u0026#34;] session_token = sts_response[\u0026#34;Credentials\u0026#34;][\u0026#34;SessionToken\u0026#34;] iam = boto3.client( \u0026#39;iam\u0026#39;, aws_access_key_id=access_key, aws_secret_access_key=secret_key, aws_session_token=session_token ) group_name = \u0026#39;sc-tools-invoke-lambda-test-group\u0026#39; response = iam.create_group( GroupName=group_name ) MemorySize: 128 Role: !GetAtt rLambdaCustomRole.Arn Runtime: python3.7 Timeout: 300 Outputs: LambdaName: Value: !Ref rLambda     You should then use AWS CloudFormation to create a stack named create-iam-group-lambda using the template you just created\n  What did we just do?  You created an AWS CloudFormation template You used the template to create a stack in CloudFormation The stack created a sample lambda function  Adding a lambda-invocation to the manifest Now we are ready to add a lambda invocation to the manifest file.\n  Navigate to the ServiceCatalogPuppet CodeCommit repository\n  Click the ServiceCatalogPuppet repository\n  Click the link to the manifest.yaml file, and then click the Edit button\n  Add the following snippet to the end of the main input field:\n   lambda-invocations: create-iam-group: function_name: create-iam-group qualifier: $LATEST invocation_type: Event parameters: RoleName: default: \u0026#34;ToolsAccountAccessRole\u0026#34; invoke_for: tags: - regions: \u0026#34;default_region\u0026#34; tag: \u0026#34;type:prod\u0026#34;    The main input field should look like this:   accounts: - account_id: \u0026#34;\u0026lt;YOUR_SPOKE_ACCOUNT_ID_WITHOUT_HYPHENS\u0026gt;\u0026#34; name: \u0026#34;puppet-account\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; - \u0026#34;eu-west-2\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34; lambda-invocations: create-iam-group: function_name: create-iam-group qualifier: $LATEST invocation_type: Event parameters: RoleName: default: \u0026#34;ToolsAccountAccessRole\u0026#34; invoke_for: tags: - regions: \u0026#34;default_region\u0026#34; tag: \u0026#34;type:prod\u0026#34;   Committing the manifest file Now that we have updated the manifest file we are ready to commit our changes.\n Set your Author name Set your Email address Set your Commit message  Using a good / unique commit message will help you understand what is going on later.\n  Click the Commit changes button:    Verifying the lambda invocation Once you have made your changes the ServiceCatalogPuppet Pipeline should have run or if you were quick may still be running. If it has not yet started feel free to the hit the Release change button.\nOnce it has completed it should show the Source and Deploy stages in green to indicate they have completed successfully:\n  Once you have verified the pipeline has run you can go to IAM Groups Console in the spoke account to view the IAM Group created by the lambda invoke labeled sc-tools-invoke-lambda-test-group.\n  You have now successfully invoked a lambda function!\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/managing-your-environments/product-sdlc.html",
	"title": "Product SDLC",
	"tags": [],
	"description": "",
	"content": " What are we going to do? This article will explain how productive teams have been structuring their AWS Accounts and how they have been managing their source code whilst working with the Service Catalog Tools.\nAWS Account structure Testing the provisioning of a single resource in AWS can be achieved using CloudFormation RSpec, awspec and others. Testing the provisioning of a single resource or even a single AWS CloudFormation stack is valuable and will give you confidence that your change will behave in the way you think without any side effects. When you have multiple developers working on the same code base, or develop over time or have different developers provisioning different stacks that must work with each other it can be difficult to manage and changes that introduce unexpected side effects can easily sneak through.\nWe do not recommend you create a whole test installation of the Service Catalog Tools for the development of products.\nThe management overhead of the installation and the complexity and delay it creates in your SDLC should be avoided and instead you should invest that effort into better automated testing so you can scale.\nWe recommend you have at least a single canary account where you can provision products that will work together. You can use actions from the Service Catalog Tools to automate the testing and promotion of products across your AWS Accounts including a mandatory provisioning into the canary account.\nManaging source code We recommend you do not modify a product version that you have shared or provisioned. If you make a change to a product then you should change the version name.\nWe have seen gitflow working well with product development. Whilst building a product the developer uses a develop branch in git. That develop version gets provisioned into the canary account for testing. Once the testing is complete the branch in merged to main and then rebranched from main to create a new version. This worked well for teams where a single developer was building a single product. If you have a product that is too big for a single developer then maybe the product needs to be split into smaller pieces. There are features in the tools that allow you split products into smaller pieces.\nIf you would like to share your SDLC process please raise a github issue to share\n "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/2021-workshop/400-self-service.html",
	"title": "Self service",
	"tags": [],
	"description": "",
	"content": " What are we going to do? We have now removed the default networking resources, created a VPC and two subnets.\nAs we build out a multi account environment we may want to allow our end user to decide how many subnets they need, what sizes they are or which VPC they should be attached to. AWS Service Catalog is a great way to achieve this. You can create a product, add it to a portfolio and then allow your users to provision the product with limited options.\nThe following steps will show you how to make a product and share it with an account:\n Using AWS Service Catalog   "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/design-considerations/using-iam-and-scp-effectively.html",
	"title": "Using IAM and SCP Effectively",
	"tags": [],
	"description": "",
	"content": " What are we going to do? This article will explain how to restrict access to the AWS services for your users but still allow them to provision resources using launch constraints.\nIt will also cover how you can use resource naming conventions and conditional IAM and SCP statements to ensure users cannot modify AWS Service Catalog products or the resources within them.\nUsing launch constraints You may wish to use a service catalog to provide a set of approved resource templates for consumption. In this instance, you may want to allow consumers to provision a product that contains an AWS S3 bucket but you may not want to allow these users to provision AWS S3 buckets directly via the console or cli.\nTo achieve this, you could use a launch constraint and an IAM role assumable by the AWS Service Catalog service. The IAM role you provide as a launch constraint is used to provision the resources in the product. The role provisioning the product must have the iam:passrole permission.\nResource naming conventions along with conditional IAM and SCP statements When writing your AWS Service Catalog products you can have a parameter to all of your products that is used as a prefix to the names of the resources provisioned. You can then use IAM boundaries to prohibit the modifying of all resources that have the given prefix or you can use a conditional SCP to prohibit the modifying of all resources that have the given prefix except for the PuppetRole.\nWithin the Service Catalog Tools all provisioning occurs using an IAM role /servicecatalog-puppet/PuppetRole.\nWhen writing your products you should use globally unique parameter names to avoid clashes. If you use the same name for the resource prefixes you can set the parameter value once in the manifest file as a global parameter.\nIf you would like to share your experiences please raise a github issue\n "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/every-day-use/600-starting-a-code-build-project.html",
	"title": "Starting a CodeBuild project",
	"tags": [],
	"description": "",
	"content": " What are we going to do? This tutorial will walk you through how to use the \u0026ldquo;Starting a CodeBuild project\u0026rdquo; feature.\nWe will assume you have:\n installed Service Catalog Puppet correctly bootstrapped a spoke created a manifest file added an account to the manifest file  We are going to perform the following steps to \u0026ldquo;Starting a CodeBuild project\u0026rdquo;:\n Create an AWS CodeBuild project in your puppet account that will be triggered Adding the codebuild run to your manifest file that will do the triggering  Step by step guide Here are the steps you need to follow to \u0026ldquo;Starting a CodeBuild project\u0026rdquo;\nCreate an AWS CodeBuild project in your puppet account Log into the account where you installed service catalog puppet and navigate to the AWS CodeBuild console in the region where you installed service catalog puppet.\nCreate a new project named ping-host. It should have three environmental variables named:\n TARGET_ACCOUNT_ID TARGET_REGION HOST_TO_PING  For the build spec you should have the following command:\necho \u0026quot;I have been asked to ping ${HOST_TO_PING} from ${TARGET_REGION} of ${TARGET_ACCOUNT_ID}\u0026quot;\nAdding the codebuild run to your manifest file Add the following snippet to your manifest file:\n code-build-runs: ping-host: project_name: ping-some-host parameters: HOST_TO_PING: default: 10.0.0.2 run_for: tags: - regions: regions_enabled tag: role:all   If you already had a code-build-runs please append the ping-host declaration to the existing code-build-runs section.\nYou will most likely need to update the tag from role:all to whatever you are using in your environment.\nThe parameters section supports all capabilities that launches supports. You can read more in the \u0026ldquo;Using parameters in the real world\u0026rdquo; section under the \u0026ldquo;Design considerations\u0026rdquo; heading.\nWhat did we just do? You told service catalog puppet to get a list of all regions for all accounts with the tag you specified. For each item in the list you told service catalog puppet to run a codebuild project. If you have 2 regions and 4 accounts the codebuild project will be triggered 8 times. The codebuild projects will run in serial to avoid hitting service or api throttling limits.\nVerifying the codebuild run To verify the codebuild runs you can use the codebuild console. Look at the history for the ping-host project and verify the number of runs was correct. For each run you can verify the environmental variables that were overridden.\nYou have now successfully run a codebuild project!\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/every-day-use/680-using-policy-simulations.html",
	"title": "Using policy simulations",
	"tags": [],
	"description": "",
	"content": " What are we going to do? With policy simulations, you tell the framework to interact with AWS IAM simulate_principal_policy or simulate_custom_policy. Using policy simulations you can verify the configuration of your access management ensuring IAM roles, users and groups have access to the resources you want them to and do not have permission to those actions for which they should not have access to.\nUsing the framework you can run simulations in parallel across many regions of many accounts quickly and easily.\nThis tutorial will walk you through how to use the \u0026ldquo;Using policy simulations\u0026rdquo; feature.\nWe will assume you have:\n installed Service Catalog Puppet correctly bootstrapped a spoke created a manifest file added an account to the manifest file  We are going to perform the following steps to \u0026ldquo;Using policy simulations\u0026rdquo;:\n Adding a principal policy simulation to your manifest file Adding a custom policy simulation to your manifest file  Step by step guide Here are the steps you need to follow to \u0026ldquo;Using policy simulations\u0026rdquo;\nAdding a principal policy simulation to your manifest file Add the following snippet to your manifest file:\n simulate-policies: explicitDeny-s3-createbucket-for-devops-in-prod: simulation_type: \u0026#34;principal\u0026#34; policy_source_arn: \u0026#34;arn:aws:iam::${AWS::AccountId}:role/DevOps\u0026#34; action_names: - \u0026#34;s3:CreateBucket\u0026#34; expected_decision: \u0026#34;explicitDeny\u0026#34; simulate_for: tags: - tag: \u0026#34;type:prod\u0026#34; regions: \u0026#34;default_region\u0026#34;  You will most likely need to update the tag from role:all to whatever you are using in your environment.\nWhat did we just do? In each region of each account in your simulate_for you asked service catalog puppet to do the following:\n assume role into the region of the account using an AWS IAM client execute the simulate_principal_policy function using the parameters specified verify the EvalDecisionDetails returned was \u0026ldquo;explicitDeny\u0026rdquo;  If the EvalDecisionDetails was not \u0026ldquo;explicitDeny\u0026rdquo; a failure would occur, anything depending on the simulate policy would not execute and if AWS OpsCentre support was enabled an OpsIssue would have been created.\nRecommendations  It is recommended to add principal simulate policies for critical scenarios in high risk environments.  Extra notes  simulate-policies can be executed in any execution mode  Going Further  The simulation_type can be \u0026ldquo;principal\u0026rdquo; or \u0026ldquo;custom\u0026rdquo;. These map on to https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/iam.html#IAM.Client.simulate_principal_policy and https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/iam.html#IAM.Client.simulate_custom_policy You can pass extra arguments into the either of the calls. Any parameter in the docs specified as CamelCase should be written as snake_case - eg PermissionsBoundaryPolicyInputList should become permissions_boundary_policy_input_list  You have now successfully executed a principal policy simulation!\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/2021-workshop.html",
	"title": "2021 Workshop",
	"tags": [],
	"description": "",
	"content": "Welcome to the Service Catalog tools network capability workshop - 2021 edition.\nThe average completion time for the workshop is between 2 - 3 hours.\nWhat are we going to be doing? In this workshop you will be building out the networking capability for your multi-account environment.\nYou will learn about some of the essential building blocks the Service Catalog tools provide you when creating your multi account workflow:\n provisioning resources into accounts using AWS CloudFormation, AWS Service Catalog and Hashicorp Terraform executing AWS Lambda functions Sharing Service Catalog portfolios  You will be using these building blocks to:\n Remove the default networking resources from your AWS accounts Provision a new AWS VPC using AWS CloudFormation Provision a new subnet using AWS Service Catalog Provision a new subnet using Hashicorp Terraform Share a Service Catalog portfolio so end users can create their own subnets  Let's get going! When you are ready, click the right arrow to begin!\nYou can use the left and right arrows to navigate through the Workshop\n "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/every-day-use/700-using-assertions.html",
	"title": "Using assertions",
	"tags": [],
	"description": "",
	"content": " What are we going to do? With assertions, you tell the framework to compare expected results to actual results. If they do not match then the framework sees this as a failure and anything depending on your assertion will not execute.\nYou can declare the expected results object in your manifest file where you can also tell the framework how to build up an actual results.\nThis tutorial will walk you through how to use the \u0026ldquo;Using assertions\u0026rdquo; feature.\nWe will assume you have:\n installed Service Catalog Puppet correctly bootstrapped a spoke created a manifest file added an account to the manifest file  We are going to perform the following steps to \u0026ldquo;Using assertions\u0026rdquo;:\n Adding an assertion to your manifest file  Step by step guide Here are the steps you need to follow to \u0026ldquo;Using assertions\u0026rdquo;\nAdding an assertion to your manifest file Add the following snippet to your manifest file:\n assertions: assert-no-default-vpcs: expected: source: manifest config: value: - \u0026#34;\u0026#34; actual: source: boto3 config: client: \u0026#39;ec2\u0026#39; call: describe_vpcs arguments: {} use_paginator: true filter: Vpcs[?IsDefault==`true`].State assert_for: tags: - regions: regions_enabled tag: role:all   If you already had an assertions please append the assert-puppet-role-path declaration to the existing assertions section.\nYou will most likely need to update the tag from role:all to whatever you are using in your environment.\nWhat did we just do? In each region of each account in your assert_for you asked service catalog puppet to do the following:\n assume role into the region of the account create an iam client using boto3 using the client, call the describe_vpcs command using a paginator and providing no arguments you then told service catalog puppet to use a filter to remove items from the results of the describe_vpcs command - this is useful to remove things like CreateDate, Arns and other properties that vary by region / account.  Recommendations  It is recommended to add assertions verifying default VPCs are removed. It is recommended to use fine-grained depends_on statements when using assertions to reduce choke points.  You have now successfully executed an assertion!\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/every-day-use/750-managing-organizational-units.html",
	"title": "Managing Organizational Units",
	"tags": [],
	"description": "",
	"content": " What are we going to do? This tutorial will walk you through how to use the \u0026ldquo;Managing Organizational Units\u0026rdquo; feature.\nWe will assume you have:\n installed Service Catalog Puppet correctly set up AWS Organizations support for Puppet created a manifest file  We are going to perform the following steps to \u0026ldquo;Managing Organizational Units\u0026rdquo;:\n Creating an Organizational Unit Optimising Your Organizations Usage  Step by step guide Here are the steps you need to follow to \u0026ldquo;Managing Organizational Units\u0026rdquo;\nCreating an Organizational Unit To create an organizational unit you must create an organizational-units entry in your manifest file:\n organizational-units: workloads-production: path: \u0026#34;/workloads/production\u0026#34; create_in: accounts: - account_id: \u0026#34;012345678910\u0026#34; regions: \u0026#34;default_region\u0026#34; tags: - Key: \u0026#34;WorkloadType\u0026#34; Value: \u0026#34;Production\u0026#34;   The snippet above will create a workloads OU within the root of your organization and will then create a production OU within it. If either of the two OUs exist already this solution will not try to recreate them.\nWhen this solution creates the OU it will assign the tags you have specified.\nOptimising Your Organizations Usage AWS Organization API throttling limits are associated to your management account and shared between all consumers. It is best practice to reduce the calls you make to AWS Organizations. To reduce the calls you can specify parent_ou_id within the configuration. This will stop the solution from \u0026lsquo;looking up\u0026rsquo; the OUs parent using the API and thus save you some API calls.\n organizational-units: workloads-production: path: \u0026#34;/workloads/production\u0026#34; parent_ou_id: \u0026#34;ou-aaaa-bbbbbbbb\u0026#34; create_in: accounts: - account_id: \u0026#34;012345678910\u0026#34; regions: \u0026#34;default_region\u0026#34; tags: - Key: \u0026#34;WorkloadType\u0026#34; Value: \u0026#34;Production\u0026#34;  "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/reinvent2019/",
	"title": "2019 AWS re:Invent Workshop",
	"tags": [],
	"description": "",
	"content": "Welcome to the Service Catalog tools workshop at re:Invent 2019. In this workshop, we will walk through using Service Catalog tools to build controls for governance. At the end of the session, we hope that you will go away with tools and techniques to help you build for security and governance requirements using AWS Service Catalog.\nHouse keeping Please check through the following to help you get started.\nWhat to do if you need help Raise your hand and a helper will be with you as soon as possible.\nMeet the Team In today's workshop you have the following team\nPresenters:\n Eamonn Faherty Jamie McKay  Workshop helpers:\n Ritesh Sinha Thivan Visvanathan Alex Nicot Charles Roberts  Lets get going When you are ready, click the right arrow to begin!\nYou can use the left and right arrows to navigate through the Workshop\n "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/every-day-use/800-applying-service-control-policies.html",
	"title": "Applying Service Control Policies",
	"tags": [],
	"description": "",
	"content": " What are we going to do? With service-control-policies, you tell the framework to apply an SCP to a specific account, every account with a specified tag or to an organizational unit.\nThis tutorial will walk you through how to use the \u0026ldquo;Applying Service Control Policies\u0026rdquo; feature.\nWe will assume you have:\n installed Service Catalog Puppet correctly bootstrapped a spoke created a manifest file added an account to the manifest file  We are going to perform the following steps to \u0026ldquo;Applying Service Control Policies\u0026rdquo;:\n Setting up the IAM role Applying an SCP to a specific account Applying an SCP to every account with a specified tag Applying an SCP to an organizational unit  Step by step guide Here are the steps you need to follow to \u0026ldquo;Applying Service Control Policies\u0026rdquo;\nSetting up the IAM role When using service-control-policies you will need an IAM role assumable from the account where you have installed this framework.\nUsing AWS CloudFormation to create the IAM Role Within your AWS Organizations management account you should create an AWS CloudFormation stack with the following name:\npuppet-organizations-service-control-policies-stack\nUsing the template of the URL:\nhttps://service-catalog-tools.s3.eu-west-2.amazonaws.com/puppet/latest/servicecatalog-puppet-scp-master.template.yaml\nThis stack will have an output named PuppetOrgRoleForExpandsArn. Take a note of this Arn as you will need it in the next step.\nSetting the Org IAM role ARN Once you have created the IAM Role, you need to tell the framework which role you want to use. You do this by creating an AWS Systems Manager Parameter Store parameter named /servicecatalog-puppet/org-scp-role-arn in the region of the account where you will install puppet and use as your hub account. You can do this via the console or via the cli.\n aws ssm put-parameter --name /servicecatalog-puppet/org-scp-role-arn --type String --value \u0026lt;VALUE-FROM-STEP-ABOVE\u0026gt;   Applying an SCP to a specific account To apply an SCP to a specific account add the following snippet to your manifest file:\n service-control-policies: deny-organizations-leave-organization: description: \u0026#34;do not allow accounts to leave\u0026#34; tags: - Key: Category Value: Foundational content: default: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Deny\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;organizations:LeaveOrganization\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } apply_to: accounts: - account_id: \u0026#34;000000000000\u0026#34;   You will need to update the account_id from 000000000000 to the account_id you want to apply the service control policy to.\nPlease note, you do not need to specify a region in the apply_to section.\nApplying an SCP to every account with a specified tag To apply an SCP to every account with specified tag add the following snippet to your manifest file:\n service-control-policies: deny-organizations-leave-organization: description: \u0026#34;do not allow accounts to leave\u0026#34; tags: - Key: Category Value: Foundational content: default: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Deny\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;organizations:LeaveOrganization\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } apply_to: tags: - tag: type:prod   You will need to update the tag from type:prod to the tag you want to apply the service control policy to.\nPlease note, you do not need to specify a region in the apply_to section.\nApplying an SCP to an organizational unit To apply an SCP to a specific organizational unit add the following snippet to your manifest file:\n service-control-policies: deny-organizations-leave-organization: description: \u0026#34;do not allow accounts to leave\u0026#34; tags: - Key: Category Value: Foundational content: default: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Deny\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;organizations:LeaveOrganization\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } apply_to: ous: - ou: /workloads   You will need to update the ou from /workloads to the path you want to apply the service control policy to.\nAlternatively you can use an ou id instead of a path:\n service-control-policies: deny-organizations-leave-organization: description: \u0026#34;do not allow accounts to leave\u0026#34; tags: - Key: Category Value: Foundational content: default: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Deny\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;organizations:LeaveOrganization\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } apply_to: ous: - ou: ou-japk-38pz9nbt   Storing your SCPs in Amazon S3 You can choose to store you Service Control Policies in Amazon S3 using the following syntax:\n service-control-policies: deny-organizations-leave-organization: description: \u0026#34;do not allow accounts to leave\u0026#34; tags: - Key: Category Value: Foundational content: s3: bucket: my-s3-scp-store key: deny-organizations-leave-organization.json apply_to: ous: - ou: /workloads   Extra notes  You do not need to specify a region in the apply_to section Service Control policies can only be used in hub execution mode All json documents will be minimised before being applied  You have now successfully applied a service control policy!\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/every-day-use/850-applying-tag-policies.html",
	"title": "Applying Tag Policies",
	"tags": [],
	"description": "",
	"content": " What are we going to do? With tag-policies, you tell the framework to apply a tag policy to a specific account, every account with a specified tag or to an organizational unit.\nThis tutorial will walk you through how to use the \u0026ldquo;Applying Tag Policies\u0026rdquo; feature.\nWe will assume you have:\n installed Service Catalog Puppet correctly bootstrapped a spoke created a manifest file added an account to the manifest file  We are going to perform the following steps to \u0026ldquo;Applying Tag Policies\u0026rdquo;:\n Setting up the IAM role Applying a tag policy to a specific account Applying a tag policy to every account with a specified tag Applying a tag policy to an organizational unit  Step by step guide Here are the steps you need to follow to \u0026ldquo;Applying Tag Policies\u0026rdquo;\nSetting up the IAM role When using tag-policies you will need an IAM role assumable from the account where you have installed this framework.\nUsing AWS CloudFormation to create the IAM Role Within your AWS Organizations management account you should create an AWS CloudFormation stack with the following name:\npuppet-organizations-service-control-policies-stack\nUsing the template of the URL:\nhttps://service-catalog-tools.s3.eu-west-2.amazonaws.com/puppet/latest/servicecatalog-puppet-scp-master.template.yaml\nThis stack will have an output named PuppetOrgRoleForExpandsArn. Take a note of this Arn as you will need it in the next step.\nSetting the Org IAM role ARN Once you have created the IAM Role, you need to tell the framework which role you want to use. You do this by creating an AWS Systems Manager Parameter Store parameter named /servicecatalog-puppet/org-scp-role-arn in the region of the account where you will install puppet and use as your hub account. You can do this via the console or via the cli.\n aws ssm put-parameter --name /servicecatalog-puppet/org-scp-role-arn --type String --value \u0026lt;VALUE-FROM-STEP-ABOVE\u0026gt;   Applying a tag policy to a specific account To apply a tag policy to a specific account add the following snippet to your manifest file:\n tag-policies: force-cost-center: description: \u0026#34;force cost center\u0026#34; tags: - Key: Category Value: Foundational content: default: { \u0026#34;tags\u0026#34;: { \u0026#34;costcenter\u0026#34;: { \u0026#34;tag_key\u0026#34;: { \u0026#34;@@assign\u0026#34;: \u0026#34;CostCenter\u0026#34; }, \u0026#34;tag_value\u0026#34;: { \u0026#34;@@assign\u0026#34;: [ \u0026#34;100\u0026#34;, \u0026#34;200\u0026#34; ] }, \u0026#34;enforced_for\u0026#34;: { \u0026#34;@@assign\u0026#34;: [ \u0026#34;secretsmanager:*\u0026#34; ] } } } } apply_to: accounts: - account_id: \u0026#34;338024302548\u0026#34;  You will need to update the account_id from 000000000000 to the account_id you want to apply the tag policy to.\nPlease note, you do not need to specify a region in the apply_to section.\nApplying a tag policy to every account with a specified tag To apply a tag policy to every account with specified tag add the following snippet to your manifest file:\n tag-policies: force-cost-center: description: \u0026#34;force cost center\u0026#34; tags: - Key: Category Value: Foundational content: default: { \u0026#34;tags\u0026#34;: { \u0026#34;costcenter\u0026#34;: { \u0026#34;tag_key\u0026#34;: { \u0026#34;@@assign\u0026#34;: \u0026#34;CostCenter\u0026#34; }, \u0026#34;tag_value\u0026#34;: { \u0026#34;@@assign\u0026#34;: [ \u0026#34;100\u0026#34;, \u0026#34;200\u0026#34; ] }, \u0026#34;enforced_for\u0026#34;: { \u0026#34;@@assign\u0026#34;: [ \u0026#34;secretsmanager:*\u0026#34; ] } } } } apply_to: tags: - tag: type:prod   You will need to update the tag from type:prod to the tag you want to apply the tag policy to.\nPlease note, you do not need to specify a region in the apply_to section.\nApplying a tag policy to an organizational unit To apply a tag policy to a specific organizational unit add the following snippet to your manifest file:\n tag-policies: force-cost-center: description: \u0026#34;force cost center\u0026#34; tags: - Key: Category Value: Foundational content: default: { \u0026#34;tags\u0026#34;: { \u0026#34;costcenter\u0026#34;: { \u0026#34;tag_key\u0026#34;: { \u0026#34;@@assign\u0026#34;: \u0026#34;CostCenter\u0026#34; }, \u0026#34;tag_value\u0026#34;: { \u0026#34;@@assign\u0026#34;: [ \u0026#34;100\u0026#34;, \u0026#34;200\u0026#34; ] }, \u0026#34;enforced_for\u0026#34;: { \u0026#34;@@assign\u0026#34;: [ \u0026#34;secretsmanager:*\u0026#34; ] } } } } apply_to: ous: - ou: /workloads   You will need to update the ou from /workloads to the path you want to apply the tag policy to.\nAlternatively you can use an ou id instead of a path:\n tag-policies: force-cost-center: description: \u0026#34;force cost center\u0026#34; tags: - Key: Category Value: Foundational content: default: { \u0026#34;tags\u0026#34;: { \u0026#34;costcenter\u0026#34;: { \u0026#34;tag_key\u0026#34;: { \u0026#34;@@assign\u0026#34;: \u0026#34;CostCenter\u0026#34; }, \u0026#34;tag_value\u0026#34;: { \u0026#34;@@assign\u0026#34;: [ \u0026#34;100\u0026#34;, \u0026#34;200\u0026#34; ] }, \u0026#34;enforced_for\u0026#34;: { \u0026#34;@@assign\u0026#34;: [ \u0026#34;secretsmanager:*\u0026#34; ] } } } } apply_to: ous: - ou: ou-japk-38pz9nbt   Storing your policies in Amazon S3 You can choose to store you tag policies in Amazon S3 using the following syntax:\n tag-policies: force-cost-center: description: \u0026#34;force cost center\u0026#34; tags: - Key: Category Value: Foundational content: s3: bucket: my-policy-store key: force-cost-center.json apply_to: ous: - ou: /workloads   Extra notes  You do not need to specify a region in the apply_to section Tag policies can only be used in hub execution mode All json documents will be minimised before being applied  You have now successfully applied a tag policy!\n"
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/every-day-use/applying-cloud-custodian-policies.html",
	"title": "Applying Cloud Custodian policies",
	"tags": [],
	"description": "",
	"content": " What are we going to do? Cloud Custodian enables you to manage your cloud resources by filtering, tagging, and then applying actions to them. The YAML DSL allows definition of rules to enable well-managed cloud infrastructure that's both secure and cost optimized.\nThis solution automates the setup of a multi account Cloud Custodian installation allowing you to make use of the AWS Serverless services. This solution will set up the AWS EventBridge policies, event buses, event forwarding, cross account roles, provisioning of the policy AWS Lambda Functions for real time monitoring and the periodic triggering needed for delayed actions and retrospective monitoring.\nModes This solution allows you to write policies in any of the c7n supported modes. If you write pull based policies they will be run periodically within an AWS CodeBuild environment on a schedule of your choosing. AWS Lambda policies are provisioned within an AWS CodeBuild project run.\nGetting started To get started you must add a c7n-aws-lambdas section to the manifest file and specify the custodian, policies and apply_to configurations.\nHere is an example cloudtrail policy:\n c7n-aws-lambdas: policies-for-132608235283: custodian: \u0026#39;132608235283\u0026#39; policies: - name: stop-all-ec2s resource: ec2 mode: type: cloudtrail events: - RunInstances actions: - type: mark-for-op op: stop apply_to: tags: - tag: \u0026#39;group:spokes\u0026#39; regions: \u0026#34;enabled_regions\u0026#34;  If you want to use delayed actions - for example terminate unused ebs volumes after 30 days then you will need to set a schedule_expression. This will trigger the solution to run c7n as often as you have specified. You can use AWS Amazon EventBridge cron or rate expressions when specifying the value. You should choose a value that makes sense depending on your requirements, for example if you wait 30 days before performing actions you could probably use a 24hr schedule or if you want to be more aggressive with your cost savings or have stricter policies you can set the schedule to run hourly. We do not recommend running the schedule in such a way where run N is starting before run N-1 has finished.\nHere is an example:\n c7n-aws-lambdas: policies-for-132608235283: execution: hub custodian: \u0026#39;132608235283\u0026#39; schedule_expression: rate(1 day) role_path: \u0026#34;/c7nc7n/\u0026#34; role_name: \u0026#34;C7NExecutor\u0026#34; policies: - name: ebs-mark-unattached-deletion resource: ebs comments: | Mark any unattached EBS volumes for deletion in 30 days. Volumes set to not delete on instance termination do have valid use cases as data drives, but 99% of the time they appear to be just garbage creation. filters: - Attachments: [ ] - \u0026#34;tag:maid_status\u0026#34;: absent actions: - type: mark-for-op op: delete days: 30 - name: ebs-unmark-attached-deletion resource: ebs comments: | Unmark any attached EBS volumes that were scheduled for deletion if they are currently attached filters: - type: value key: \u0026#34;Attachments[0].Device\u0026#34; value: not-null - \u0026#34;tag:maid_status\u0026#34;: not-null actions: - unmark - name: ebs-delete-marked resource: ebs comments: | Delete any attached EBS volumes that were scheduled for deletion filters: - type: marked-for-op op: delete actions: - delete apply_to: tags: - tag: \u0026#39;group:hundred\u0026#39; regions: \u0026#34;enabled_regions\u0026#34;  What does this do? For the accounts you specify as custodians an Amazon EventBridge EventBus will be provisioned in the default region with a bus policy allowing the accounts you are monitoring to put events - cross region and cross account. For each region of each account you monitor an event rule will be provisioned, forwarding events to the custodian account using an IAM role provisioned in the account being monitored.\nWhen you specify a policy, Cloud Custodian provisions a specific event rule based on the events you specified that trigger an AWS Lamdba function it created for that policy to perform the actions you specified.\nAdding the policies Whatever policies you would have written in Cloud Custodian can be pasted into the manifest file \u0026lsquo;as is\u0026rsquo; - you do not need to make any changes. If you do not specify a mode: type: cloudtrail it will be added for you. If you specify a mode: type: something-else it will be replaced with mode: type: cloudtrail. You should not specify a mode: member-role attribute, one will be added for you and any you add will be overriden.\nCustomising the permissions and the roles used You can customise the name of the role, the path and the managed policies attached to the role:\n c7n-aws-cloudtrails: policies-for-132608235283: custodian: \u0026#39;132608235283\u0026#39; role_path: \u0026#34;/c7nc7n/\u0026#34; role_name: \u0026#34;C7NExecutor\u0026#34; role_managed_policy_arns: - arn:aws:iam::aws:policy/AnotherPolicy policies: - name: stop-all-ec2s resource: ec2 mode: type: cloudtrail events: - RunInstances actions: - type: mark-for-op op: stop apply_to: tags: - tag: \u0026#39;group:spokes\u0026#39; regions: \u0026#34;enabled_regions\u0026#34;  Running in spoke execution mode To reduce the overall execution time you can deploy Cloud Custodian in spoke execution mode by specifying execution:\n c7n-aws-cloudtrails: policies-for-132608235283: execution: spoke custodian: \u0026#39;132608235283\u0026#39; policies: - name: stop-all-ec2s resource: ec2 mode: type: cloudtrail events: - RunInstances actions: - type: mark-for-op op: stop apply_to: tags: - tag: \u0026#39;group:spokes\u0026#39; regions: \u0026#34;enabled_regions\u0026#34;  "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/managing-your-environments/multi-org-usage.html",
	"title": "Multi Organization usage",
	"tags": [],
	"description": "",
	"content": " What support is there for multiple AWS Organizations? When writing your manifest you can specify accounts or ou within the accounts section. Accounts can belong to any AWS Organization and the solution will manage sharing on your behalf so long as you have bootstrapped the spoke correctly. Currently, spokes can only easily be bootstrapped for one AWS Organization at a time. The OUs you specify in the manifest file have to exist within the same AWS Organization where you have installed this solution.\nWhen you configure your installations of this solution you specify the git source where your manifest file resides. You can choose to have a single git repo for many installs of this solution. You can then use a branch for each install or you could use intrinsic functions and conditions within your manifest file to manage variance between the different installs.\nIf the configuration between your installs is significant we recommend using differnet git repositories or different branches for each install. If there is minimal different then you can use intrinsic functions, conditions and manifest properties to reduce the duplicated configuration.\nIntrinsic Functions There is a built in intrinsic function ${AWS::PuppetAccountId} that you can use anywhere within your manifest file.\nWhen your manifest file being expanded the string ${AWS::PuppetAccountId} will be replace with the value of the account id. This allows you avoid hard coding the account id of the hub account.\n accounts: - account_id: \u0026#34;${AWS::PuppetAccountId}\u0026#34; name: \u0026#34;puppet-account\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; - \u0026#34;eu-west-2\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34;   You can specify your own functions within a file named intrinsic-functions.properties within the root of your ServiceCatalogPuppet repository. Within this you can specify name value pairs:\n SecurityToolingAccountId=012345678910   You can then use these within your manifest file anywhere using ${Custom::\u0026lt;THE_NAME_YOU_SPECIFIED\u0026gt;}:\n accounts: - account_id: \u0026#34;${Custom::SecurityToolingAccountId}\u0026#34; name: \u0026#34;puppet-account\u0026#34; default_region: \u0026#34;eu-west-1\u0026#34; regions_enabled: - \u0026#34;eu-west-1\u0026#34; - \u0026#34;eu-west-2\u0026#34; tags: - \u0026#34;type:prod\u0026#34; - \u0026#34;partition:eu\u0026#34;   To improve readability we recommend you use the functions to set global parameters and use YAML alias and anchors to reduce the size and complexity of your expanded manifest file by removing duplication.\nIf you want to specify different values for different installs you can override values in an intrinsic-functions-0123456789010.properties file. In this example when the manifest file is expanded in account 0123456789010 any values in intrinsic-functions-0123456789010.properties will override values set in the intrinsic-functions.properties file.\nConditions You can create conditions within your manifest file. These conditions work just like the AWS CloudFormation conditions and can be used to decide whether actions occur or not:\n conditions: IsDev: !Equals - 156551640785 - ${AWS::PuppetAccountId} IsProd: !Not - !Equals - 156551640785 - ${AWS::PuppetAccountId}   At the moment you can use Equals and Not functions to create conditions. Then you can specify conditions for your stacks and other actions:\n stacks: amazon-guardduty-multi-account-prereqs-orgs-account: condition: IsDev name: amazon-guardduty-multi-account-prereqs-orgs-account version: v1 execution: hub capabilities: - CAPABILITY_NAMED_IAM deploy_to: tags: - tag: \u0026#39;role:org_management\u0026#39; regions: default_region outputs: ssm: - param_name: \u0026#34;/foundational/GuardDutyMultiAccount/GuardDutyMultiAccountDelegateAdminRoleArn\u0026#34; stack_output: GuardDutyMultiAccountDelegateAdminRoleArn   When you use conditions you are responsible for ensuring that all actions referenced within the depends_on have the same conditions.\nManifest properties If you are using puppet to manage multiple environments you may find it easier to keep the versions of your launches in properties files instead of the manifest.yaml files. To do this you create a file named manifest.properties in the same directory as your manifest.yaml file. Within this file you can specify the following:\n [launches] IAM-1.version = v50  This will set the version for the launch with the name IAM-1 to v50.\nPlease note this will overwrite the values specified in the manifest.yaml files with no warning.\nIf you are using multiple instances of puppet you can also create a file named manifest-.properties. Values in this file will overwrite all other values making the order of reading:\n manifest.yaml other manifest files (eg in manifests/) manifest.properties manifest-\u0026laquo;puppet-account-id\u0026raquo;.properties  "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/999-further_reading.html",
	"title": "Further Reading",
	"tags": [],
	"description": "",
	"content": "From here you can find out more about the Service Catalog Tools the AWS managed services used in the tutorials and workshops presented on this site.\nService Catalog Tools The following links provide more information on the open source tools used in the tutorials and workshops presented on this site:\nService Catalog Factory  Github project page for Service Catalog Factory Documentation for Service Catalog Factory  Service Catalog Puppet  Github project page for Service Catalog Puppet Documentation for Service Catalog Puppet  Shared products  We have Service Catalog products that you can use with these tools. These products perform actions like setting up multi account AWS Config or AWS Security Hub and more. You can view them on their Github project page.  AWS Services The following links provide more information on the AWS managed services used in the tutorials and workshops presented on this site:\nAWS Service Catalog  AWS Service Catalog home page  "
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/categories.html",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://aws-samples.github.io/aws-service-catalog-tools-workshop/tags.html",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]